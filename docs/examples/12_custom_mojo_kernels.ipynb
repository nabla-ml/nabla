{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f30266",
   "metadata": {},
   "source": [
    "# Example 11: Custom Mojo Kernels\n",
    "\n",
    "Nabla lets you write custom operations in **Mojo** — a high-performance\n",
    "language that compiles to MAX graphs. This means you can:\n",
    "\n",
    "1. Write a Mojo kernel (elementwise, reduction, etc.)\n",
    "2. Wrap it as a Nabla `Operation` in Python\n",
    "3. Use it like any built-in op — including with `nb.grad`, `nb.vmap`, etc.\n",
    "\n",
    "**Requirements:** The `modular` package must be installed (`pip install modular`).\n",
    "Mojo kernels are compiled automatically by the MAX engine at graph execution time.\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────────┐     ┌────────────────┐\n",
    "│  Mojo kernel │ ──▶ │ Python Operation │ ──▶ │  Nabla Tensor  │\n",
    "│  (.mojo file)│     │ (UnaryOperation) │     │  computation   │\n",
    "└──────────────┘     └──────────────────┘     └────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7a7190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX SDK available — custom kernels enabled\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "# Check if MAX/Mojo is available\n",
    "try:\n",
    "    from max.graph import TensorValue\n",
    "    from nabla.ops import UnaryOperation, call_custom_kernel\n",
    "    HAS_MOJO = True\n",
    "    print(\"MAX SDK available — custom kernels enabled\")\n",
    "except ImportError:\n",
    "    HAS_MOJO = False\n",
    "    print(\"MAX SDK not installed — showing code patterns only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6f97d",
   "metadata": {},
   "source": [
    "## 1. Writing a Mojo Kernel\n",
    "\n",
    "A Mojo kernel is a struct registered with `@compiler.register(\"name\")`.\n",
    "The `execute` method receives input/output tensors and a device context.\n",
    "\n",
    "Here's a simple kernel that adds 1 to every element:\n",
    "\n",
    "```mojo\n",
    "# kernels/custom_kernel.mojo\n",
    "import compiler\n",
    "from runtime.asyncrt import DeviceContextPtr\n",
    "from tensor import InputTensor, OutputTensor, foreach\n",
    "from utils.index import IndexList\n",
    "\n",
    "\n",
    "@compiler.register(\"add_one\")\n",
    "struct AddOneKernel:\n",
    "    @staticmethod\n",
    "    fn execute[\n",
    "        target: StaticString\n",
    "    ](\n",
    "        output: OutputTensor,\n",
    "        x: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        ctx: DeviceContextPtr,\n",
    "    ):\n",
    "        @parameter\n",
    "        fn add_one[width: Int](idx: IndexList[x.rank]) -> SIMD[x.dtype, width]:\n",
    "            return x.load[width](idx) + 1\n",
    "\n",
    "        foreach[add_one, target=target](output, ctx)\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- `@compiler.register(\"add_one\")` — the name you'll reference from Python\n",
    "- `foreach` auto-vectorizes the elementwise function across the tensor\n",
    "- `InputTensor` / `OutputTensor` handle memory layout automatically\n",
    "- The kernel directory also needs an `__init__.mojo` file (can be empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc1149",
   "metadata": {},
   "source": [
    "## 2. Python Operation Wrapper\n",
    "\n",
    "To use the Mojo kernel in Nabla, wrap it as a `UnaryOperation` subclass.\n",
    "The `kernel` method bridges Python tensors to the Mojo function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963b8502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddOneOp registered\n"
     ]
    }
   ],
   "source": [
    "if HAS_MOJO:\n",
    "    class AddOneOp(UnaryOperation):\n",
    "        \"\"\"Custom op: adds 1 to every element using a Mojo kernel.\"\"\"\n",
    "\n",
    "        @property\n",
    "        def name(self) -> str:\n",
    "            return \"add_one\"\n",
    "\n",
    "        def kernel(self, args, kwargs):\n",
    "            \"\"\"Invoke the Mojo kernel via call_custom_kernel.\"\"\"\n",
    "            x = args[0]\n",
    "            # Point to the directory containing the .mojo kernel files\n",
    "            kernel_dir = Path(\"../../tests/mojo/kernels\")\n",
    "            result = call_custom_kernel(\"my_kernel\", kernel_dir, x, x.type)\n",
    "            return [result]  # Must return a list of TensorValues\n",
    "\n",
    "        def _derivative(self, primals, output):\n",
    "            \"\"\"d(x+1)/dx = 1 — gradient passes through unchanged.\"\"\"\n",
    "            return nb.ones_like(primals)\n",
    "\n",
    "    # Instantiate the op (ops are stateless singletons)\n",
    "    _add_one_op = AddOneOp()\n",
    "\n",
    "    def add_one(x):\n",
    "        \"\"\"Add 1 to every element using our custom Mojo kernel.\"\"\"\n",
    "        return _add_one_op([x], {})[0]\n",
    "\n",
    "    print(\"AddOneOp registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba09be",
   "metadata": {},
   "source": [
    "**Important details:**\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `name` | Must match the `@compiler.register(\"...\")` name |\n",
    "| `kernel(args, kwargs)` | `args` is a `list[TensorValue]`, `kwargs` is a `dict` |\n",
    "| `_derivative(primals, output)` | Enables `nb.grad` — return $\\frac{\\partial \\text{out}}{\\partial \\text{in}}$ |\n",
    "\n",
    "For non-elementwise ops, override `vjp_rule` and `jvp_rule` directly\n",
    "instead of `_derivative`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892e6be",
   "metadata": {},
   "source": [
    "## 3. Using the Custom Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c8ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [1. 2. 3.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute '_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     y = add_one(x)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.to_numpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# [2.0, 3.0, 4.0]\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSkipped — MAX SDK not available\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/tensor/api.py:1006\u001b[39m, in \u001b[36mTensor.to_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert tensor to numpy array.\"\"\"\u001b[39;00m\n\u001b[32m   1005\u001b[39m t = \u001b[38;5;28mself\u001b[39m.gather()\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._impl._buffers:\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to realize tensor for NumPy export\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/tensor/api.py:740\u001b[39m, in \u001b[36mTensor.realize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRAPH\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[43mGRAPH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:538\u001b[39m, in \u001b[36mComputeGraph.evaluate\u001b[39m\u001b[34m(self, tensor, return_model, *extra_outputs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Replay trace to build MAX graph\u001b[39;00m\n\u001b[32m    537\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: replaying trace to build graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_replay_trace_to_build_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Build graph outputs\u001b[39;00m\n\u001b[32m    541\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: building graph outputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:793\u001b[39m, in \u001b[36mComputeGraph._replay_trace_to_build_graph\u001b[39m\u001b[34m(self, targets)\u001b[39m\n\u001b[32m    790\u001b[39m op_args = pytree.tree_map(to_tensor, opnode.op_args)\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     raw_result = \u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m executed_ops += \u001b[32m1\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# Extract graph values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/base.py:191\u001b[39m, in \u001b[36mOperation.execute\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m mesh = spmd.get_mesh_from_args(args)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m GRAPH.graph:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     shard_results = \u001b[43mspmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_on_shards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapted_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m output_sharding = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._infer_output_sharding:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/sharding/spmd.py:411\u001b[39m, in \u001b[36mexecute_on_shards\u001b[39m\u001b[34m(op_fn, args, kwargs, mesh, input_shardings, op)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    404\u001b[39m     op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    405\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m output_sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(op, \u001b[33m\"\u001b[39m\u001b[33m_transform_shard_kwargs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m ):\n\u001b[32m    408\u001b[39m     local_kwargs = op._transform_shard_kwargs(\n\u001b[32m    409\u001b[39m         kwargs, output_sharding, \u001b[32m0\u001b[39m, shard_args\n\u001b[32m    410\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m result = \u001b[43mop_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# kernel returns list[TensorValue]; for single-output we need the single value for shard_results\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# But execute_on_shards returns list-of-shard-results. Each shard result should be\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# a single TensorValue (single output) or a list/tuple of TensorValue (multi output).\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# Since kernel now always returns list, we unwrap single-element lists for backward compat\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# with the packaging layer.\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mAddOneOp.kernel\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Point to the directory containing the .mojo kernel files\u001b[39;00m\n\u001b[32m     13\u001b[39m kernel_dir = Path(\u001b[33m\"\u001b[39m\u001b[33m../../tests/mojo/kernels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m result = \u001b[43mcall_custom_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmy_kernel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/utils.py:767\u001b[39m, in \u001b[36mcall_custom_kernel\u001b[39m\u001b[34m(func_name, kernel_path, values, out_types, device, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m     path_obj = Path(p).resolve()\n\u001b[32m    766\u001b[39m     resolved_paths.append(path_obj)\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m GRAPH.graph._kernel_library.load_paths(\u001b[43mGRAPH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m, resolved_paths)\n\u001b[32m    769\u001b[39m results = ops.custom(\n\u001b[32m    770\u001b[39m     name=func_name,\n\u001b[32m    771\u001b[39m     device=device,\n\u001b[32m   (...)\u001b[39m\u001b[32m    774\u001b[39m     **kwargs,\n\u001b[32m    775\u001b[39m )\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "\u001b[31mAttributeError\u001b[39m: 'Graph' object has no attribute '_context'"
     ]
    }
   ],
   "source": [
    "if HAS_MOJO:\n",
    "    x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "    y = add_one(x)\n",
    "    print(f\"Input:  {x.to_numpy()}\")\n",
    "    print(f\"Output: {y.to_numpy()}\")  # [2.0, 3.0, 4.0]\n",
    "else:\n",
    "    print(\"Skipped — MAX SDK not available\")\n",
    "    print(\"Expected: add_one([1.0, 2.0, 3.0]) → [2.0, 3.0, 4.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c37b59",
   "metadata": {},
   "source": [
    "## 4. Differentiating Through Custom Ops\n",
    "\n",
    "Because we implemented `_derivative`, Nabla can differentiate through\n",
    "our custom kernel just like any built-in op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MOJO:\n",
    "    def f(x):\n",
    "        \"\"\"A function using our custom kernel.\"\"\"\n",
    "        return nb.sum(add_one(x) * x)  # sum((x+1) * x) = sum(x² + x)\n",
    "\n",
    "    x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "    grad_f = nb.grad(f)\n",
    "    g = grad_f(x)\n",
    "\n",
    "    print(f\"f(x) = sum((x+1) * x)\")\n",
    "    print(f\"f'(x) = 2x + 1\")\n",
    "    print(f\"Input:    {x.to_numpy()}\")\n",
    "    print(f\"Gradient: {g.to_numpy()}\")  # [3.0, 5.0, 7.0]\n",
    "else:\n",
    "    print(\"Skipped — expected gradient of sum((x+1)*x) = 2x+1 = [3.0, 5.0, 7.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290b17f",
   "metadata": {},
   "source": [
    "## 5. Writing More Complex Kernels\n",
    "\n",
    "The `foreach` pattern handles elementwise ops, but you can write any\n",
    "computation in Mojo. Here's a sketch of a fused multiply-add kernel:\n",
    "\n",
    "```mojo\n",
    "@compiler.register(\"fused_mul_add\")\n",
    "struct FusedMulAdd:\n",
    "    @staticmethod\n",
    "    fn execute[\n",
    "        target: StaticString\n",
    "    ](\n",
    "        output: OutputTensor,\n",
    "        a: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        b: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        c: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        ctx: DeviceContextPtr,\n",
    "    ):\n",
    "        # output = a * b + c\n",
    "        @parameter\n",
    "        fn fma[width: Int](idx: IndexList[a.rank]) -> SIMD[a.dtype, width]:\n",
    "            return a.load[width](idx) * b.load[width](idx) + c.load[width](idx)\n",
    "\n",
    "        foreach[fma, target=target](output, ctx)\n",
    "```\n",
    "\n",
    "For multi-input ops, extend `Operation` instead of `UnaryOperation`\n",
    "and implement `vjp_rule` / `jvp_rule` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b126f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- Custom Mojo kernels compile automatically via the MAX engine\n",
    "- Wrap kernels as `UnaryOperation` (or `Operation`) subclasses\n",
    "- `call_custom_kernel` handles kernel loading and invocation\n",
    "- Implement `_derivative` (elementwise) or `vjp_rule`/`jvp_rule` for autodiff\n",
    "- Custom ops compose with all Nabla transforms (`grad`, `vmap`, `compile`)\n",
    "\n",
    "**Previous:** [10 — LoRA & QLoRA Fine-tuning](10_lora_finetuning_mvp.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
