{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a1b1c8",
   "metadata": {},
   "source": [
    "# Example 3a: MLP Training — PyTorch-Style (Imperative)\n",
    "\n",
    "Nabla supports **two distinct training paradigms**:\n",
    "\n",
    "| Paradigm | Gradient API | Optimizer API |\n",
    "|----------|--------------|---------------|\n",
    "| **PyTorch-style** (this notebook) | `loss.backward()` + `.grad` | `AdamW(model)` → `optimizer.step()` |\n",
    "| **JAX-style** ([03b](03b_mlp_training_jax)) | `nb.value_and_grad(fn)(args)` | `adamw_init` + `adamw_update` |\n",
    "\n",
    "Here we demonstrate the **PyTorch-style** imperative API end-to-end.\n",
    "The training loop mirrors PyTorch exactly: `zero_grad → forward → backward → step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9925241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla MLP Training — PyTorch-style\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla MLP Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719ce33",
   "metadata": {},
   "source": [
    "## 1. Define the Model\n",
    "\n",
    "Subclass `nb.nn.Module` and define layers in `__init__`. The `forward()`\n",
    "method specifies the computation. Parameters (from `nb.nn.Linear`, etc.)\n",
    "are automatically registered and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3bd14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: fc1 [Dim(4), Dim(32)], fc2 [Dim(32), Dim(1)]\n",
      "Total trainable parameters: 193\n"
     ]
    }
   ],
   "source": [
    "class MLP(nb.nn.Module):\n",
    "    \"\"\"Two-layer MLP with ReLU activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nb.nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nb.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nb.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MLP(4, 32, 1)\n",
    "print(f\"Model: fc1 {model.fc1.weight.shape}, fc2 {model.fc2.weight.shape}\")\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a9261",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Data\n",
    "\n",
    "We'll create a regression dataset: predict `y = sin(x0) + cos(x1) + 0.5*x2 - x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea08149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: X [Dim(200), Dim(4)], y [Dim(200), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_np = np.random.randn(n_samples, 4).astype(np.float32)\n",
    "y_np = (\n",
    "    np.sin(X_np[:, 0])\n",
    "    + np.cos(X_np[:, 1])\n",
    "    + 0.5 * X_np[:, 2]\n",
    "    - X_np[:, 3]\n",
    ").reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "print(f\"Dataset: X {X.shape}, y {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065352d",
   "metadata": {},
   "source": [
    "## 3. Set Up the Stateful Optimizer\n",
    "\n",
    "`nb.nn.optim.AdamW` is a **stateful optimizer** — it holds references to\n",
    "the model parameters and maintains its own moment estimates (m, v).\n",
    "This is Nabla's counterpart to `torch.optim.AdamW`.\n",
    "\n",
    "> **JAX-style note:** Nabla's *functional* optimizer (`nb.nn.optim.adamw_init`\n",
    "> + `nb.nn.optim.adamw_update`) takes params and optimizer state as explicit\n",
    "> arguments and returns new values — no internal state at all. See [03b](03b_mlp_training_jax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d49dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (lr=0.01, β=(0.9, 0.999))\n"
     ]
    }
   ],
   "source": [
    "optimizer = nb.nn.optim.AdamW(model, lr=1e-2)\n",
    "print(f\"Optimizer: AdamW (lr={optimizer.lr}, β=({optimizer.beta1}, {optimizer.beta2}))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d3756e",
   "metadata": {},
   "source": [
    "## 4. PyTorch-Style Training Loop\n",
    "\n",
    "The four-step rhythm is identical to PyTorch:\n",
    "\n",
    "1. **`model.zero_grad()`** — clear accumulated `.grad` tensors from the previous iteration\n",
    "2. **Forward pass** — build the lazy computation graph\n",
    "3. **`loss.backward()`** — propagate gradients; every parameter with `requires_grad=True` gets its `.grad` populated and **batch-realized**\n",
    "4. **`optimizer.step()`** — read `.grad` from each parameter, apply the AdamW update, return the updated model\n",
    "\n",
    "> **Lazy execution note:** Because Nabla cannot mutate tensor data in-place\n",
    "> without breaking the lazy graph, `optimizer.step()` returns the new model.\n",
    "> Assign it back to `model` each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb65fa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch    Loss        \n",
      "----------------------\n",
      "10       0.625402    \n",
      "20       0.242037    \n",
      "30       0.159796    \n",
      "40       0.118573    \n",
      "50       0.082741    \n",
      "60       0.064866    \n",
      "70       0.054502    \n",
      "80       0.047352    \n",
      "90       0.041529    \n",
      "100      0.037468    \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Clear gradients from the previous iteration\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = nb.nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "    # 3. Backward pass — gradients stored in p.grad for each parameter\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Optimizer step — reads .grad, applies AdamW update\n",
    "    model = optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4f85c",
   "metadata": {},
   "source": [
    "## 5. Inspecting Gradients via `.grad`\n",
    "\n",
    "After `loss.backward()`, every trainable parameter exposes its gradient\n",
    "via `.grad` — exactly like PyTorch. The gradients are already realized\n",
    "(not lazy) when `.backward()` returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a8bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter gradients after backward():\n",
      "  fc1.bias                        shape [Dim(1), Dim(32)]  |grad|=0.0201\n",
      "  fc1.weight                      shape [Dim(4), Dim(32)]  |grad|=0.0227\n",
      "  fc2.bias                        shape [Dim(1), Dim(1)]  |grad|=0.0086\n",
      "  fc2.weight                      shape [Dim(32), Dim(1)]  |grad|=0.0203\n"
     ]
    }
   ],
   "source": [
    "# One more backward pass to show .grad access\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "b_loss = nb.nn.functional.mse_loss(model(X), y)\n",
    "b_loss.backward()\n",
    "\n",
    "import numpy as _np\n",
    "print(\"Parameter gradients after backward():\")\n",
    "for name, param in model.named_parameters():\n",
    "    g = param.grad\n",
    "    if g is not None:\n",
    "        g_np = _np.from_dlpack(g)\n",
    "        print(f\"  {name:30s}  shape {str(g.shape):<16}  |grad|={float(_np.linalg.norm(g_np)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b015d",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3643fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final MSE loss: 0.037132\n",
      "\n",
      "  Prediction        Target\n",
      "----------------------------\n",
      "      0.1029        0.2678\n",
      "      0.7215        0.7629\n",
      "      0.7124        0.6380\n",
      "     -0.3226       -0.3964\n",
      "      1.2237        1.0610\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "final_loss = nb.nn.functional.mse_loss(model(X), y)\n",
    "print(f\"\\nFinal MSE loss: {final_loss.item():.6f}\")\n",
    "\n",
    "predictions = model(X)\n",
    "print(f\"\\n{'Prediction':>12}  {'Target':>12}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(5):\n",
    "    pred_i = nb.gather(predictions, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    true_i = nb.gather(y, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    print(f\"{pred_i.item():>12.4f}  {true_i.item():>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82635613",
   "metadata": {},
   "source": [
    "## 7. Contrast: JAX-Style API (for reference)\n",
    "\n",
    "The JAX-style equivalent of the same training step — note the absence of\n",
    "`.backward()`, `.grad`, and stateful optimizer mutations:\n",
    "\n",
    "```python\n",
    "# JAX-style (functional) — see 03b_mlp_training_jax.py\n",
    "def loss_fn(model, X, y):\n",
    "    return nb.nn.functional.mse_loss(model(X), y)\n",
    "\n",
    "# Single call computes both the loss value and all gradients\n",
    "loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, X, y)\n",
    "\n",
    "# Functional optimizer — returns new model + new state (no mutation)\n",
    "model, opt_state = nb.nn.optim.adamw_update(model, grads, opt_state, lr=1e-2)\n",
    "```\n",
    "\n",
    "Both paradigms are fully supported in Nabla:\n",
    "- **PyTorch-style**: familiar to PyTorch users, great for interactive\n",
    "  debugging and stateful training loops\n",
    "- **JAX-style**: composable with `nb.vmap`, `@nb.compile`, `nb.jacrev`, etc.;\n",
    "  required when nesting transforms or writing pure-functional pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a688e585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Example 03a completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Example 03a completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d23c8c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### PyTorch-Style API (this notebook)\n",
    "\n",
    "| Concept | API |\n",
    "|---------|-----|\n",
    "| Define model | `class MyModel(nb.nn.Module)` |\n",
    "| Linear layer | `nb.nn.Linear(in_dim, out_dim)` |\n",
    "| Loss functions | `nb.nn.functional.mse_loss`, `cross_entropy_loss` |\n",
    "| Clear gradients | `model.zero_grad()` |\n",
    "| Compute gradients | `loss.backward()` |\n",
    "| Inspect gradients | `param.grad` |\n",
    "| Create optimizer | `optimizer = nb.nn.optim.AdamW(model, lr=...)` |\n",
    "| Update parameters | `model = optimizer.step()` |\n",
    "\n",
    "### JAX-Style API (see [03b](03b_mlp_training_jax))\n",
    "\n",
    "| Concept | API |\n",
    "|---------|-----|\n",
    "| Compute loss + grads | `loss, grads = nb.value_and_grad(fn, argnums=0)(model, ...)` |\n",
    "| Optimizer init | `opt_state = nb.nn.optim.adamw_init(params)` |\n",
    "| Optimizer update | `model, opt_state = nb.nn.optim.adamw_update(...)` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
