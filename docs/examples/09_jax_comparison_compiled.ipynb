{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98764fd7",
   "metadata": {},
   "source": [
    "# Example 9: Compile vs Eager vs JAX\n",
    "\n",
    "This benchmark-style example compares three modes:\n",
    "- Nabla compiled training (`@nb.compile`)\n",
    "- Nabla eager training\n",
    "- JAX `@jit` training (when JAX is installed)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1331d56",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "# Try to import JAX\n",
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax import grad, jit\n",
    "\n",
    "    HAS_JAX = True\n",
    "except ImportError:\n",
    "    HAS_JAX = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921252b6",
   "metadata": {},
   "source": [
    "## 1. Dataset and Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae6a984b",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 500\n",
    "X_np = np.linspace(0, 1, n_samples).reshape(-1, 1).astype(np.float32)\n",
    "y_np = (np.sin(8 * np.pi * X_np) + 1) / 2.0\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MLP Training: Fitting Complex Sine Curve\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {n_samples} samples, fitting (sin(8Ï€*x) + 1)/2\")\n",
    "\n",
    "layers = [1, 16, 32, 64, 64, 64, 64, 32, 16, 1]\n",
    "params = {}\n",
    "\n",
    "for i in range(len(layers) - 1):\n",
    "    in_dim, out_dim = layers[i], layers[i + 1]\n",
    "    limit = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    w_np = np.random.uniform(-limit, limit, (in_dim, out_dim)).astype(np.float32)\n",
    "    b_np = np.zeros((out_dim,), dtype=np.float32)\n",
    "    params[f\"layer{i + 1}\"] = {\n",
    "        \"w\": nb.Tensor.from_dlpack(w_np),\n",
    "        \"b\": nb.Tensor.from_dlpack(b_np),\n",
    "    }\n",
    "\n",
    "total_params = sum(\n",
    "    (layers[i] * layers[i + 1] + layers[i + 1]) for i in range(len(layers) - 1)\n",
    ")\n",
    "print(f\"Architecture: {' -> '.join(map(str, layers))} ({total_params} params)\\n\")\n",
    "\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    h = x\n",
    "    for i in range(1, len(layers)):\n",
    "        h = h @ params[f\"layer{i}\"][\"w\"] + params[f\"layer{i}\"][\"b\"]\n",
    "        if i < len(layers) - 1:\n",
    "            h = nb.relu(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    pred = mlp_forward(params, x)\n",
    "    diff = pred - y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def train_step_compiled(params, x, y):\n",
    "    loss, grads = nb.value_and_grad(loss_fn)(params, x, y)\n",
    "    lr = 0.01\n",
    "    new_params = {}\n",
    "    for layer_name in params:\n",
    "        new_params[layer_name] = {\n",
    "            \"w\": params[layer_name][\"w\"] - grads[layer_name][\"w\"] * lr,\n",
    "            \"b\": params[layer_name][\"b\"] - grads[layer_name][\"b\"] * lr,\n",
    "        }\n",
    "    return loss, new_params\n",
    "\n",
    "\n",
    "def train_step_eager(params, x, y):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, realize=False)(params, x, y)\n",
    "    lr = 0.01\n",
    "    new_params = {}\n",
    "    for layer_name in params:\n",
    "        new_params[layer_name] = {\n",
    "            \"w\": params[layer_name][\"w\"] - grads[layer_name][\"w\"] * lr,\n",
    "            \"b\": params[layer_name][\"b\"] - grads[layer_name][\"b\"] * lr,\n",
    "        }\n",
    "    # Batch realize all outputs\n",
    "    all_outputs = [loss]\n",
    "    for layer_params in new_params.values():\n",
    "        all_outputs.extend(layer_params.values())\n",
    "    nb.realize_all(*all_outputs)\n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d6e88",
   "metadata": {},
   "source": [
    "## 2. Nabla Benchmarks (Compiled vs Eager)"
   ]
  },
  {
   "cell_type": "code",
   "id": "210a55d9",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Compiled (@nb.compile)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "params_compiled = params\n",
    "n_steps = 200\n",
    "\n",
    "loss, params_compiled = train_step_compiled(params_compiled, X, y)\n",
    "print(f\"Warmup: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "losses_compiled = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_compiled = train_step_compiled(params_compiled, X, y)\n",
    "    losses_compiled.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "elapsed_compiled = time.perf_counter() - start\n",
    "print(f\"\\nTime: {elapsed_compiled:.4f}s ({n_steps / elapsed_compiled:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_compiled[0]:.6f} -> {losses_compiled[-1]:.6f}\")\n",
    "print(f\"Compile stats: {train_step_compiled.stats}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Eager (no compile)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "params_eager = params\n",
    "\n",
    "loss, params_eager = train_step_eager(params_eager, X, y)\n",
    "print(f\"Warmup: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "losses_eager = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_eager = train_step_eager(params_eager, X, y)\n",
    "    losses_eager.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "elapsed_eager = time.perf_counter() - start\n",
    "print(f\"\\nTime: {elapsed_eager:.4f}s ({n_steps / elapsed_eager:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_eager[0]:.6f} -> {losses_eager[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "speedup = elapsed_eager / elapsed_compiled\n",
    "print(f\"Speedup: {speedup:.2f}x with compile\")\n",
    "print(f\"  Compiled: {elapsed_compiled:.4f}s\")\n",
    "print(f\"  Eager:    {elapsed_eager:.4f}s\")\n",
    "\n",
    "loss_diff = abs(losses_compiled[-1] - losses_eager[-1])\n",
    "print(f\"\\nLoss difference: {loss_diff:.8f}\")\n",
    "if loss_diff < 1e-4:\n",
    "    print(\"âœ“ Compiled and eager match!\")\n",
    "else:\n",
    "    print(\"âš  Compiled and eager differ!\")\n",
    "\n",
    "# Test 2b: Eager MAX Graph (builds MAX graph eagerly each step)\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 2b: Eager MAX Graph (EAGER_MAX_GRAPH=1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import nabla.config as nabla_config\n",
    "\n",
    "orig_eager_max = nabla_config.EAGER_MAX_GRAPH\n",
    "nabla_config.EAGER_MAX_GRAPH = True\n",
    "\n",
    "params_eager_max = params\n",
    "\n",
    "loss, params_eager_max = train_step_eager(params_eager_max, X, y)\n",
    "print(f\"Warmup: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "losses_eager_max = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_eager_max = train_step_eager(params_eager_max, X, y)\n",
    "    losses_eager_max.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "\n",
    "elapsed_eager_max = time.perf_counter() - start\n",
    "nabla_config.EAGER_MAX_GRAPH = orig_eager_max\n",
    "\n",
    "print(f\"\\nTime: {elapsed_eager_max:.4f}s ({n_steps / elapsed_eager_max:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_eager_max[0]:.6f} -> {losses_eager_max[-1]:.6f}\")\n",
    "\n",
    "loss_diff_max = abs(losses_compiled[-1] - losses_eager_max[-1])\n",
    "print(f\"Loss diff vs compiled: {loss_diff_max:.8f}\")\n",
    "if loss_diff_max < 1e-4:\n",
    "    print(\"âœ“ Eager MAX Graph and compiled match!\")\n",
    "else:\n",
    "    print(\"âš  Eager MAX Graph and compiled differ!\")\n",
    "\n",
    "# Test 3: JAX JIT comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419e38f",
   "metadata": {},
   "source": [
    "## 3. JAX Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "id": "25e3cf13",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_JAX:\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TEST 3: JAX with @jit (for comparison)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Convert params to JAX format (flat list for simplicity)\n",
    "    jax_params = []\n",
    "    for layer_name in sorted(params.keys()):\n",
    "        w_np = params[layer_name][\"w\"].to_numpy()\n",
    "        b_np = params[layer_name][\"b\"].to_numpy()\n",
    "        jax_params.append(jnp.array(w_np))\n",
    "        jax_params.append(jnp.array(b_np))\n",
    "\n",
    "    X_jax = jnp.array(X_np)\n",
    "    y_jax = jnp.array(y_np)\n",
    "\n",
    "    def jax_mlp(params_flat, x):\n",
    "        h = x\n",
    "        for i in range(0, len(params_flat) - 2, 2):\n",
    "            h = h @ params_flat[i] + params_flat[i + 1]\n",
    "            h = jax.nn.relu(h)\n",
    "        h = h @ params_flat[-2] + params_flat[-1]\n",
    "        return h\n",
    "\n",
    "    def jax_loss(params_flat, x, y):\n",
    "        pred = jax_mlp(params_flat, x)\n",
    "        return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "    @jit\n",
    "    def jax_train_step(params_flat, x, y):\n",
    "        loss = jax_loss(params_flat, x, y)\n",
    "        grads = grad(jax_loss)(params_flat, x, y)\n",
    "        lr = 0.01\n",
    "        new_params = [p - g * lr for p, g in zip(params_flat, grads, strict=False)]\n",
    "        return loss, new_params\n",
    "\n",
    "    # Warmup\n",
    "    loss_jax, jax_params = jax_train_step(jax_params, X_jax, y_jax)\n",
    "    jax.block_until_ready(loss_jax)\n",
    "    print(f\"Warmup (trace): loss = {float(loss_jax):.6f}\")\n",
    "\n",
    "    # Timed training\n",
    "    start = time.perf_counter()\n",
    "    losses_jax = []\n",
    "    for i in range(n_steps):\n",
    "        loss_jax, jax_params = jax_train_step(jax_params, X_jax, y_jax)\n",
    "        jax.block_until_ready(loss_jax)\n",
    "        losses_jax.append(float(loss_jax))\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Step {i + 1:3d}: loss = {float(loss_jax):.6f}\")\n",
    "\n",
    "    elapsed_jax = time.perf_counter() - start\n",
    "    print(\"\\nJAX JIT version:\")\n",
    "    print(f\"  Time: {elapsed_jax:.4f}s ({n_steps / elapsed_jax:.1f} steps/sec)\")\n",
    "    print(f\"  Final loss: {losses_jax[-1]:.6f}\")\n",
    "    print(\n",
    "        f\"  Loss reduction: {losses_jax[0]:.6f} -> {losses_jax[-1]:.6f} ({(1 - losses_jax[-1] / losses_jax[0]) * 100:.1f}% reduction)\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    f\"Nabla Compiled:        {elapsed_compiled:.4f}s ({n_steps / elapsed_compiled:.1f} steps/sec)\"\n",
    ")\n",
    "print(\n",
    "    f\"Nabla Eager (deferred):{elapsed_eager:.4f}s ({n_steps / elapsed_eager:.1f} steps/sec)\"\n",
    ")\n",
    "print(\n",
    "    f\"Nabla Eager (MAX):     {elapsed_eager_max:.4f}s ({n_steps / elapsed_eager_max:.1f} steps/sec)\"\n",
    ")\n",
    "if HAS_JAX:\n",
    "    print(\n",
    "        f\"JAX JIT:               {elapsed_jax:.4f}s ({n_steps / elapsed_jax:.1f} steps/sec)\"\n",
    "    )\n",
    "    print()\n",
    "    speedup_vs_jax = elapsed_jax / elapsed_compiled\n",
    "    if speedup_vs_jax > 1:\n",
    "        print(f\"ðŸš€ Nabla is {speedup_vs_jax:.2f}x FASTER than JAX!\")\n",
    "    else:\n",
    "        print(f\"JAX is {1 / speedup_vs_jax:.2f}x faster than Nabla\")\n",
    "print()\n",
    "print(f\"Nabla speedup over eager (deferred): {speedup:.2f}x\")\n",
    "print(\n",
    "    f\"Nabla speedup over eager (MAX graph): {elapsed_eager_max / elapsed_compiled:.2f}x\"\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eed7d",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "13816fee",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ MLP training works with compile!\")\n",
    "print(\"âœ“ Full pytree parameters (weights + biases) work correctly\")\n",
    "print(\n",
    "    f\"âœ“ Loss decreases properly: {losses_compiled[0]:.6f} -> {losses_compiled[-1]:.6f}\"\n",
    ")\n",
    "print(f\"âœ“ {speedup:.2f}x speedup from compilation\")\n",
    "print(f\"âœ“ Cache hit rate: {train_step_compiled.stats.hit_rate:.1f}%\")\n",
    "if HAS_JAX:\n",
    "    print(\"âœ“ Compared against JAX JIT successfully\")"
   ]
  }
 ]
}
