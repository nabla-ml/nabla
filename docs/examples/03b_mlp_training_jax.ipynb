{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18c96dc",
   "metadata": {},
   "source": [
    "# Example 3b: MLP Training (JAX-Style / Functional)\n",
    "\n",
    "In this style, the model is a **pure function** that takes parameters\n",
    "explicitly. Parameters are stored in nested dicts (pytrees). This is\n",
    "the same approach used by JAX and Flax.\n",
    "\n",
    "This example trains the same 2-layer MLP from Example 3a, but purely\n",
    "functionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def22625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla MLP Training — JAX-style (functional)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla.nn.functional import xavier_normal\n",
    "\n",
    "print(\"Nabla MLP Training — JAX-style (functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832f1e2",
   "metadata": {},
   "source": [
    "## 1. Initialize Parameters\n",
    "\n",
    "Instead of a class, we create a nested dict of parameter tensors.\n",
    "Each tensor gets `requires_grad=True` so autodiff can track through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36c6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter shapes:\n",
      "  fc1.weight: [Dim(4), Dim(32)]\n",
      "  fc1.bias: [Dim(1), Dim(32)]\n",
      "  fc2.weight: [Dim(32), Dim(1)]\n",
      "  fc2.bias: [Dim(1), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "def init_mlp_params(in_dim: int, hidden_dim: int, out_dim: int) -> dict:\n",
    "    \"\"\"Initialize MLP parameters as a pytree (nested dict).\"\"\"\n",
    "    params = {\n",
    "        \"fc1\": {\n",
    "            \"weight\": xavier_normal((in_dim, hidden_dim)),\n",
    "            \"bias\": nb.zeros((1, hidden_dim)),\n",
    "        },\n",
    "        \"fc2\": {\n",
    "            \"weight\": xavier_normal((hidden_dim, out_dim)),\n",
    "            \"bias\": nb.zeros((1, out_dim)),\n",
    "        },\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "params = init_mlp_params(4, 32, 1)\n",
    "print(\"Parameter shapes:\")\n",
    "for name, layer in params.items():\n",
    "    for pname, p in layer.items():\n",
    "        print(f\"  {name}.{pname}: {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a9a8b",
   "metadata": {},
   "source": [
    "## 2. Define the Forward Pass\n",
    "\n",
    "The model is a pure function: it takes parameters and input, returns output.\n",
    "No side effects, no mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a2cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass test: input [Dim(3), Dim(4)] → output [Dim(3), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "def mlp_forward(params: dict, x):\n",
    "    \"\"\"Pure-function MLP forward pass.\"\"\"\n",
    "    x = x @ params[\"fc1\"][\"weight\"] + params[\"fc1\"][\"bias\"]\n",
    "    x = nb.relu(x)\n",
    "    x = x @ params[\"fc2\"][\"weight\"] + params[\"fc2\"][\"bias\"]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Quick test\n",
    "x_test = nb.uniform((3, 4))\n",
    "y_test = mlp_forward(params, x_test)\n",
    "print(f\"Forward pass test: input {x_test.shape} → output {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d0db8",
   "metadata": {},
   "source": [
    "## 3. Create Data & Define Loss\n",
    "\n",
    "Same synthetic dataset as Example 3a: `y = sin(x0) + cos(x1) + 0.5*x2 - x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72f72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: X [Dim(200), Dim(4)], y [Dim(200), Dim(1)]\n",
      "Initial loss: Tensor(2.5328 : f32[])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_np = np.random.randn(n_samples, 4).astype(np.float32)\n",
    "y_np = (\n",
    "    np.sin(X_np[:, 0])\n",
    "    + np.cos(X_np[:, 1])\n",
    "    + 0.5 * X_np[:, 2]\n",
    "    - X_np[:, 3]\n",
    ").reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "print(f\"Dataset: X {X.shape}, y {y.shape}\")\n",
    "\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    \"\"\"MSE loss as a pure function of params.\"\"\"\n",
    "    predictions = mlp_forward(params, X)\n",
    "    diff = predictions - y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "initial_loss = loss_fn(params, X, y)\n",
    "print(f\"Initial loss: {initial_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ac101",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "The key insight: `value_and_grad(loss_fn, argnums=0)` differentiates w.r.t.\n",
    "the first argument (`params`), which is a dict. It returns gradients with\n",
    "the **same pytree structure** as `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e6a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch    Loss        \n",
      "----------------------\n",
      "10       0.625402    \n",
      "20       0.242037    \n",
      "30       0.159796    \n",
      "40       0.118573    \n",
      "50       0.082741    \n",
      "60       0.064866    \n",
      "70       0.054502    \n",
      "80       0.047352    \n",
      "90       0.041529    \n",
      "100      0.037468    \n"
     ]
    }
   ],
   "source": [
    "opt_state = nb.nn.optim.adamw_init(params)\n",
    "lr = 1e-2\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, X, y)\n",
    "\n",
    "    # grads has the same structure as params:\n",
    "    # grads[\"fc1\"][\"weight\"], grads[\"fc1\"][\"bias\"], etc.\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=lr\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1a497",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8c8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final loss: Tensor(0.0371 : f32[])\n",
      "\n",
      "Sample predictions vs targets:\n",
      "Prediction     Target        \n",
      "----------------------------\n",
      "0.1029         0.2678        \n",
      "0.7215         0.7629        \n",
      "0.7124         0.6380        \n",
      "-0.3226        -0.3964       \n",
      "1.2237         1.0610        \n"
     ]
    }
   ],
   "source": [
    "final_loss = loss_fn(params, X, y)\n",
    "print(f\"\\nFinal loss: {final_loss}\")\n",
    "\n",
    "predictions = mlp_forward(params, X)\n",
    "print(f\"\\nSample predictions vs targets:\")\n",
    "print(f\"{'Prediction':<14} {'Target':<14}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(5):\n",
    "    pred_i = nb.gather(predictions, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    true_i = nb.gather(y, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    print(f\"{pred_i.item():<14.4f} {true_i.item():<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ddf499",
   "metadata": {},
   "source": [
    "## 6. Manual SGD (No Optimizer)\n",
    "\n",
    "The functional style makes it trivial to implement gradient descent manually\n",
    "using `tree_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060ac6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual SGD training:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "20       0.204635    \n",
      "40       0.145309    \n",
      "60       0.117855    \n",
      "80       0.095323    \n",
      "100      0.077824    \n"
     ]
    }
   ],
   "source": [
    "params_sgd = init_mlp_params(4, 32, 1)\n",
    "sgd_lr = 0.05\n",
    "\n",
    "print(f\"\\nManual SGD training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(100):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params_sgd, X, y)\n",
    "\n",
    "    # Manual SGD: params = params - lr * grads\n",
    "    params_sgd = nb.tree_map(\n",
    "        lambda p, g: p - sgd_lr * g, params_sgd, grads\n",
    "    )\n",
    "\n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad6a4a",
   "metadata": {},
   "source": [
    "## PyTorch-Style vs JAX-Style: Comparison\n",
    "\n",
    "| Aspect | PyTorch-style (03a) | JAX-style (03b) |\n",
    "|--------|-------------------|-----------------|\n",
    "| Model | `class MLP(nn.Module)` | `def mlp_forward(params, x)` |\n",
    "| Params | Auto-tracked by Module | Explicit dict (pytree) |\n",
    "| State | Mutable attributes | Immutable, returned from functions |\n",
    "| Optimizer | Can be stateful or functional | Typically functional |\n",
    "| `@nb.compile` | Works with both | Works with both |\n",
    "\n",
    "Both styles are fully supported in Nabla. Choose the one that fits your\n",
    "mental model!\n",
    "\n",
    "**Next:** [04_transforms_and_compile](04_transforms_and_compile)\n",
    "— Advanced transforms (vmap, jacrev, jacfwd) and `@nb.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402af968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Example 03b completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Example 03b completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
