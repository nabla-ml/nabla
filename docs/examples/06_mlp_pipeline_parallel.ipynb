{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c60be0",
   "metadata": {},
   "source": [
    "# Example 6: MLP Pipeline Parallelism (GPipe)\n",
    "\n",
    "This example demonstrates a minimal pipeline-parallel training setup:\n",
    "- 4 pipeline stages and micro-batching\n",
    "- stage-to-stage communication with `ppermute`\n",
    "- gradient validation against a JAX reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1a843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from max.dtype import DType\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec\n",
    "from nabla.core.sharding import PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "# --- Project Constants ---\n",
    "STAGES = 4\n",
    "MICRO_BATCHES = 8\n",
    "MICRO_BATCH_SIZE = 4\n",
    "DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac553",
   "metadata": {},
   "source": [
    "## 1. Define Pipeline Primitives\n",
    "\n",
    "These helpers implement one pipeline step and the full streamed pipeline loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efee855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w, b):\n",
    "    return ops.relu(ops.matmul(x, w) + b)\n",
    "\n",
    "\n",
    "def pipeline_step(\n",
    "    current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract result -> inject input.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result\n",
    "\n",
    "\n",
    "def pipeline_loop(\n",
    "    padded_inputs,\n",
    "    weight_stack,\n",
    "    bias_stack,\n",
    "    current_state,\n",
    "    mask_0,\n",
    "    step_fn,\n",
    "    perm,\n",
    "    total_steps,\n",
    "):\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d4067",
   "metadata": {},
   "source": [
    "## 2. Run Gradient Parity Check\n",
    "\n",
    "Build sharded tensors, compute gradients, and compare against JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69787a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPipe Grads Test on Mesh: @pp = <[\"stage\"=4]>\n",
      "Computing Gradients...\n",
      "Running Reference (JAX)...\n",
      "Max X Grad Diff:      0.000061\n",
      "Max Weight Grad Diff: 0.000122\n",
      "Max Bias Grad Diff:   0.000015\n",
      "✅ SUCCESS: All (Checked) Gradients Match\n"
     ]
    }
   ],
   "source": [
    "def test_pp_grad_with_bias():\n",
    "    mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "    print(f\"Running GPipe Grads Test on Mesh: {mesh}\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "    b_np = np.random.randn(STAGES, DIM).astype(np.float32)\n",
    "    x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "    y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "    w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "    b_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None)]\n",
    "\n",
    "    w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec)\n",
    "    b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec)\n",
    "\n",
    "    padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "    x_padded_nb = nb.Tensor.from_dlpack(np.concatenate([x_np, padding], axis=0))\n",
    "    y_nb = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "    state_sharded = ops.shard(\n",
    "        nb.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=DType.float32), mesh, w_spec\n",
    "    )\n",
    "\n",
    "    mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "    mask_0_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec)\n",
    "\n",
    "    nb.realize_all(w_sharded, b_sharded, state_sharded, mask_0_sharded)\n",
    "\n",
    "    # 3. Communication & VMap Setup\n",
    "    idx = mesh.axis_names.index(\"stage\")\n",
    "    size = mesh.shape[idx]\n",
    "    perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "\n",
    "    # Auto-vectorize the stage calculation over the 'stage' axis\n",
    "    # in_axes=(0, 0, 0) means x, w, and b are all sharded/vmapped over dim 0\n",
    "    step_fn = vmap(\n",
    "        stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"stage\", mesh=mesh\n",
    "    )\n",
    "\n",
    "    # 4. Define Loss Function for Grad\n",
    "    def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
    "        total_steps = MICRO_BATCHES + STAGES\n",
    "        stream_outputs, _ = pipeline_loop(\n",
    "            inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
    "        )\n",
    "\n",
    "        # Slice valid range [STAGES : STAGES+MB] where results start emerging\n",
    "        indices = ops.arange(STAGES, STAGES + MICRO_BATCHES, dtype=DType.int64)\n",
    "        valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
    "\n",
    "        # MSE Loss\n",
    "        diff = valid_preds - targets\n",
    "        return ops.mean(diff * diff)\n",
    "\n",
    "    print(\"Computing Gradients...\")\n",
    "    from nabla.core.transforms import grad\n",
    "\n",
    "    grad_fn = grad(pipeline_loss, argnums=(0, 1, 2), realize=False)\n",
    "    x_grad_sharded, w_grad_sharded, b_grad_sharded = grad_fn(\n",
    "        x_padded_nb, w_sharded, b_sharded, state_sharded, mask_0_sharded, y_nb\n",
    "    )\n",
    "\n",
    "    x_grad_all_np, w_grad_np, b_grad_np = nb.Tensor.to_numpy_all(\n",
    "        x_grad_sharded, w_grad_sharded, b_grad_sharded\n",
    "    )\n",
    "    x_grad_np = x_grad_all_np[:MICRO_BATCHES]\n",
    "\n",
    "    print(\"Running Reference (JAX)...\")\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "    def jax_ref(x, params_w, params_b, y):\n",
    "        def apply(curr, w, b):\n",
    "            return jax.nn.relu(curr @ w + b)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(MICRO_BATCHES):\n",
    "            a = x[i]\n",
    "            for w, b in zip(params_w, params_b, strict=False):\n",
    "                a = apply(a, w, b)\n",
    "            preds.append(a)\n",
    "        preds = jnp.stack(preds)\n",
    "        return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "    grad_ref_fn = jax.jit(jax.grad(jax_ref, argnums=(0, 1, 2)))\n",
    "    x_grad_ref, w_grad_ref, b_grad_ref = grad_ref_fn(x_np, w_np, b_np, y_np)\n",
    "\n",
    "    x_diff = np.max(np.abs(x_grad_np - x_grad_ref)) if x_grad_np is not None else 0.0\n",
    "\n",
    "    w_diff = np.max(np.abs(w_grad_np - w_grad_ref))\n",
    "    b_diff = np.max(np.abs(b_grad_np - b_grad_ref))\n",
    "\n",
    "    if x_grad_np is not None:\n",
    "        print(f\"Max X Grad Diff:      {x_diff:.6f}\")\n",
    "    else:\n",
    "        print(\"Max X Grad Diff:      N/A\")\n",
    "\n",
    "    print(f\"Max Weight Grad Diff: {w_diff:.6f}\")\n",
    "    print(f\"Max Bias Grad Diff:   {b_diff:.6f}\")\n",
    "\n",
    "    passed = (w_diff < 5e-4) and (b_diff < 5e-4)\n",
    "    if x_grad_np is not None:\n",
    "        passed = passed and (x_diff < 5e-4)\n",
    "\n",
    "    if passed:\n",
    "        print(\"✅ SUCCESS: All (Checked) Gradients Match\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE: Gradients Mismatch\")\n",
    "        if w_diff >= 5e-4:\n",
    "            print(\"Weight Grad Mismatch!\")\n",
    "        if b_diff >= 5e-4:\n",
    "            print(\"Bias Grad Mismatch!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pp_grad_with_bias()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
