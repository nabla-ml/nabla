{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c60be0",
   "metadata": {},
   "source": [
    "# Example 6: Pipeline Parallelism (GPipe)\n",
    "\n",
    "**Pipeline parallelism** splits a model into sequential stages across devices.\n",
    "Instead of running each input through all stages sequentially, we overlap\n",
    "computation by feeding new micro-batches into the pipeline while earlier ones\n",
    "are still being processed at later stages.\n",
    "\n",
    "```\n",
    "Time →  t0    t1    t2    t3    t4    t5    t6\n",
    "Stage 0  [x0]  [x1]  [x2]  [x3]   ·     ·     ·\n",
    "Stage 1   ·    [x0]  [x1]  [x2]  [x3]   ·     ·\n",
    "Stage 2   ·     ·    [x0]  [x1]  [x2]  [x3]   ·\n",
    "Stage 3   ·     ·     ·    [x0]  [x1]  [x2]  [x3]\n",
    "                                  ↑ results start emerging\n",
    "```\n",
    "\n",
    "In this example we'll:\n",
    "1. Shard an MLP across 4 pipeline stages\n",
    "2. Use `ppermute` for stage-to-stage communication\n",
    "3. Compute gradients through the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1a843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Pipeline Parallelism example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec, PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "print(\"Nabla Pipeline Parallelism example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac553",
   "metadata": {},
   "source": [
    "## 1. Setup: Model and Device Mesh\n",
    "\n",
    "We'll use a simple 4-layer MLP, with **each layer on a separate pipeline stage**.\n",
    "A `DeviceMesh` defines the logical layout of devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "STAGES = 4           # Number of pipeline stages (= devices)\n",
    "MICRO_BATCHES = 8    # Number of micro-batches to stream through\n",
    "MICRO_BATCH_SIZE = 4 # Samples per micro-batch\n",
    "DIM = 16             # Hidden dimension\n",
    "\n",
    "# Create a 1D device mesh for pipeline parallelism\n",
    "mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "print(f\"Device mesh: {STAGES} stages\")\n",
    "print(f\"Pipeline: {MICRO_BATCHES} micro-batches × {MICRO_BATCH_SIZE} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cc930",
   "metadata": {},
   "source": [
    "## 2. Pipeline Primitives\n",
    "\n",
    "Each pipeline stage applies one linear layer followed by ReLU.\n",
    "We define three small helpers:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `stage_compute` | Apply one stage's weight + bias → ReLU |\n",
    "| `pipeline_step` | One tick: compute → shift → inject next micro-batch |\n",
    "| `pipeline_loop` | Iterate steps, collecting outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w, b):\n",
    "    \"\"\"One pipeline stage: linear + ReLU.\"\"\"\n",
    "    return ops.relu(ops.matmul(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3faf9",
   "metadata": {},
   "source": [
    "`pipeline_step` is the core of GPipe: after computing all stages in parallel,\n",
    "`ppermute` shifts outputs one stage forward (stage 0→1, 1→2, ...).\n",
    "The last stage's result is extracted via a mask, and the fresh micro-batch is\n",
    "injected into stage 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6020d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_step(\n",
    "    current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract result -> inject input.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    # Extract the final stage's output (mask selects stage 0 after the shift)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    # Inject the fresh micro-batch at stage 0, pass shifted activations elsewhere\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d8e7",
   "metadata": {},
   "source": [
    "The **pipeline loop** feeds `MICRO_BATCHES + STAGES` ticks through the pipeline.\n",
    "During the first `STAGES - 1` ticks the pipeline is \"filling up\"; results\n",
    "start emerging at tick `STAGES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_loop(\n",
    "    padded_inputs, weight_stack, bias_stack, current_state, mask_0,\n",
    "    step_fn, perm, total_steps,\n",
    "):\n",
    "    \"\"\"Stream micro-batches through the pipeline for `total_steps` ticks.\"\"\"\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d4067",
   "metadata": {},
   "source": [
    "## 2. Run Gradient Parity Check\n",
    "\n",
    "Build sharded tensors, compute gradients, and compare against JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69787a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STAGES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBias Grad Mismatch!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43mtest_pp_grad_with_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_pp_grad_with_bias\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_pp_grad_with_bias\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     mesh = DeviceMesh(\u001b[33m\"\u001b[39m\u001b[33mpp\u001b[39m\u001b[33m\"\u001b[39m, (\u001b[43mSTAGES\u001b[49m,), (\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m,))\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning GPipe Grads Test on Mesh: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmesh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     np.random.seed(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'STAGES' is not defined"
     ]
    }
   ],
   "source": [
    "def test_pp_grad_with_bias():\n",
    "    mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "    print(f\"Running GPipe Grads Test on Mesh: {mesh}\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "    b_np = np.random.randn(STAGES, DIM).astype(np.float32)\n",
    "    x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "    y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "    w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "    b_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None)]\n",
    "\n",
    "    w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec)\n",
    "    b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec)\n",
    "\n",
    "    padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "    x_padded_nb = nb.Tensor.from_dlpack(np.concatenate([x_np, padding], axis=0))\n",
    "    y_nb = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "    state_sharded = ops.shard(\n",
    "        nb.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=DType.float32), mesh, w_spec\n",
    "    )\n",
    "\n",
    "    mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "    mask_0_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec)\n",
    "\n",
    "    nb.realize_all(w_sharded, b_sharded, state_sharded, mask_0_sharded)\n",
    "\n",
    "    # 3. Communication & VMap Setup\n",
    "    idx = mesh.axis_names.index(\"stage\")\n",
    "    size = mesh.shape[idx]\n",
    "    perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "\n",
    "    # Auto-vectorize the stage calculation over the 'stage' axis\n",
    "    # in_axes=(0, 0, 0) means x, w, and b are all sharded/vmapped over dim 0\n",
    "    step_fn = vmap(\n",
    "        stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"stage\", mesh=mesh\n",
    "    )\n",
    "\n",
    "    # 4. Define Loss Function for Grad\n",
    "    def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
    "        total_steps = MICRO_BATCHES + STAGES\n",
    "        stream_outputs, _ = pipeline_loop(\n",
    "            inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
    "        )\n",
    "\n",
    "        # Slice valid range [STAGES : STAGES+MB] where results start emerging\n",
    "        indices = ops.arange(STAGES, STAGES + MICRO_BATCHES, dtype=DType.int64)\n",
    "        valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
    "\n",
    "        # MSE Loss\n",
    "        diff = valid_preds - targets\n",
    "        return ops.mean(diff * diff)\n",
    "\n",
    "    print(\"Computing Gradients...\")\n",
    "    from nabla.core.transforms import grad\n",
    "\n",
    "    grad_fn = grad(pipeline_loss, argnums=(0, 1, 2), realize=False)\n",
    "    x_grad_sharded, w_grad_sharded, b_grad_sharded = grad_fn(\n",
    "        x_padded_nb, w_sharded, b_sharded, state_sharded, mask_0_sharded, y_nb\n",
    "    )\n",
    "\n",
    "    x_grad_all_np, w_grad_np, b_grad_np = nb.Tensor.to_numpy_all(\n",
    "        x_grad_sharded, w_grad_sharded, b_grad_sharded\n",
    "    )\n",
    "    x_grad_np = x_grad_all_np[:MICRO_BATCHES]\n",
    "\n",
    "    print(\"Running Reference (JAX)...\")\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "    def jax_ref(x, params_w, params_b, y):\n",
    "        def apply(curr, w, b):\n",
    "            return jax.nn.relu(curr @ w + b)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(MICRO_BATCHES):\n",
    "            a = x[i]\n",
    "            for w, b in zip(params_w, params_b, strict=False):\n",
    "                a = apply(a, w, b)\n",
    "            preds.append(a)\n",
    "        preds = jnp.stack(preds)\n",
    "        return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "    grad_ref_fn = jax.jit(jax.grad(jax_ref, argnums=(0, 1, 2)))\n",
    "    x_grad_ref, w_grad_ref, b_grad_ref = grad_ref_fn(x_np, w_np, b_np, y_np)\n",
    "\n",
    "    x_diff = np.max(np.abs(x_grad_np - x_grad_ref)) if x_grad_np is not None else 0.0\n",
    "\n",
    "    w_diff = np.max(np.abs(w_grad_np - w_grad_ref))\n",
    "    b_diff = np.max(np.abs(b_grad_np - b_grad_ref))\n",
    "\n",
    "    if x_grad_np is not None:\n",
    "        print(f\"Max X Grad Diff:      {x_diff:.6f}\")\n",
    "    else:\n",
    "        print(\"Max X Grad Diff:      N/A\")\n",
    "\n",
    "    print(f\"Max Weight Grad Diff: {w_diff:.6f}\")\n",
    "    print(f\"Max Bias Grad Diff:   {b_diff:.6f}\")\n",
    "\n",
    "    passed = (w_diff < 5e-4) and (b_diff < 5e-4)\n",
    "    if x_grad_np is not None:\n",
    "        passed = passed and (x_diff < 5e-4)\n",
    "\n",
    "    if passed:\n",
    "        print(\"✅ SUCCESS: All (Checked) Gradients Match\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE: Gradients Mismatch\")\n",
    "        if w_diff >= 5e-4:\n",
    "            print(\"Weight Grad Mismatch!\")\n",
    "        if b_diff >= 5e-4:\n",
    "            print(\"Bias Grad Mismatch!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pp_grad_with_bias()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
