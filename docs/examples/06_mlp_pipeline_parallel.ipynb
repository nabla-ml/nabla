{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c60be0",
   "metadata": {},
   "source": [
    "# Example 6: Pipeline Parallelism (GPipe)\n",
    "\n",
    "**Pipeline parallelism** splits a model into sequential stages across devices.\n",
    "Instead of running each input through all stages sequentially, we overlap\n",
    "computation by feeding new micro-batches into the pipeline while earlier ones\n",
    "are still being processed at later stages.\n",
    "\n",
    "```\n",
    "Time →  t0    t1    t2    t3    t4    t5    t6\n",
    "Stage 0  [x0]  [x1]  [x2]  [x3]   ·     ·     ·\n",
    "Stage 1   ·    [x0]  [x1]  [x2]  [x3]   ·     ·\n",
    "Stage 2   ·     ·    [x0]  [x1]  [x2]  [x3]   ·\n",
    "Stage 3   ·     ·     ·    [x0]  [x1]  [x2]  [x3]\n",
    "                                  ↑ results start emerging\n",
    "```\n",
    "\n",
    "In this example we'll:\n",
    "1. Shard an MLP across 4 pipeline stages\n",
    "2. Use `ppermute` for stage-to-stage communication\n",
    "3. Compute gradients through the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1a843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Pipeline Parallelism example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec, PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "print(\"Nabla Pipeline Parallelism example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac553",
   "metadata": {},
   "source": [
    "## 1. Setup: Model and Device Mesh\n",
    "\n",
    "We'll use a simple 4-layer MLP, with **each layer on a separate pipeline stage**.\n",
    "A `DeviceMesh` defines the logical layout of devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "STAGES = 4           # Number of pipeline stages (= devices)\n",
    "MICRO_BATCHES = 8    # Number of micro-batches to stream through\n",
    "MICRO_BATCH_SIZE = 4 # Samples per micro-batch\n",
    "DIM = 16             # Hidden dimension\n",
    "\n",
    "# Create a 1D device mesh for pipeline parallelism\n",
    "mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "print(f\"Device mesh: {STAGES} stages\")\n",
    "print(f\"Pipeline: {MICRO_BATCHES} micro-batches × {MICRO_BATCH_SIZE} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cc930",
   "metadata": {},
   "source": [
    "## 2. Pipeline Primitives\n",
    "\n",
    "Each pipeline stage applies one linear layer followed by ReLU.\n",
    "We define three small helpers:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `stage_compute` | Apply one stage's weight + bias → ReLU |\n",
    "| `pipeline_step` | One tick: compute → shift → inject next micro-batch |\n",
    "| `pipeline_loop` | Iterate steps, collecting outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w, b):\n",
    "    \"\"\"One pipeline stage: linear + ReLU.\"\"\"\n",
    "    return ops.relu(ops.matmul(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3faf9",
   "metadata": {},
   "source": [
    "`pipeline_step` is the core of GPipe: after computing all stages in parallel,\n",
    "`ppermute` shifts outputs one stage forward (stage 0→1, 1→2, ...).\n",
    "The last stage's result is extracted via a mask, and the fresh micro-batch is\n",
    "injected into stage 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6020d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_step(\n",
    "    current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract result -> inject input.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    # Extract the final stage's output (mask selects stage 0 after the shift)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    # Inject the fresh micro-batch at stage 0, pass shifted activations elsewhere\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d8e7",
   "metadata": {},
   "source": [
    "The **pipeline loop** feeds `MICRO_BATCHES + STAGES` ticks through the pipeline.\n",
    "During the first `STAGES - 1` ticks the pipeline is \"filling up\"; results\n",
    "start emerging at tick `STAGES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_loop(\n",
    "    padded_inputs, weight_stack, bias_stack, current_state, mask_0,\n",
    "    step_fn, perm, total_steps,\n",
    "):\n",
    "    \"\"\"Stream micro-batches through the pipeline for `total_steps` ticks.\"\"\"\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d4067",
   "metadata": {},
   "source": [
    "## 3. Shard Data Across Stages\n",
    "\n",
    "Each weight matrix lives on one stage. We use `ops.shard` with a\n",
    "`PartitionSpec` to place the first dimension on the `\"stage\"` mesh axis.\n",
    "We also need zero-padded inputs (the pipeline needs `STAGES` empty ticks\n",
    "to fill up) and a boolean mask that selects stage 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69787a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STAGES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBias Grad Mismatch!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43mtest_pp_grad_with_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_pp_grad_with_bias\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_pp_grad_with_bias\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     mesh = DeviceMesh(\u001b[33m\"\u001b[39m\u001b[33mpp\u001b[39m\u001b[33m\"\u001b[39m, (\u001b[43mSTAGES\u001b[49m,), (\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m,))\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning GPipe Grads Test on Mesh: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmesh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     np.random.seed(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'STAGES' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Random weights (one per stage), inputs, and targets\n",
    "w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "b_np = np.random.randn(STAGES, DIM).astype(np.float32)\n",
    "x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "# Shard weights: first axis → \"stage\" mesh axis\n",
    "w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "b_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None)]\n",
    "\n",
    "w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec)\n",
    "b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec)\n",
    "\n",
    "# Pad inputs with STAGES zero-slices for pipeline warm-up\n",
    "padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "x_padded = nb.Tensor.from_dlpack(np.concatenate([x_np, padding], axis=0))\n",
    "y_nb = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "# Initial pipeline state: zeros on each stage\n",
    "state_sharded = ops.shard(\n",
    "    nb.zeros((STAGES, MICRO_BATCH_SIZE, DIM)), mesh, w_spec\n",
    ")\n",
    "\n",
    "# Stage-0 mask for injecting fresh inputs\n",
    "mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "mask_0 = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec)\n",
    "\n",
    "nb.realize_all(w_sharded, b_sharded, state_sharded, mask_0)\n",
    "print(f\"Weights sharded: {w_sharded.shape}, Inputs padded: {x_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691456b9",
   "metadata": {},
   "source": [
    "## 4. Communication & Vectorized Stages\n",
    "\n",
    "`ppermute` shifts tensors between devices according to a permutation list.\n",
    "For a 4-stage pipeline, stage *i* sends its output to stage *i+1* (with wrap-around):\n",
    "\n",
    "```\n",
    "perm = [(0,1), (1,2), (2,3), (3,0)]\n",
    "```\n",
    "\n",
    "We then use `vmap` with `spmd_axis_name=\"stage\"` to auto-vectorize\n",
    "`stage_compute` over the stage dimension — each stage computes with its\n",
    "own weight/bias slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the circular permutation for ppermute\n",
    "idx = mesh.axis_names.index(\"stage\")\n",
    "size = mesh.shape[idx]\n",
    "perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "print(f\"ppermute permutation: {perm}\")\n",
    "\n",
    "# Vectorize stage_compute over the stage axis\n",
    "step_fn = vmap(\n",
    "    stage_compute,\n",
    "    in_axes=(0, 0, 0),\n",
    "    out_axes=0,\n",
    "    spmd_axis_name=\"stage\",\n",
    "    mesh=mesh,\n",
    ")\n",
    "print(\"step_fn ready — each stage runs its own weights in parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2b9b4",
   "metadata": {},
   "source": [
    "## 5. Define the Pipeline Loss\n",
    "\n",
    "The loss function runs the full pipeline loop, slices out the valid outputs\n",
    "(the first `STAGES` ticks produce incomplete results), and computes MSE\n",
    "against the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
    "    \"\"\"MSE loss through the full GPipe pipeline.\"\"\"\n",
    "    total_steps = MICRO_BATCHES + STAGES\n",
    "    stream_outputs, _ = pipeline_loop(\n",
    "        inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
    "    )\n",
    "\n",
    "    # Slice valid range [STAGES : STAGES + MICRO_BATCHES]\n",
    "    indices = ops.arange(STAGES, STAGES + MICRO_BATCHES)\n",
    "    valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
    "\n",
    "    # MSE loss\n",
    "    diff = valid_preds - targets\n",
    "    return ops.mean(diff * diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0722dc",
   "metadata": {},
   "source": [
    "## 6. Compute Gradients Through the Pipeline\n",
    "\n",
    "`nb.grad` differentiates through the entire pipeline — including `ppermute`\n",
    "shifts and per-stage `vmap` — computing gradients for inputs, weights, and\n",
    "biases simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = nb.grad(pipeline_loss, argnums=(0, 1, 2), realize=False)\n",
    "\n",
    "x_grad, w_grad, b_grad = grad_fn(\n",
    "    x_padded, w_sharded, b_sharded, state_sharded, mask_0, y_nb\n",
    ")\n",
    "\n",
    "# Materialize results as numpy arrays\n",
    "x_grad_np, w_grad_np, b_grad_np = nb.Tensor.to_numpy_all(x_grad, w_grad, b_grad)\n",
    "x_grad_np = x_grad_np[:MICRO_BATCHES]  # trim padding region\n",
    "\n",
    "print(f\"Input gradient shape:  {x_grad_np.shape}\")\n",
    "print(f\"Weight gradient shape: {w_grad_np.shape}\")\n",
    "print(f\"Bias gradient shape:   {b_grad_np.shape}\")\n",
    "print(f\"Weight grad range:     [{w_grad_np.min():.4f}, {w_grad_np.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd231e1",
   "metadata": {},
   "source": [
    "## 7. Verify Against JAX Reference\n",
    "\n",
    "As a sanity check, we run the same computation sequentially in JAX and\n",
    "compare gradients. The pipeline scheduling should not change the mathematical\n",
    "result — only how computation is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "    def jax_ref(x, params_w, params_b, y):\n",
    "        def apply(curr, w, b):\n",
    "            return jax.nn.relu(curr @ w + b)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(MICRO_BATCHES):\n",
    "            a = x[i]\n",
    "            for w, b in zip(params_w, params_b, strict=False):\n",
    "                a = apply(a, w, b)\n",
    "            preds.append(a)\n",
    "        preds = jnp.stack(preds)\n",
    "        return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "    grad_ref_fn = jax.jit(jax.grad(jax_ref, argnums=(0, 1, 2)))\n",
    "    x_grad_ref, w_grad_ref, b_grad_ref = grad_ref_fn(x_np, w_np, b_np, y_np)\n",
    "\n",
    "    x_diff = np.max(np.abs(x_grad_np - x_grad_ref))\n",
    "    w_diff = np.max(np.abs(w_grad_np - w_grad_ref))\n",
    "    b_diff = np.max(np.abs(b_grad_np - b_grad_ref))\n",
    "\n",
    "    print(f\"Max input grad diff:  {x_diff:.6f}\")\n",
    "    print(f\"Max weight grad diff: {w_diff:.6f}\")\n",
    "    print(f\"Max bias grad diff:   {b_diff:.6f}\")\n",
    "    assert w_diff < 5e-4 and b_diff < 5e-4 and x_diff < 5e-4, \"Gradient mismatch!\"\n",
    "    print(\"✅ Nabla pipeline gradients match JAX sequential reference\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"JAX not installed — skipping reference comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b67ebf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- `DeviceMesh` + `PartitionSpec` place tensors on specific stages\n",
    "- `ppermute` handles inter-stage communication without explicit send/recv\n",
    "- `vmap` with `spmd_axis_name` vectorizes computation across stages\n",
    "- `nb.grad` differentiates through the entire sharded pipeline\n",
    "\n",
    "**Previous:** [05b — Transformer (JAX-style)](05b_transformer_jax.ipynb) · **Next:** [07 — Pipeline + Data Parallelism](07_mlp_pp_dp_training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
