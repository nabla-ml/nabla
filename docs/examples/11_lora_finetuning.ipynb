{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212f30d7",
   "metadata": {},
   "source": [
    "# Example 10: LoRA & QLoRA Fine-Tuning\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) trains a small adapter instead of full weights:\n",
    "\n",
    "$$W_{\\text{effective}} = W_{\\text{frozen}} + \\frac{\\alpha}{r} \\cdot B \\cdot A$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times d}$ are\n",
    "the trainable low-rank factors, $r \\ll d$.\n",
    "\n",
    "**QLoRA** goes further: the frozen weight $W$ is quantized to NF4 (4 bits per\n",
    "parameter), saving ~75% memory while keeping full-precision adapters.\n",
    "\n",
    "In this example we'll:\n",
    "1. Train a LoRA adapter on a synthetic regression task\n",
    "2. Save and reload a finetune checkpoint\n",
    "3. Quantize weights to NF4 and train QLoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce174fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla LoRA/QLoRA Fine-Tuning example\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla LoRA/QLoRA Fine-Tuning example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6ac4",
   "metadata": {},
   "source": [
    "## 1. Synthetic Regression Data\n",
    "\n",
    "We create a dataset where the true weight is $W_{\\text{base}} + \\Delta$,\n",
    "where $\\Delta$ is a rank-4 perturbation. The adapter should learn to approximate $\\Delta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471809ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 512 samples, in=64, out=32\n",
      "LoRA rank=8, alpha=16.0\n"
     ]
    }
   ],
   "source": [
    "def make_regression_data(n_samples, in_dim, out_dim, seed=123, delta_scale=0.35):\n",
    "    \"\"\"Generate synthetic linear regression data with a low-rank perturbation.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.normal(size=(n_samples, in_dim)).astype(np.float32)\n",
    "    w_base = rng.normal(size=(in_dim, out_dim)).astype(np.float32) * 0.5\n",
    "    # Low-rank perturbation (rank 4)\n",
    "    u = rng.normal(size=(in_dim, 4)).astype(np.float32)\n",
    "    v = rng.normal(size=(4, out_dim)).astype(np.float32)\n",
    "    delta = delta_scale * (u @ v)\n",
    "    y = x @ (w_base + delta)\n",
    "    return x, y.astype(np.float32), w_base.astype(np.float32)\n",
    "\n",
    "# Shared hyperparameters\n",
    "IN_DIM, OUT_DIM = 64, 32\n",
    "RANK = 8\n",
    "ALPHA = 16.0\n",
    "LR = 2e-2\n",
    "STEPS = 120\n",
    "\n",
    "x_np, y_np, w_base_np = make_regression_data(512, IN_DIM, OUT_DIM)\n",
    "x = nb.Tensor.from_dlpack(x_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "frozen_weight = nb.Tensor.from_dlpack(w_base_np)\n",
    "\n",
    "print(f\"Data: {x_np.shape[0]} samples, in={IN_DIM}, out={OUT_DIM}\")\n",
    "print(f\"LoRA rank={RANK}, alpha={ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba9d1f",
   "metadata": {},
   "source": [
    "## 2. LoRA Training\n",
    "\n",
    "Initialize a LoRA adapter and train with AdamW.\n",
    "`nb.nn.finetune.lora_linear` computes $xW + \\frac{\\alpha}{r} \\cdot x A B$\n",
    "in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f6d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 32.337105\n",
      "Step  50: loss = 0.199841\n",
      "Step 100: loss = 0.002403\n",
      "Final loss:   0.000900\n",
      "✅ LoRA adapter trained successfully\n"
     ]
    }
   ],
   "source": [
    "lora_params = nb.nn.finetune.init_lora_adapter(frozen_weight, rank=RANK, init_std=0.01)\n",
    "opt_state = nb.nn.optim.adamw_init(lora_params)\n",
    "\n",
    "def lora_loss_fn(adapter, batch_x, batch_y):\n",
    "    \"\"\"MSE loss with LoRA-adapted linear layer.\"\"\"\n",
    "    pred = nb.nn.finetune.lora_linear(batch_x, frozen_weight, adapter, alpha=ALPHA)\n",
    "    diff = pred - batch_y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "def train_step(loss_fn, adapter, optimizer_state, batch_x, batch_y):\n",
    "    \"\"\"One training step: forward + backward + AdamW update.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0, realize=False)(\n",
    "        adapter, batch_x, batch_y\n",
    "    )\n",
    "    new_adapter, new_state = nb.nn.optim.adamw_update(\n",
    "        adapter, grads, optimizer_state, lr=LR, weight_decay=0.0\n",
    "    )\n",
    "    # Batch-realize all tensors\n",
    "    to_realize = [loss]\n",
    "    to_realize.extend(t for t in nb.tree_leaves(grads) if isinstance(t, nb.Tensor))\n",
    "    to_realize.extend(t for t in nb.tree_leaves(new_adapter) if isinstance(t, nb.Tensor))\n",
    "    to_realize.extend(t for t in nb.tree_leaves(new_state) if isinstance(t, nb.Tensor))\n",
    "    nb.realize_all(*to_realize)\n",
    "    return loss, new_adapter, new_state\n",
    "\n",
    "initial_loss = float(lora_loss_fn(lora_params, x, y).to_numpy())\n",
    "print(f\"Initial loss: {initial_loss:.6f}\")\n",
    "\n",
    "for step in range(STEPS):\n",
    "    loss, lora_params, opt_state = train_step(lora_loss_fn, lora_params, opt_state, x, y)\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step + 1:>3d}: loss = {float(loss.to_numpy()):.6f}\")\n",
    "\n",
    "final_loss = float(lora_loss_fn(lora_params, x, y).to_numpy())\n",
    "print(f\"Final loss:   {final_loss:.6f}\")\n",
    "assert final_loss < initial_loss, \"LoRA training did not reduce loss\"\n",
    "print(\"✅ LoRA adapter trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cf7dd",
   "metadata": {},
   "source": [
    "## 3. Save and Load Checkpoint\n",
    "\n",
    "Nabla provides checkpoint utilities for LoRA adapters. We save the trained\n",
    "adapter + optimizer state, reload them, and verify predictions match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171820b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint max prediction diff: 0.00000000\n",
      "✅ Checkpoint roundtrip verified\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = Path(\".tmp_lora_ckpt\")\n",
    "if ckpt_dir.exists():\n",
    "    shutil.rmtree(ckpt_dir)\n",
    "\n",
    "# Save checkpoint\n",
    "nb.nn.finetune.save_finetune_checkpoint(\n",
    "    ckpt_dir,\n",
    "    lora_params=lora_params,\n",
    "    optimizer_state=opt_state,\n",
    "    metadata={\"alpha\": ALPHA, \"rank\": RANK},\n",
    ")\n",
    "\n",
    "# Reload from checkpoint\n",
    "lora_template = nb.nn.finetune.init_lora_adapter(frozen_weight, rank=RANK, init_std=0.01)\n",
    "opt_template = nb.nn.optim.adamw_init(lora_template)\n",
    "loaded_lora, loaded_opt, meta = nb.nn.finetune.load_finetune_checkpoint(\n",
    "    ckpt_dir, lora_template=lora_template, optimizer_template=opt_template,\n",
    ")\n",
    "\n",
    "# Verify predictions match\n",
    "original_pred = nb.nn.finetune.lora_linear(x, frozen_weight, lora_params, alpha=ALPHA)\n",
    "loaded_pred = nb.nn.finetune.lora_linear(x, frozen_weight, loaded_lora, alpha=ALPHA)\n",
    "max_diff = np.max(np.abs(original_pred.to_numpy() - loaded_pred.to_numpy()))\n",
    "\n",
    "print(f\"Checkpoint max prediction diff: {max_diff:.8f}\")\n",
    "assert max_diff < 1e-5, f\"Checkpoint mismatch: {max_diff}\"\n",
    "print(\"✅ Checkpoint roundtrip verified\")\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(ckpt_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56798a4a",
   "metadata": {},
   "source": [
    "## 4. QLoRA: Quantized Base Weights\n",
    "\n",
    "**QLoRA** quantizes the frozen weight $W$ to NF4 (4-bit Normal Float).\n",
    "During the forward pass, $W$ is dequantized on-the-fly and combined with\n",
    "the LoRA adapter. This saves ~75% memory for the base weight while the\n",
    "adapter remains in full precision.\n",
    "\n",
    "First, let's check quantization quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b801f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4 relative reconstruction error: 0.0912\n",
      "Quantized weight type: dict\n"
     ]
    }
   ],
   "source": [
    "qweight = nb.nn.finetune.quantize_nf4(frozen_weight, block_size=64)\n",
    "dense_recon = nb.nn.finetune.dequantize_nf4(qweight)\n",
    "quant_err = float(\n",
    "    np.linalg.norm(dense_recon.to_numpy() - frozen_weight.to_numpy())\n",
    "    / (np.linalg.norm(frozen_weight.to_numpy()) + 1e-8)\n",
    ")\n",
    "print(f\"NF4 relative reconstruction error: {quant_err:.4f}\")\n",
    "print(f\"Quantized weight type: {type(qweight).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b21701",
   "metadata": {},
   "source": [
    "### QLoRA Training\n",
    "\n",
    "Training is identical to LoRA except we use `qlora_linear` instead of\n",
    "`lora_linear`. The quantized weight is dequantized during the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a041fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA initial loss: 32.562389\n",
      "Step  50: loss = 0.301735\n",
      "Step 100: loss = 0.095010\n",
      "QLoRA final loss:   0.090136\n",
      "✅ QLoRA adapter trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Fresh adapter for QLoRA\n",
    "qlora_params = nb.nn.finetune.init_lora_adapter(frozen_weight, rank=RANK, init_std=0.01)\n",
    "qopt_state = nb.nn.optim.adamw_init(qlora_params)\n",
    "\n",
    "def qlora_loss_fn(adapter, batch_x, batch_y):\n",
    "    \"\"\"MSE loss with QLoRA-adapted linear layer.\"\"\"\n",
    "    pred = nb.nn.finetune.qlora_linear(\n",
    "        batch_x, qweight, adapter, alpha=ALPHA, compute_dtype=nb.DType.float32\n",
    "    )\n",
    "    diff = pred - batch_y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "q_initial_loss = float(qlora_loss_fn(qlora_params, x, y).to_numpy())\n",
    "print(f\"QLoRA initial loss: {q_initial_loss:.6f}\")\n",
    "\n",
    "for step in range(STEPS):\n",
    "    loss, qlora_params, qopt_state = train_step(qlora_loss_fn, qlora_params, qopt_state, x, y)\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step + 1:>3d}: loss = {float(loss.to_numpy()):.6f}\")\n",
    "\n",
    "q_final_loss = float(qlora_loss_fn(qlora_params, x, y).to_numpy())\n",
    "print(f\"QLoRA final loss:   {q_final_loss:.6f}\")\n",
    "assert q_final_loss < q_initial_loss, \"QLoRA training did not reduce loss\"\n",
    "print(\"✅ QLoRA adapter trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad48e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- LoRA trains only $2 \\times r \\times d$ parameters instead of $d^2$\n",
    "- QLoRA adds NF4 quantization for ~4x memory reduction on frozen weights\n",
    "- Both methods converge on our synthetic task with negligible quality loss\n",
    "- Checkpointing supports adapter + optimizer state + custom metadata\n",
    "\n",
    "**Previous:** [09 — Compiled vs Eager vs JAX](09_jax_comparison_compiled.ipynb) · **Next:** [11 — Custom Mojo Kernels](11_custom_mojo_kernels.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
