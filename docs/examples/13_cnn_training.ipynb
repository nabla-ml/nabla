{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66eb2bd6",
   "metadata": {},
   "source": [
    "# Example 13: CNN Training\n",
    "\n",
    "This notebook trains a small 2D CNN on a synthetic regression task\n",
    "and shows the same training step written **twice** — eager and compiled —\n",
    "then benchmarks them head-to-head.\n",
    "\n",
    "| Version | Code | When to use |\n",
    "|---------|------|-------------|\n",
    "| **Eager** (baseline) | plain function | debugging, single shots |\n",
    "| **Compiled** | `@nb.compile` | repeated training loops |\n",
    "\n",
    "**CNN architecture:**\n",
    "- Stage 1: `conv2d (1→8 ch, 3×3) + ReLU + avg_pool2d 2×2`\n",
    "- Stage 2: `conv2d (8→16 ch, 3×3) + ReLU + max_pool2d 2×2`\n",
    "- Head: flatten → `matmul + bias + ReLU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d54483",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1df6c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla CNN — Eager vs. Compiled\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla CNN — Eager vs. Compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6e02d",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset\n",
    "\n",
    "Task: predict the **mean squared activation of the center 8×8 patch** of a 16×16 grayscale image.\n",
    "This gives a cheap, differentiable regression target that depends on a spatial crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5167eafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  [Dim(64), Dim(16), Dim(16), Dim(1)]\n",
      "Targets: [Dim(64), Dim(1), Dim(1), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(seed: int = 0, batch_size: int = 64):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.normal(size=(batch_size, 16, 16, 1)).astype(np.float32)\n",
    "    center = x[:, 4:12, 4:12, :]\n",
    "    y = np.mean(center ** 2, axis=(1, 2, 3), keepdims=True).astype(np.float32)\n",
    "    return (\n",
    "        nb.Tensor.from_dlpack(x),\n",
    "        nb.Tensor.from_dlpack(y),\n",
    "    )\n",
    "\n",
    "\n",
    "X, Y = make_dataset(seed=0)\n",
    "print(f\"Inputs:  {X.shape}\")\n",
    "print(f\"Targets: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5f3cc",
   "metadata": {},
   "source": [
    "## 3. CNN Architecture\n",
    "\n",
    "The model is a **pure function over a flat parameter list** — no module classes, no hidden state.\n",
    "This is Nabla's functional API, analogous to JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36de721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn(x, params) defined\n"
     ]
    }
   ],
   "source": [
    "def cnn(x: nb.Tensor, params: list[nb.Tensor]) -> nb.Tensor:\n",
    "    w1, b1, w2, b2, wh, bh = params\n",
    "\n",
    "    # Stage 1: conv + ReLU + avg pool\n",
    "    y = nb.relu(nb.conv2d(x, w1, bias=b1, stride=(1, 1), padding=(1, 1, 1, 1)))\n",
    "    y = nb.avg_pool2d(y, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "    # Stage 2: conv + ReLU + max pool\n",
    "    y = nb.relu(nb.conv2d(y, w2, bias=b2, stride=(1, 1), padding=(1, 1, 1, 1)))\n",
    "    y = nb.max_pool2d(y, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "    # Head: flatten → linear\n",
    "    y = nb.reshape(y, (int(y.shape[0]), int(y.shape[1] * y.shape[2] * y.shape[3])))\n",
    "    return nb.relu(nb.matmul(y, wh) + bh)\n",
    "\n",
    "\n",
    "print(\"cnn(x, params) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a542f1",
   "metadata": {},
   "source": [
    "## 4. Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b58157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter shapes:\n",
      "  [Dim(3), Dim(3), Dim(1), Dim(8)]\n",
      "  [Dim(8)]\n",
      "  [Dim(3), Dim(3), Dim(8), Dim(16)]\n",
      "  [Dim(16)]\n",
      "  [Dim(256), Dim(1)]\n",
      "  [Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "def init_params(seed: int = 1) -> list[nb.Tensor]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    return [\n",
    "        nb.Tensor.from_dlpack((0.10 * rng.normal(size=(3, 3, 1, 8))).astype(np.float32)),\n",
    "        nb.Tensor.from_dlpack(np.zeros((8,), dtype=np.float32)),\n",
    "        nb.Tensor.from_dlpack((0.08 * rng.normal(size=(3, 3, 8, 16))).astype(np.float32)),\n",
    "        nb.Tensor.from_dlpack(np.zeros((16,), dtype=np.float32)),\n",
    "        nb.Tensor.from_dlpack((0.10 * rng.normal(size=(16 * 4 * 4, 1))).astype(np.float32)),\n",
    "        nb.Tensor.from_dlpack(np.zeros((1,), dtype=np.float32)),\n",
    "    ]\n",
    "\n",
    "\n",
    "demo_params = init_params()\n",
    "print(\"Parameter shapes:\")\n",
    "for p in demo_params:\n",
    "    print(f\"  {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e121f",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "Plain MSE loss. Both training variants (eager and compiled) share this function unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd9504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params: list[nb.Tensor], x: nb.Tensor, y: nb.Tensor) -> nb.Tensor:\n",
    "    diff = cnn(x, params) - y\n",
    "    return nb.mean(diff * diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a83226",
   "metadata": {},
   "source": [
    "## 6. Eager Training Step (Baseline)\n",
    "\n",
    "`value_and_grad` traces and executes the computation graph on every call.\n",
    "No caching, no compilation overhead — but also no reuse.\n",
    "\n",
    "> Use the eager step for **debugging** or when you only call it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87739312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_train_step(\n",
    "    params: list[nb.Tensor], x: nb.Tensor, y: nb.Tensor, lr: float = 3e-2\n",
    "):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, x, y)\n",
    "    new_params = [p - lr * g for p, g in zip(params, grads)]\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b923c8",
   "metadata": {},
   "source": [
    "## 7. Compiled Training Step\n",
    "\n",
    "Decorating the **exact same logic** with `@nb.compile`:\n",
    "- **First call** — Nabla traces the Python function and compiles it to a MAX graph.\n",
    "- **All later calls** with the same input shapes/dtypes hit the cache and skip Python dispatch entirely.\n",
    "\n",
    "The compiled and eager versions produce **identical numerical results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975d33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.compile\n",
    "def compiled_train_step(\n",
    "    params: list[nb.Tensor], x: nb.Tensor, y: nb.Tensor, lr: float = 3e-2\n",
    "):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, x, y)\n",
    "    new_params = [p - lr * g for p, g in zip(params, grads)]\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ca02b",
   "metadata": {},
   "source": [
    "## 8. Head-to-Head: Eager vs. Compiled\n",
    "\n",
    "We train two identical models from the same seed for 60 steps.\n",
    "The **first step is a warmup** (for the compiled version this triggers trace + compile)\n",
    "and is excluded from the timing measurement.\n",
    "\n",
    "What to observe:\n",
    "- Loss curves should be **numerically identical**.\n",
    "- Compiled should report a lower average `ms/step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857609bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(step_fn, label: str, steps: int = 60, lr: float = 3e-2, seed: int = 0):\n",
    "    nb._clear_caches()\n",
    "    x, y = make_dataset(seed=seed)\n",
    "    params = init_params(seed=seed + 1)\n",
    "\n",
    "    # Warmup (for compiled: triggers trace + compile)\n",
    "    params, loss_warmup = step_fn(params, x, y, lr)\n",
    "    nb.realize_all(loss_warmup, *params)\n",
    "\n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "    print(\"-\" * 22)\n",
    "\n",
    "    losses = []\n",
    "    t0 = time.perf_counter()\n",
    "    for step in range(steps):\n",
    "        params, loss = step_fn(params, x, y, lr)\n",
    "        nb.realize_all(loss, *params)\n",
    "        loss_value = float(loss.item())\n",
    "        losses.append(loss_value)\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"{step + 1:<8} {loss_value:<12.6f}\")\n",
    "\n",
    "    avg_ms = (time.perf_counter() - t0) / steps * 1000.0\n",
    "    print(f\"Avg step: {avg_ms:.1f} ms/step\")\n",
    "    return {\n",
    "        \"avg_ms\": avg_ms,\n",
    "        \"initial_loss\": losses[0],\n",
    "        \"final_loss\": losses[-1],\n",
    "        \"losses\": losses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3277ed8",
   "metadata": {},
   "source": [
    "## 9. Execute Benchmark\n",
    "\n",
    "Run both variants with identical data and initialization.\n",
    "This gives an apples-to-apples performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ddfcf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eager baseline\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       0.029387    \n",
      "20       0.029257    \n",
      "30       0.029142    \n",
      "40       0.029040    \n",
      "50       0.028946    \n",
      "60       0.028860    \n",
      "Avg step: 239.4 ms/step\n",
      "\n",
      "Compiled (@nb.compile)\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       0.029387    \n",
      "20       0.029257    \n",
      "30       0.029142    \n",
      "40       0.029040    \n",
      "50       0.028946    \n",
      "60       0.028860    \n",
      "Avg step: 24.7 ms/step\n",
      "\n",
      "Summary\n",
      "----------------------------------------\n",
      "Eager avg step:    239.42 ms\n",
      "Compiled avg step: 24.68 ms\n",
      "Speedup:           9.70x\n",
      "Loss check (eager final / compiled final): 0.028860 / 0.028860\n",
      "Compiled cache stats: CompilationStats(hits=121, misses=1, fallbacks=0, hit_rate=99.2%)\n"
     ]
    }
   ],
   "source": [
    "eager_result = run(eager_train_step, \"Eager baseline\", steps=60, lr=3e-2, seed=0)\n",
    "compiled_result = run(compiled_train_step, \"Compiled (@nb.compile)\", steps=60, lr=3e-2, seed=0)\n",
    "\n",
    "# Backward compatibility for partially-run kernels where run() may still return float\n",
    "if isinstance(eager_result, float):\n",
    "    eager_result = {\"avg_ms\": eager_result, \"final_loss\": float(\"nan\")}\n",
    "if isinstance(compiled_result, float):\n",
    "    compiled_result = {\"avg_ms\": compiled_result, \"final_loss\": float(\"nan\")}\n",
    "\n",
    "speedup = eager_result[\"avg_ms\"] / max(compiled_result[\"avg_ms\"], 1e-9)\n",
    "print(\"\\nSummary\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Eager avg step:    {eager_result['avg_ms']:.2f} ms\")\n",
    "print(f\"Compiled avg step: {compiled_result['avg_ms']:.2f} ms\")\n",
    "print(f\"Speedup:           {speedup:.2f}x\")\n",
    "print(\n",
    "    f\"Loss check (eager final / compiled final): \"\n",
    "    f\"{eager_result.get('final_loss', float('nan')):.6f} / \"\n",
    "    f\"{compiled_result.get('final_loss', float('nan')):.6f}\"\n",
    ")\n",
    "print(f\"Compiled cache stats: {compiled_train_step.stats}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
