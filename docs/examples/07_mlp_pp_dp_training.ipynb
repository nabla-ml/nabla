{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "81477ea0",
            "metadata": {},
            "source": [
                "# Example 7: 2D Parallel Training (Pipeline + Data Parallelism)\n",
                "\n",
                "This notebook extends pipeline parallelism (Example 6) by adding a\n",
                "**data-parallel** dimension, creating a 2D device mesh:\n",
                "\n",
                "```\n",
                "             Pipeline stages →\n",
                "            Stage 0  Stage 1  Stage 2  Stage 3\n",
                "Data  DP 0   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "Par.  DP 1   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "```\n",
                "\n",
                "**Key idea:** Weights are sharded across pipeline stages and *replicated*\n",
                "across data-parallel replicas. Input batches are sharded across DP replicas.\n",
                "\n",
                "We'll:\n",
                "1. Build a 2D `DeviceMesh(\"dp\", \"pp\")`\n",
                "2. Shard weights on `\"pp\"`, data on `\"dp\"`\n",
                "3. Use the same pipeline primitives from Example 6\n",
                "4. Compute gradients with `nb.value_and_grad`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c6e4605",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "import nabla as nb\n",
                "from nabla import ops\n",
                "from nabla.core.sharding import DeviceMesh, DimSpec\n",
                "from nabla.ops import communication\n",
                "from nabla.transforms import vmap\n",
                "\n",
                "print(\"Nabla 2D Parallelism example\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4b3d011",
            "metadata": {},
            "source": [
                "## 1. Configuration and Device Mesh\n",
                "\n",
                "The 2D mesh has shape `(DP_SIZE, PP_SIZE)` with named axes `\"dp\"` and `\"pp\"`.\n",
                "Each device is identified by a `(dp_rank, pp_rank)` pair:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e1db1b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2D mesh dimensions\n",
                "DP_SIZE = 2          # Data-parallel replicas\n",
                "PP_SIZE = 4          # Pipeline stages\n",
                "MICRO_BATCHES = 4\n",
                "MICRO_BATCH_SIZE = 4\n",
                "DIM = 16\n",
                "\n",
                "mesh = DeviceMesh(\"2d\", (DP_SIZE, PP_SIZE), (\"dp\", \"pp\"))\n",
                "print(f\"2D device mesh: {DP_SIZE} DP replicas × {PP_SIZE} PP stages = {DP_SIZE * PP_SIZE} devices\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d8d3dac",
            "metadata": {},
            "source": [
                "## 2. Pipeline Primitives (same as Example 6)\n",
                "\n",
                "The stage compute, step, and loop functions are identical to Example 6.\n",
                "Only the *sharding specification* changes — the mesh now has two axes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "c9028ffc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running 2D Parallelism Test (DP=2, PP=4)\n",
                        "Computing 2D Parallel Gradients...\n",
                        "Nabla Loss: 3828.785156\n",
                        "Running JAX Reference...\n",
                        "JAX Loss:   3828.785645\n",
                        "Nabla Weights Grad Sample: [-16.103315   5.997751  74.79328 ]\n",
                        "JAX Weights Grad Sample:   [-16.10332     5.9977446  74.793274 ]\n",
                        "Max 2D Diff - Weights: 0.000092, Bias: 0.000031\n",
                        "✅ SUCCESS: 2D Parallel Gradients Match\n"
                    ]
                }
            ],
            "source": [
                "def test_pp_dp_grad():\n",
                "    mesh = DeviceMesh(\"2d\", (DP_SIZE, PP_SIZE), (\"dp\", \"pp\"))\n",
                "    print(f\"Running 2D Parallelism Test (DP={DP_SIZE}, PP={PP_SIZE})\")\n",
                "\n",
                "    np.random.seed(42)\n",
                "\n",
                "    w_np = np.random.randn(PP_SIZE, DIM, DIM).astype(np.float32)\n",
                "    b_np = np.random.randn(PP_SIZE, DIM).astype(np.float32)\n",
                "    total_steps = MICRO_BATCHES + PP_SIZE\n",
                "    x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "    y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "\n",
                "    # Weights sharded on 'pp', replicated on 'dp'\n",
                "    w_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "    b_spec = [DimSpec.from_raw(\"pp\"), None]\n",
                "\n",
                "    # Data sharded on 'dp'\n",
                "    x_padded_np = np.concatenate(\n",
                "        [x_np, np.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=np.float32)], axis=0\n",
                "    )\n",
                "    x_spec = [None, DimSpec.from_raw(\"dp\"), None]\n",
                "\n",
                "    w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
                "    b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec).realize()\n",
                "    x_sharded = ops.shard(nb.Tensor.from_dlpack(x_padded_np), mesh, x_spec).realize()\n",
                "    y_sharded = ops.shard(nb.Tensor.from_dlpack(y_np), mesh, x_spec).realize()\n",
                "\n",
                "    state_spec = [DimSpec.from_raw(\"pp\"), DimSpec.from_raw(\"dp\"), None]\n",
                "    state_sharded = ops.shard(\n",
                "        nb.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=DType.float32),\n",
                "        mesh,\n",
                "        state_spec,\n",
                "    ).realize()\n",
                "\n",
                "    mask_np = np.eye(PP_SIZE, 1).reshape(PP_SIZE, 1, 1).astype(bool)\n",
                "    mask_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "    mask_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, mask_spec).realize()\n",
                "\n",
                "    idx = mesh.axis_names.index(\"pp\")\n",
                "    size = mesh.shape[idx]\n",
                "    perm = []\n",
                "    for dp in range(DP_SIZE):\n",
                "        for src_pp in range(PP_SIZE):\n",
                "            src = dp * PP_SIZE + src_pp\n",
                "            dst = dp * PP_SIZE + (src_pp + 1) % size\n",
                "            perm.append((src, dst))\n",
                "\n",
                "    step_fn = vmap(\n",
                "        stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"pp\", mesh=mesh\n",
                "    )\n",
                "\n",
                "    def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
                "        stream_outputs, _ = pipeline_loop(\n",
                "            inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
                "        )\n",
                "        indices = ops.arange(PP_SIZE, PP_SIZE + MICRO_BATCHES, dtype=DType.int64)\n",
                "        valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
                "        diff = valid_preds - targets\n",
                "        return ops.mean(diff * diff)\n",
                "\n",
                "    print(\"Computing 2D Parallel Gradients...\")\n",
                "    from nabla.core.transforms import value_and_grad\n",
                "\n",
                "    grad_fn = value_and_grad(pipeline_loss, argnums=(1, 2))\n",
                "    (loss_nb, (w_grad, b_grad)) = grad_fn(\n",
                "        x_sharded, w_sharded, b_sharded, state_sharded, mask_sharded, y_sharded\n",
                "    )\n",
                "    print(f\"Nabla Loss: {loss_nb.item():.6f}\")\n",
                "\n",
                "    w_grad_np = w_grad.to_numpy()\n",
                "    b_grad_np = b_grad.to_numpy()\n",
                "\n",
                "    print(\"Running JAX Reference...\")\n",
                "    import jax\n",
                "    import jax.numpy as jnp\n",
                "\n",
                "    def jax_ref(pw, pb, px, py):\n",
                "        def apply(curr, w, b):\n",
                "            return jax.nn.relu(curr @ w + b)\n",
                "\n",
                "        preds = []\n",
                "        for i in range(MICRO_BATCHES):\n",
                "            a = px[i]\n",
                "            for w, b in zip(pw, pb, strict=False):\n",
                "                a = apply(a, w, b)\n",
                "            preds.append(a)\n",
                "        preds = jnp.stack(preds)\n",
                "        return jnp.mean((preds - py) ** 2)\n",
                "\n",
                "    jax_val_grad_fn = jax.value_and_grad(jax_ref, argnums=(0, 1))\n",
                "    loss_jax, (w_ref, b_ref) = jax_val_grad_fn(w_np, b_np, x_np, y_np)\n",
                "    print(f\"JAX Loss:   {loss_jax:.6f}\")\n",
                "\n",
                "    # 7. Compare\n",
                "    print(\"Nabla Weights Grad Sample:\", w_grad_np[0, 0, :3])\n",
                "    print(\"JAX Weights Grad Sample:  \", w_ref[0, 0, :3])\n",
                "\n",
                "    w_diff = np.max(np.abs(w_grad_np - w_ref))\n",
                "    b_diff = np.max(np.abs(b_grad_np - b_ref))\n",
                "    print(f\"Max 2D Diff - Weights: {w_diff:.6f}, Bias: {b_diff:.6f}\")\n",
                "\n",
                "    if w_diff < 5e-4 and b_diff < 5e-4:\n",
                "        print(\"✅ SUCCESS: 2D Parallel Gradients Match\")\n",
                "    else:\n",
                "        print(\"❌ FAILURE\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    test_pp_dp_grad()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.13.6)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
