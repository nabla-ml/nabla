{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24b5e0f",
   "metadata": {},
   "source": [
    "# Example 7: 2D Parallel Training (PP + DP)\n",
    "\n",
    "This example extends pipeline parallelism with data parallelism:\n",
    "- a 2D device mesh (`dp`, `pp`)\n",
    "- sharded parameters and sharded batches\n",
    "- correctness checks against a JAX baseline"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a93ce32",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from max.dtype import DType\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "# --- Project Constants ---\n",
    "DP_SIZE = 2\n",
    "PP_SIZE = 4\n",
    "TOTAL_DEVICES = DP_SIZE * PP_SIZE\n",
    "\n",
    "MICRO_BATCHES = 4\n",
    "MICRO_BATCH_SIZE = 4  # Total batch size per step\n",
    "DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f1306",
   "metadata": {},
   "source": [
    "## 1. Define 2D Pipeline Helpers\n",
    "\n",
    "These functions implement one pp+dp pipeline step and the loop over micro-batches."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d5bb013",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w, b):\n",
    "    return ops.relu(ops.matmul(x, w) + b)\n",
    "\n",
    "\n",
    "def pipeline_step(\n",
    "    current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "):\n",
    "    \"\"\"Single 2D pipeline step: compute -> shift -> extract -> inject.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result\n",
    "\n",
    "\n",
    "def pipeline_loop(\n",
    "    padded_inputs,\n",
    "    weight_stack,\n",
    "    bias_stack,\n",
    "    current_state,\n",
    "    mask_0,\n",
    "    step_fn,\n",
    "    perm,\n",
    "    total_steps,\n",
    "):\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        # Fetch Input\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05af601",
   "metadata": {},
   "source": [
    "## 2. Run 2D Gradient Check\n",
    "\n",
    "Build sharded inputs/weights and compare gradients with a JAX reference."
   ]
  },
  {
   "cell_type": "code",
   "id": "bf605937",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pp_dp_grad():\n",
    "    mesh = DeviceMesh(\"2d\", (DP_SIZE, PP_SIZE), (\"dp\", \"pp\"))\n",
    "    print(f\"Running 2D Parallelism Test (DP={DP_SIZE}, PP={PP_SIZE})\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    w_np = np.random.randn(PP_SIZE, DIM, DIM).astype(np.float32)\n",
    "    b_np = np.random.randn(PP_SIZE, DIM).astype(np.float32)\n",
    "    total_steps = MICRO_BATCHES + PP_SIZE\n",
    "    x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "    y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "    # Weights sharded on 'pp', replicated on 'dp'\n",
    "    w_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
    "    b_spec = [DimSpec.from_raw(\"pp\"), None]\n",
    "\n",
    "    # Data sharded on 'dp'\n",
    "    x_padded_np = np.concatenate(\n",
    "        [x_np, np.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=np.float32)], axis=0\n",
    "    )\n",
    "    x_spec = [None, DimSpec.from_raw(\"dp\"), None]\n",
    "\n",
    "    w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
    "    b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec).realize()\n",
    "    x_sharded = ops.shard(nb.Tensor.from_dlpack(x_padded_np), mesh, x_spec).realize()\n",
    "    y_sharded = ops.shard(nb.Tensor.from_dlpack(y_np), mesh, x_spec).realize()\n",
    "\n",
    "    state_spec = [DimSpec.from_raw(\"pp\"), DimSpec.from_raw(\"dp\"), None]\n",
    "    state_sharded = ops.shard(\n",
    "        nb.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=DType.float32),\n",
    "        mesh,\n",
    "        state_spec,\n",
    "    ).realize()\n",
    "\n",
    "    mask_np = np.eye(PP_SIZE, 1).reshape(PP_SIZE, 1, 1).astype(bool)\n",
    "    mask_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
    "    mask_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, mask_spec).realize()\n",
    "\n",
    "    idx = mesh.axis_names.index(\"pp\")\n",
    "    size = mesh.shape[idx]\n",
    "    perm = []\n",
    "    for dp in range(DP_SIZE):\n",
    "        for src_pp in range(PP_SIZE):\n",
    "            src = dp * PP_SIZE + src_pp\n",
    "            dst = dp * PP_SIZE + (src_pp + 1) % size\n",
    "            perm.append((src, dst))\n",
    "\n",
    "    step_fn = vmap(\n",
    "        stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"pp\", mesh=mesh\n",
    "    )\n",
    "\n",
    "    def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
    "        stream_outputs, _ = pipeline_loop(\n",
    "            inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
    "        )\n",
    "        indices = ops.arange(PP_SIZE, PP_SIZE + MICRO_BATCHES, dtype=DType.int64)\n",
    "        valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
    "        diff = valid_preds - targets\n",
    "        return ops.mean(diff * diff)\n",
    "\n",
    "    print(\"Computing 2D Parallel Gradients...\")\n",
    "    from nabla.core.autograd import value_and_grad\n",
    "\n",
    "    grad_fn = value_and_grad(pipeline_loss, argnums=(1, 2))\n",
    "    (loss_nb, (w_grad, b_grad)) = grad_fn(\n",
    "        x_sharded, w_sharded, b_sharded, state_sharded, mask_sharded, y_sharded\n",
    "    )\n",
    "    print(f\"Nabla Loss: {loss_nb.item():.6f}\")\n",
    "\n",
    "    w_grad_np = w_grad.to_numpy()\n",
    "    b_grad_np = b_grad.to_numpy()\n",
    "\n",
    "    print(\"Running JAX Reference...\")\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    def jax_ref(pw, pb, px, py):\n",
    "        def apply(curr, w, b):\n",
    "            return jax.nn.relu(curr @ w + b)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(MICRO_BATCHES):\n",
    "            a = px[i]\n",
    "            for w, b in zip(pw, pb, strict=False):\n",
    "                a = apply(a, w, b)\n",
    "            preds.append(a)\n",
    "        preds = jnp.stack(preds)\n",
    "        return jnp.mean((preds - py) ** 2)\n",
    "\n",
    "    jax_val_grad_fn = jax.value_and_grad(jax_ref, argnums=(0, 1))\n",
    "    loss_jax, (w_ref, b_ref) = jax_val_grad_fn(w_np, b_np, x_np, y_np)\n",
    "    print(f\"JAX Loss:   {loss_jax:.6f}\")\n",
    "\n",
    "    # 7. Compare\n",
    "    print(\"Nabla Weights Grad Sample:\", w_grad_np[0, 0, :3])\n",
    "    print(\"JAX Weights Grad Sample:  \", w_ref[0, 0, :3])\n",
    "\n",
    "    w_diff = np.max(np.abs(w_grad_np - w_ref))\n",
    "    b_diff = np.max(np.abs(b_grad_np - b_ref))\n",
    "    print(f\"Max 2D Diff - Weights: {w_diff:.6f}, Bias: {b_diff:.6f}\")\n",
    "\n",
    "    if w_diff < 5e-4 and b_diff < 5e-4:\n",
    "        print(\"✅ SUCCESS: 2D Parallel Gradients Match\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pp_dp_grad()"
   ]
  }
 ]
}
