{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212f30d7",
   "metadata": {},
   "source": [
    "# Example 10: LoRA Fine-Tuning MVP\n",
    "\n",
    "This example shows a minimal parameter-efficient fine-tuning workflow:\n",
    "- keep base weights frozen\n",
    "- train only LoRA adapters\n",
    "- save and reload a finetune checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ce174fd",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6ac4",
   "metadata": {},
   "source": [
    "## 1. Synthetic Data Helper"
   ]
  },
  {
   "cell_type": "code",
   "id": "471809ca",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_data(\n",
    "    n_samples: int, in_dim: int, out_dim: int\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(123)\n",
    "    x = rng.normal(size=(n_samples, in_dim)).astype(np.float32)\n",
    "\n",
    "    w_base = rng.normal(size=(in_dim, out_dim)).astype(np.float32) * 0.5\n",
    "    u = rng.normal(size=(in_dim, 4)).astype(np.float32)\n",
    "    v = rng.normal(size=(4, out_dim)).astype(np.float32)\n",
    "    delta = 0.35 * (u @ v)\n",
    "\n",
    "    y = x @ (w_base + delta)\n",
    "    return x, y.astype(np.float32), w_base.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba9d1f",
   "metadata": {},
   "source": [
    "## 2. Train Adapter and Validate Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6f6d9ca",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    in_dim, out_dim = 64, 32\n",
    "    rank = 8\n",
    "    alpha = 16.0\n",
    "    learning_rate = 2e-2\n",
    "    steps = 120\n",
    "\n",
    "    x_np, y_np, w_base_np = make_regression_data(\n",
    "        n_samples=512, in_dim=in_dim, out_dim=out_dim\n",
    "    )\n",
    "\n",
    "    x = nb.Tensor.from_dlpack(x_np)\n",
    "    y = nb.Tensor.from_dlpack(y_np)\n",
    "    frozen_weight = nb.Tensor.from_dlpack(w_base_np)\n",
    "\n",
    "    lora_params = nb.nn.finetune.init_lora_adapter(\n",
    "        frozen_weight, rank=rank, init_std=0.01\n",
    "    )\n",
    "    opt_state = nb.nn.optim.adamw_init(lora_params)\n",
    "\n",
    "    def loss_fn(adapter, batch_x, batch_y):\n",
    "        pred = nb.nn.finetune.lora_linear(batch_x, frozen_weight, adapter, alpha=alpha)\n",
    "        diff = pred - batch_y\n",
    "        return nb.mean(diff * diff)\n",
    "\n",
    "    def train_step(adapter, optimizer_state, batch_x, batch_y):\n",
    "        loss, grads = nb.value_and_grad(loss_fn, argnums=0, realize=False)(\n",
    "            adapter, batch_x, batch_y\n",
    "        )\n",
    "        new_adapter, new_state = nb.nn.optim.adamw_update(\n",
    "            adapter,\n",
    "            grads,\n",
    "            optimizer_state,\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.0,\n",
    "        )\n",
    "        to_realize = [loss]\n",
    "        to_realize.extend(t for t in nb.tree_leaves(grads) if isinstance(t, nb.Tensor))\n",
    "        to_realize.extend(\n",
    "            t for t in nb.tree_leaves(new_adapter) if isinstance(t, nb.Tensor)\n",
    "        )\n",
    "        to_realize.extend(\n",
    "            t for t in nb.tree_leaves(new_state) if isinstance(t, nb.Tensor)\n",
    "        )\n",
    "        nb.realize_all(*to_realize)\n",
    "        return loss, new_adapter, new_state\n",
    "\n",
    "    initial_loss = float(loss_fn(lora_params, x, y).to_numpy())\n",
    "    print(f\"Initial loss: {initial_loss:.6f}\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        loss, lora_params, opt_state = train_step(lora_params, opt_state, x, y)\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f\"Step {step + 1:>3d}: loss={float(loss.to_numpy()):.6f}\")\n",
    "\n",
    "    final_loss = float(loss_fn(lora_params, x, y).to_numpy())\n",
    "    print(f\"Final loss:   {final_loss:.6f}\")\n",
    "\n",
    "    ckpt_dir = Path(\".tmp_lora_ckpt\")\n",
    "    if ckpt_dir.exists():\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    nb.nn.finetune.save_finetune_checkpoint(\n",
    "        ckpt_dir,\n",
    "        lora_params=lora_params,\n",
    "        optimizer_state=opt_state,\n",
    "        metadata={\"alpha\": alpha, \"rank\": rank},\n",
    "    )\n",
    "\n",
    "    lora_template = nb.nn.finetune.init_lora_adapter(\n",
    "        frozen_weight, rank=rank, init_std=0.01\n",
    "    )\n",
    "    opt_template = nb.nn.optim.adamw_init(lora_template)\n",
    "\n",
    "    loaded_lora, loaded_opt, meta = nb.nn.finetune.load_finetune_checkpoint(\n",
    "        ckpt_dir,\n",
    "        lora_template=lora_template,\n",
    "        optimizer_template=opt_template,\n",
    "    )\n",
    "\n",
    "    original_pred = nb.nn.finetune.lora_linear(\n",
    "        x, frozen_weight, lora_params, alpha=alpha\n",
    "    )\n",
    "    loaded_pred = nb.nn.finetune.lora_linear(x, frozen_weight, loaded_lora, alpha=alpha)\n",
    "\n",
    "    max_diff = np.max(np.abs(original_pred.to_numpy() - loaded_pred.to_numpy()))\n",
    "    print(f\"Checkpoint step: {loaded_opt['step'] if loaded_opt else 'N/A'}\")\n",
    "    print(f\"Checkpoint max prediction diff: {max_diff:.8f}\")\n",
    "    print(f\"Checkpoint metadata keys: {sorted(meta.get('user_metadata', {}).keys())}\")\n",
    "\n",
    "    if final_loss >= initial_loss:\n",
    "        raise RuntimeError(\"LoRA training did not reduce loss.\")\n",
    "    if max_diff > 1e-5:\n",
    "        raise RuntimeError(f\"Checkpoint roundtrip mismatch too large: {max_diff}\")\n",
    "\n",
    "    print(\"âœ… LoRA MVP finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ]
}
