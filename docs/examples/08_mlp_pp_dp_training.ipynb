{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "81477ea0",
            "metadata": {},
            "source": [
                "# Example 7: 2D Parallel Training (Pipeline + Data Parallelism)\n",
                "\n",
                "This notebook extends pipeline parallelism (Example 6) by adding a\n",
                "**data-parallel** dimension, creating a 2D device mesh:\n",
                "\n",
                "```\n",
                "             Pipeline stages →\n",
                "            Stage 0  Stage 1  Stage 2  Stage 3\n",
                "Data  DP 0   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "Par.  DP 1   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "```\n",
                "\n",
                "**Key idea:** Weights are sharded across pipeline stages and *replicated*\n",
                "across data-parallel replicas. Input batches are sharded across DP replicas.\n",
                "\n",
                "We'll:\n",
                "1. Build a 2D `DeviceMesh(\"dp\", \"pp\")`\n",
                "2. Shard weights on `\"pp\"`, data on `\"dp\"`\n",
                "3. Use the same pipeline primitives from Example 6\n",
                "4. Compute gradients with `nb.value_and_grad`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0c6e4605",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Nabla 2D Parallelism example\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "\n",
                "import nabla as nb\n",
                "from nabla import ops\n",
                "from nabla.core.sharding import DeviceMesh, DimSpec\n",
                "from nabla.ops import communication\n",
                "from nabla.transforms import vmap\n",
                "\n",
                "print(\"Nabla 2D Parallelism example\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4b3d011",
            "metadata": {},
            "source": [
                "## 1. Configuration and Device Mesh\n",
                "\n",
                "The 2D mesh has shape `(DP_SIZE, PP_SIZE)` with named axes `\"dp\"` and `\"pp\"`.\n",
                "Each device is identified by a `(dp_rank, pp_rank)` pair:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "6e1db1b0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2D device mesh: 2 DP replicas × 4 PP stages = 8 devices\n"
                    ]
                }
            ],
            "source": [
                "# 2D mesh dimensions\n",
                "DP_SIZE = 2          # Data-parallel replicas\n",
                "PP_SIZE = 4          # Pipeline stages\n",
                "MICRO_BATCHES = 4\n",
                "MICRO_BATCH_SIZE = 4\n",
                "DIM = 16\n",
                "\n",
                "mesh = DeviceMesh(\"2d\", (DP_SIZE, PP_SIZE), (\"dp\", \"pp\"))\n",
                "print(f\"2D device mesh: {DP_SIZE} DP replicas × {PP_SIZE} PP stages = {DP_SIZE * PP_SIZE} devices\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d8d3dac",
            "metadata": {},
            "source": [
                "## 2. Pipeline Primitives (same as Example 6)\n",
                "\n",
                "The stage compute, step, and loop functions are identical to Example 6.\n",
                "Only the *sharding specification* changes — the mesh now has two axes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "c9028ffc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def stage_compute(x, w, b):\n",
                "    \"\"\"One pipeline stage: linear + ReLU.\"\"\"\n",
                "    return ops.relu(ops.matmul(x, w) + b)\n",
                "\n",
                "\n",
                "def pipeline_step(current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm):\n",
                "    \"\"\"Compute → shift → extract result → inject fresh input.\"\"\"\n",
                "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
                "    shifted = communication.ppermute(computed, perm)\n",
                "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
                "    result = ops.reduce_sum(res_part, axis=0)\n",
                "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
                "    return next_state, result\n",
                "\n",
                "\n",
                "def pipeline_loop(padded_inputs, weight_stack, bias_stack, current_state, mask_0, step_fn, perm, total_steps):\n",
                "    \"\"\"Stream micro-batches through the pipeline.\"\"\"\n",
                "    results = []\n",
                "    for t in range(total_steps):\n",
                "        start_idx = (t, 0, 0)\n",
                "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
                "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
                "        fresh = ops.squeeze(fraction, axis=0)\n",
                "        current_state, res = pipeline_step(current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm)\n",
                "        results.append(res)\n",
                "    return ops.stack(results, axis=0), current_state"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b88723f6",
            "metadata": {},
            "source": [
                "## 3. Shard Data for 2D Parallelism\n",
                "\n",
                "The key difference from Example 6: we now specify **two-axis sharding**.\n",
                "\n",
                "| Tensor | Sharding | Meaning |\n",
                "|--------|----------|---------|\n",
                "| Weights `w` | `(\"pp\", None, None)` | Partitioned across pipeline stages, replicated across DP |\n",
                "| Biases `b` | `(\"pp\", None)` | Same as weights |\n",
                "| Inputs `x` | `(None, \"dp\", None)` | Replicated across PP, partitioned across DP |\n",
                "| State | `(\"pp\", \"dp\", None)` | Partitioned on both axes |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "5c9d63c4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Weights: [Dim(4), Dim(16), Dim(16)] sharded on 'pp'\n",
                        "Inputs:  [Dim(8), Dim(4), Dim(16)] sharded on 'dp'\n"
                    ]
                }
            ],
            "source": [
                "np.random.seed(42)\n",
                "total_steps = MICRO_BATCHES + PP_SIZE\n",
                "\n",
                "w_np = np.random.randn(PP_SIZE, DIM, DIM).astype(np.float32)\n",
                "b_np = np.random.randn(PP_SIZE, DIM).astype(np.float32)\n",
                "x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "\n",
                "# Weights: sharded on \"pp\", replicated on \"dp\"\n",
                "w_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "b_spec = [DimSpec.from_raw(\"pp\"), None]\n",
                "w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
                "b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec).realize()\n",
                "\n",
                "# Data: sharded on \"dp\" (batch dim), replicated on \"pp\"\n",
                "x_padded_np = np.concatenate(\n",
                "    [x_np, np.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=np.float32)], axis=0\n",
                ")\n",
                "x_spec = [None, DimSpec.from_raw(\"dp\"), None]\n",
                "x_sharded = ops.shard(nb.Tensor.from_dlpack(x_padded_np), mesh, x_spec).realize()\n",
                "y_sharded = ops.shard(nb.Tensor.from_dlpack(y_np), mesh, x_spec).realize()\n",
                "\n",
                "# Pipeline state: sharded on both axes\n",
                "state_spec = [DimSpec.from_raw(\"pp\"), DimSpec.from_raw(\"dp\"), None]\n",
                "state_sharded = ops.shard(\n",
                "    nb.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM)), mesh, state_spec\n",
                ").realize()\n",
                "\n",
                "# Stage-0 mask\n",
                "mask_np = np.eye(PP_SIZE, 1).reshape(PP_SIZE, 1, 1).astype(bool)\n",
                "mask_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "mask_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, mask_spec).realize()\n",
                "\n",
                "print(f\"Weights: {w_sharded.shape} sharded on 'pp'\")\n",
                "print(f\"Inputs:  {x_sharded.shape} sharded on 'dp'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed8dae3b",
            "metadata": {},
            "source": [
                "## 4. 2D Communication Setup\n",
                "\n",
                "With a 2D mesh, `ppermute` needs device-level permutations that shift only\n",
                "within each DP replica's pipeline. For DP=2, PP=4, the 8 devices are numbered\n",
                "`0..7` where device `dp*PP_SIZE + pp` is at position `(dp, pp)`.\n",
                "Each DP replica independently shifts its pipeline stages:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9bf73959",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2D ppermute: [(0, 1), (1, 2), (2, 3), (3, 0), (4, 5), (5, 6), (6, 7), (7, 4)]\n"
                    ]
                }
            ],
            "source": [
                "# Build per-DP-replica pipeline permutations\n",
                "idx = mesh.axis_names.index(\"pp\")\n",
                "size = mesh.shape[idx]\n",
                "perm = []\n",
                "for dp in range(DP_SIZE):\n",
                "    for src_pp in range(PP_SIZE):\n",
                "        src = dp * PP_SIZE + src_pp\n",
                "        dst = dp * PP_SIZE + (src_pp + 1) % size\n",
                "        perm.append((src, dst))\n",
                "print(f\"2D ppermute: {perm}\")\n",
                "\n",
                "# Vectorize stage_compute over the \"pp\" axis\n",
                "step_fn = vmap(\n",
                "    stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"pp\", mesh=mesh\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a79a8ef2",
            "metadata": {},
            "source": [
                "## 5. Loss Function and Gradient Computation\n",
                "\n",
                "We use `nb.value_and_grad` to get both the loss value and weight/bias gradients\n",
                "in one pass. `argnums=(1, 2)` differentiates w.r.t. weights and biases\n",
                "(arguments at positions 1 and 2):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "aa5d5091",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loss: 3828.785156\n",
                        "Weight gradient shape: [Dim(4), Dim(16), Dim(16)]\n",
                        "Bias gradient shape:   [Dim(4), Dim(16)]\n"
                    ]
                }
            ],
            "source": [
                "def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
                "    \"\"\"MSE through the full 2D-parallel pipeline.\"\"\"\n",
                "    stream_outputs, _ = pipeline_loop(\n",
                "        inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
                "    )\n",
                "    indices = ops.arange(PP_SIZE, PP_SIZE + MICRO_BATCHES, dtype=nb.DType.int64)\n",
                "    valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
                "    diff = valid_preds - targets\n",
                "    return ops.mean(diff * diff)\n",
                "\n",
                "\n",
                "grad_fn = nb.value_and_grad(pipeline_loss, argnums=(1, 2))\n",
                "loss_nb, (w_grad, b_grad) = grad_fn(\n",
                "    x_sharded, w_sharded, b_sharded, state_sharded, mask_sharded, y_sharded\n",
                ")\n",
                "\n",
                "print(f\"Loss: {loss_nb.item():.6f}\")\n",
                "print(f\"Weight gradient shape: {w_grad.shape}\")\n",
                "print(f\"Bias gradient shape:   {b_grad.shape}\")\n",
                "\n",
                "w_grad_np = w_grad.to_numpy()\n",
                "b_grad_np = b_grad.to_numpy()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4260352",
            "metadata": {},
            "source": [
                "## 6. Verify Against JAX Reference\n",
                "\n",
                "We compare the 2D-parallel gradients against JAX's sequential computation\n",
                "to confirm that sharding doesn't affect numerical results:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "30a73648",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "JAX loss:   3828.785645\n",
                        "Nabla loss: 3828.785156\n",
                        "Max weight grad diff: 0.000092\n",
                        "Max bias grad diff:   0.000031\n",
                        "✅ 2D parallel gradients match JAX sequential reference\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    import jax\n",
                "    import jax.numpy as jnp\n",
                "\n",
                "    def jax_ref(pw, pb, px, py):\n",
                "        def apply(curr, w, b):\n",
                "            return jax.nn.relu(curr @ w + b)\n",
                "        preds = []\n",
                "        for i in range(MICRO_BATCHES):\n",
                "            a = px[i]\n",
                "            for w, b in zip(pw, pb, strict=False):\n",
                "                a = apply(a, w, b)\n",
                "            preds.append(a)\n",
                "        preds = jnp.stack(preds)\n",
                "        return jnp.mean((preds - py) ** 2)\n",
                "\n",
                "    jax_vg = jax.value_and_grad(jax_ref, argnums=(0, 1))\n",
                "    loss_jax, (w_ref, b_ref) = jax_vg(w_np, b_np, x_np, y_np)\n",
                "\n",
                "    print(f\"JAX loss:   {loss_jax:.6f}\")\n",
                "    print(f\"Nabla loss: {loss_nb.item():.6f}\")\n",
                "\n",
                "    w_diff = np.max(np.abs(w_grad_np - w_ref))\n",
                "    b_diff = np.max(np.abs(b_grad_np - b_ref))\n",
                "    print(f\"Max weight grad diff: {w_diff:.6f}\")\n",
                "    print(f\"Max bias grad diff:   {b_diff:.6f}\")\n",
                "    assert w_diff < 5e-4 and b_diff < 5e-4, \"Gradient mismatch!\"\n",
                "    print(\"✅ 2D parallel gradients match JAX sequential reference\")\n",
                "\n",
                "except ImportError:\n",
                "    print(\"JAX not installed — skipping reference comparison\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5df56db1",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**Key takeaways:**\n",
                "- A 2D mesh partitions tensors along *both* pipeline and data axes\n",
                "- Weights are replicated across DP, sharded across PP — no all-reduce needed for forward\n",
                "- `ppermute` permutations are constructed per-DP-replica to keep pipelines independent\n",
                "- `nb.value_and_grad` differentiates through the full 2D-parallel pipeline"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.13.6)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
