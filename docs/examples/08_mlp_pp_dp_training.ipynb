{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "81477ea0",
            "metadata": {},
            "source": [
                "# Example 7: 2D Parallel Training (Pipeline + Data Parallelism)\n",
                "\n",
                "This notebook extends pipeline parallelism (Example 6) by adding a\n",
                "**data-parallel** dimension, creating a 2D device mesh:\n",
                "\n",
                "```\n",
                "             Pipeline stages →\n",
                "            Stage 0  Stage 1  Stage 2  Stage 3\n",
                "Data  DP 0   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "Par.  DP 1   [w0]     [w1]     [w2]     [w3]    ← same weights, different data\n",
                "```\n",
                "\n",
                "**Key idea:** Weights are sharded across pipeline stages and *replicated*\n",
                "across data-parallel replicas. Input batches are sharded across DP replicas.\n",
                "\n",
                "We'll:\n",
                "1. Build a 2D `DeviceMesh(\"dp\", \"pp\")`\n",
                "2. Shard weights on `\"pp\"`, data on `\"dp\"`\n",
                "3. Use the same pipeline primitives from Example 6\n",
                "4. Compute gradients with `nb.value_and_grad`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "0c6e4605",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Nabla 2D Parallelism example\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "\n",
                "import nabla as nb\n",
                "from nabla import ops\n",
                "from nabla.core.sharding import DeviceMesh, DimSpec\n",
                "from nabla.ops import communication\n",
                "from nabla.transforms import vmap\n",
                "\n",
                "print(\"Nabla 2D Parallelism example\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4b3d011",
            "metadata": {},
            "source": [
                "## 1. Configuration and Device Mesh\n",
                "\n",
                "The 2D mesh has shape `(DP_SIZE, PP_SIZE)` with named axes `\"dp\"` and `\"pp\"`.\n",
                "Each device is identified by a `(dp_rank, pp_rank)` pair:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "6e1db1b0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2D device mesh: 2 DP replicas × 4 PP stages = 8 devices\n"
                    ]
                }
            ],
            "source": [
                "# 2D mesh dimensions\n",
                "DP_SIZE = 2          # Data-parallel replicas\n",
                "PP_SIZE = 4          # Pipeline stages\n",
                "MICRO_BATCHES = 4\n",
                "MICRO_BATCH_SIZE = 4\n",
                "DIM = 16\n",
                "\n",
                "mesh = DeviceMesh(\"2d\", (DP_SIZE, PP_SIZE), (\"dp\", \"pp\"))\n",
                "print(f\"2D device mesh: {DP_SIZE} DP replicas × {PP_SIZE} PP stages = {DP_SIZE * PP_SIZE} devices\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d8d3dac",
            "metadata": {},
            "source": [
                "## 2. Pipeline Primitives (same as Example 6)\n",
                "\n",
                "The stage compute, step, and loop functions are identical to Example 6.\n",
                "Only the *sharding specification* changes — the mesh now has two axes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "c9028ffc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def stage_compute(x, w, b):\n",
                "    \"\"\"One pipeline stage: linear + ReLU.\"\"\"\n",
                "    return ops.relu(ops.matmul(x, w) + b)\n",
                "\n",
                "\n",
                "def pipeline_step(current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm):\n",
                "    \"\"\"Compute → shift → extract result → inject fresh input.\"\"\"\n",
                "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
                "    shifted = communication.ppermute(computed, perm)\n",
                "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
                "    result = ops.reduce_sum(res_part, axis=0)\n",
                "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
                "    return next_state, result\n",
                "\n",
                "\n",
                "def pipeline_loop(padded_inputs, weight_stack, bias_stack, current_state, mask_0, step_fn, perm, total_steps):\n",
                "    \"\"\"Stream micro-batches through the pipeline.\"\"\"\n",
                "    results = []\n",
                "    for t in range(total_steps):\n",
                "        start_idx = (t, 0, 0)\n",
                "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
                "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
                "        fresh = ops.squeeze(fraction, axis=0)\n",
                "        current_state, res = pipeline_step(current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm)\n",
                "        results.append(res)\n",
                "    return ops.stack(results, axis=0), current_state"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b88723f6",
            "metadata": {},
            "source": [
                "## 3. Shard Data for 2D Parallelism\n",
                "\n",
                "The key difference from Example 6: we now specify **two-axis sharding**.\n",
                "\n",
                "| Tensor | Sharding | Meaning |\n",
                "|--------|----------|---------|\n",
                "| Weights `w` | `(\"pp\", None, None)` | Partitioned across pipeline stages, replicated across DP |\n",
                "| Biases `b` | `(\"pp\", None)` | Same as weights |\n",
                "| Inputs `x` | `(None, \"dp\", None)` | Replicated across PP, partitioned across DP |\n",
                "| State | `(\"pp\", \"dp\", None)` | Partitioned on both axes |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "5c9d63c4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Weights: [Dim(4), Dim(16), Dim(16)] sharded on 'pp'\n",
                        "Inputs:  [Dim(8), Dim(4), Dim(16)] sharded on 'dp'\n"
                    ]
                }
            ],
            "source": [
                "np.random.seed(42)\n",
                "total_steps = MICRO_BATCHES + PP_SIZE\n",
                "\n",
                "w_np = np.random.randn(PP_SIZE, DIM, DIM).astype(np.float32)\n",
                "b_np = np.random.randn(PP_SIZE, DIM).astype(np.float32)\n",
                "x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
                "\n",
                "# Weights: sharded on \"pp\", replicated on \"dp\"\n",
                "w_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "b_spec = [DimSpec.from_raw(\"pp\"), None]\n",
                "w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
                "b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec).realize()\n",
                "\n",
                "# Data: sharded on \"dp\" (batch dim), replicated on \"pp\"\n",
                "x_padded_np = np.concatenate(\n",
                "    [x_np, np.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM), dtype=np.float32)], axis=0\n",
                ")\n",
                "x_spec = [None, DimSpec.from_raw(\"dp\"), None]\n",
                "x_sharded = ops.shard(nb.Tensor.from_dlpack(x_padded_np), mesh, x_spec).realize()\n",
                "y_sharded = ops.shard(nb.Tensor.from_dlpack(y_np), mesh, x_spec).realize()\n",
                "\n",
                "# Pipeline state: sharded on both axes\n",
                "state_spec = [DimSpec.from_raw(\"pp\"), DimSpec.from_raw(\"dp\"), None]\n",
                "state_sharded = ops.shard(\n",
                "    nb.zeros((PP_SIZE, MICRO_BATCH_SIZE, DIM)), mesh, state_spec\n",
                ").realize()\n",
                "\n",
                "# Stage-0 mask\n",
                "mask_np = np.eye(PP_SIZE, 1).reshape(PP_SIZE, 1, 1).astype(bool)\n",
                "mask_spec = [DimSpec.from_raw(\"pp\"), None, None]\n",
                "mask_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, mask_spec).realize()\n",
                "\n",
                "print(f\"Weights: {w_sharded.shape} sharded on 'pp'\")\n",
                "print(f\"Inputs:  {x_sharded.shape} sharded on 'dp'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed8dae3b",
            "metadata": {},
            "source": [
                "## 4. 2D Communication Setup\n",
                "\n",
                "With a 2D mesh, `ppermute` needs device-level permutations that shift only\n",
                "within each DP replica's pipeline. For DP=2, PP=4, the 8 devices are numbered\n",
                "`0..7` where device `dp*PP_SIZE + pp` is at position `(dp, pp)`.\n",
                "Each DP replica independently shifts its pipeline stages:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "9bf73959",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2D ppermute: [(0, 1), (1, 2), (2, 3), (3, 0), (4, 5), (5, 6), (6, 7), (7, 4)]\n"
                    ]
                }
            ],
            "source": [
                "# Build per-DP-replica pipeline permutations\n",
                "idx = mesh.axis_names.index(\"pp\")\n",
                "size = mesh.shape[idx]\n",
                "perm = []\n",
                "for dp in range(DP_SIZE):\n",
                "    for src_pp in range(PP_SIZE):\n",
                "        src = dp * PP_SIZE + src_pp\n",
                "        dst = dp * PP_SIZE + (src_pp + 1) % size\n",
                "        perm.append((src, dst))\n",
                "print(f\"2D ppermute: {perm}\")\n",
                "\n",
                "# Vectorize stage_compute over the \"pp\" axis\n",
                "step_fn = vmap(\n",
                "    stage_compute, in_axes=(0, 0, 0), out_axes=0, spmd_axis_name=\"pp\", mesh=mesh\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a79a8ef2",
            "metadata": {},
            "source": [
                "## 5. Loss Function and Gradient Computation\n",
                "\n",
                "We use `nb.value_and_grad` to get both the loss value and weight/bias gradients\n",
                "in one pass. `argnums=(1, 2)` differentiates w.r.t. weights and biases\n",
                "(arguments at positions 1 and 2):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "aa5d5091",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "Failed to create op 'mo_gather':\nInputs:\n    result = TensorType(dtype=float32, shape=[Dim(4), Dim(4), Dim(16)], device=cpu:0)\n    input = TensorValue(dtype=float32, shape=[Dim(8), Dim(4), Dim(16)], device=cpu:0)\n    indices = TensorValue(dtype=float32, shape=[Dim(4)], device=cpu:0)\n    axis = TensorValue(dtype=int64, shape=[], device=cpu:0)\n\nDiagnostics:\n    \nDiagnostics:\n    \nVerification failed:\nerror: unknown: 'rmo.mo.gather' op operand #1 must be tensor with integer or index elements, but got '!mo.tensor<[4], f32>'\n note: unknown: see current operation: %1339 = \"rmo.mo.gather\"(%1326, %1337, %1338) <{outputParamDecls = #kgen<param.decls[]>}> : (!mo.tensor<[8, 4, 16], f32>, !mo.tensor<[4], f32>, !mo.tensor<[], si64>) -> !mo.tensor<[4, 4, 16], f32>",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m grad_fn = nb.value_and_grad(pipeline_loss, argnums=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m     13\u001b[39m loss_nb, (w_grad, b_grad) = grad_fn(\n\u001b[32m     14\u001b[39m     x_sharded, w_sharded, b_sharded, state_sharded, mask_sharded, y_sharded\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss_nb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeight gradient shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_grad.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBias gradient shape:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_grad.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/tensor/api.py:998\u001b[39m, in \u001b[36mTensor.item\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Unified item access.\"\"\"\u001b[39;00m\n\u001b[32m    997\u001b[39m t = \u001b[38;5;28mself\u001b[39m.gather()\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t._impl._buffers:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to realize tensor for item access\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/tensor/api.py:740\u001b[39m, in \u001b[36mTensor.realize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRAPH\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[43mGRAPH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:538\u001b[39m, in \u001b[36mComputeGraph.evaluate\u001b[39m\u001b[34m(self, tensor, return_model, *extra_outputs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Replay trace to build MAX graph\u001b[39;00m\n\u001b[32m    537\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: replaying trace to build graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_replay_trace_to_build_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Build graph outputs\u001b[39;00m\n\u001b[32m    541\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: building graph outputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:793\u001b[39m, in \u001b[36mComputeGraph._replay_trace_to_build_graph\u001b[39m\u001b[34m(self, targets)\u001b[39m\n\u001b[32m    790\u001b[39m op_args = pytree.tree_map(to_tensor, opnode.op_args)\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     raw_result = \u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m executed_ops += \u001b[32m1\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# Extract graph values\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/base.py:191\u001b[39m, in \u001b[36mOperation.execute\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m mesh = spmd.get_mesh_from_args(args)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m GRAPH.graph:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     shard_results = \u001b[43mspmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_on_shards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapted_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m output_sharding = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._infer_output_sharding:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/sharding/spmd.py:441\u001b[39m, in \u001b[36mexecute_on_shards\u001b[39m\u001b[34m(op_fn, args, kwargs, mesh, input_shardings, op)\u001b[39m\n\u001b[32m    438\u001b[39m     local_kwargs = op._transform_shard_kwargs(kwargs, output_sharding, i, args)\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Execute with unified kernel signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m result = \u001b[43mop_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[32m1\u001b[39m:\n\u001b[32m    446\u001b[39m     results.append(result[\u001b[32m0\u001b[39m])\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/view/indexing.py:162\u001b[39m, in \u001b[36mGatherOp.kernel\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [ops.gather_nd(x, indices, batch_dims=batch_dims)]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/ops/gather.py:54\u001b[39m, in \u001b[36mgather\u001b[39m\u001b[34m(input, indices, axis)\u001b[39m\n\u001b[32m     52\u001b[39m output_shape = [*shape[:axis], *indices.shape, *shape[axis + \u001b[32m1\u001b[39m :]]\n\u001b[32m     53\u001b[39m assert_same_device(\u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m, indices=indices)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrmo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmo_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDeviceRef\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCPU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tensor\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/graph.py:788\u001b[39m, in \u001b[36mGraph._add_op\u001b[39m\u001b[34m(self, op, *args, **kwargs)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, op, *args, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[Value[Any]]:  \u001b[38;5;66;03m# noqa: ANN001\u001b[39;00m\n\u001b[32m    787\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for clients that only require the op results.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     results, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_op_get_op_with_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/graph.py:848\u001b[39m, in \u001b[36mGraph._add_op_get_op_with_results\u001b[39m\u001b[34m(self, op, _ip, *args, **kwargs)\u001b[39m\n\u001b[32m    846\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    847\u001b[39m             mapped_args = {\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(args), **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    849\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to create op \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInputs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    850\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    851\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mapped_args.items()\n\u001b[32m    852\u001b[39m             )\n\u001b[32m    853\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    854\u001b[39m             \u001b[38;5;66;03m# Intentionally suppress extra stack traces from max._mlir.\u001b[39;00m\n\u001b[32m    855\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    857\u001b[39m _set_output_param_decls(Operation._from_cmlir(staged_op), \u001b[38;5;28mself\u001b[39m._params)\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, mlir.Operation | mlir.OpView):\n",
                        "\u001b[31mValueError\u001b[39m: Failed to create op 'mo_gather':\nInputs:\n    result = TensorType(dtype=float32, shape=[Dim(4), Dim(4), Dim(16)], device=cpu:0)\n    input = TensorValue(dtype=float32, shape=[Dim(8), Dim(4), Dim(16)], device=cpu:0)\n    indices = TensorValue(dtype=float32, shape=[Dim(4)], device=cpu:0)\n    axis = TensorValue(dtype=int64, shape=[], device=cpu:0)\n\nDiagnostics:\n    \nDiagnostics:\n    \nVerification failed:\nerror: unknown: 'rmo.mo.gather' op operand #1 must be tensor with integer or index elements, but got '!mo.tensor<[4], f32>'\n note: unknown: see current operation: %1339 = \"rmo.mo.gather\"(%1326, %1337, %1338) <{outputParamDecls = #kgen<param.decls[]>}> : (!mo.tensor<[8, 4, 16], f32>, !mo.tensor<[4], f32>, !mo.tensor<[], si64>) -> !mo.tensor<[4, 4, 16], f32>"
                    ]
                }
            ],
            "source": [
                "def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
                "    \"\"\"MSE through the full 2D-parallel pipeline.\"\"\"\n",
                "    stream_outputs, _ = pipeline_loop(\n",
                "        inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
                "    )\n",
                "    indices = ops.arange(PP_SIZE, PP_SIZE + MICRO_BATCHES)\n",
                "    valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
                "    diff = valid_preds - targets\n",
                "    return ops.mean(diff * diff)\n",
                "\n",
                "\n",
                "grad_fn = nb.value_and_grad(pipeline_loss, argnums=(1, 2))\n",
                "loss_nb, (w_grad, b_grad) = grad_fn(\n",
                "    x_sharded, w_sharded, b_sharded, state_sharded, mask_sharded, y_sharded\n",
                ")\n",
                "\n",
                "print(f\"Loss: {loss_nb.item():.6f}\")\n",
                "print(f\"Weight gradient shape: {w_grad.shape}\")\n",
                "print(f\"Bias gradient shape:   {b_grad.shape}\")\n",
                "\n",
                "w_grad_np = w_grad.to_numpy()\n",
                "b_grad_np = b_grad.to_numpy()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4260352",
            "metadata": {},
            "source": [
                "## 6. Verify Against JAX Reference\n",
                "\n",
                "We compare the 2D-parallel gradients against JAX's sequential computation\n",
                "to confirm that sharding doesn't affect numerical results:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "30a73648",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import jax\n",
                "    import jax.numpy as jnp\n",
                "\n",
                "    def jax_ref(pw, pb, px, py):\n",
                "        def apply(curr, w, b):\n",
                "            return jax.nn.relu(curr @ w + b)\n",
                "        preds = []\n",
                "        for i in range(MICRO_BATCHES):\n",
                "            a = px[i]\n",
                "            for w, b in zip(pw, pb, strict=False):\n",
                "                a = apply(a, w, b)\n",
                "            preds.append(a)\n",
                "        preds = jnp.stack(preds)\n",
                "        return jnp.mean((preds - py) ** 2)\n",
                "\n",
                "    jax_vg = jax.value_and_grad(jax_ref, argnums=(0, 1))\n",
                "    loss_jax, (w_ref, b_ref) = jax_vg(w_np, b_np, x_np, y_np)\n",
                "\n",
                "    print(f\"JAX loss:   {loss_jax:.6f}\")\n",
                "    print(f\"Nabla loss: {loss_nb.item():.6f}\")\n",
                "\n",
                "    w_diff = np.max(np.abs(w_grad_np - w_ref))\n",
                "    b_diff = np.max(np.abs(b_grad_np - b_ref))\n",
                "    print(f\"Max weight grad diff: {w_diff:.6f}\")\n",
                "    print(f\"Max bias grad diff:   {b_diff:.6f}\")\n",
                "    assert w_diff < 5e-4 and b_diff < 5e-4, \"Gradient mismatch!\"\n",
                "    print(\"✅ 2D parallel gradients match JAX sequential reference\")\n",
                "\n",
                "except ImportError:\n",
                "    print(\"JAX not installed — skipping reference comparison\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5df56db1",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**Key takeaways:**\n",
                "- A 2D mesh partitions tensors along *both* pipeline and data axes\n",
                "- Weights are replicated across DP, sharded across PP — no all-reduce needed for forward\n",
                "- `ppermute` permutations are constructed per-DP-replica to keep pipelines independent\n",
                "- `nb.value_and_grad` differentiates through the full 2D-parallel pipeline\n",
                "\n",
                "**Previous:** [06 — Pipeline Parallelism](06_mlp_pipeline_parallel.ipynb) · **Next:** [08 — Pipeline Inference](08_mlp_pipeline_inference.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.13.6)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
