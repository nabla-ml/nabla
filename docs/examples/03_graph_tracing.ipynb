{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2548706b",
   "metadata": {},
   "source": [
    "# Example 3: Graph Tracing — Under the Hood\n",
    "\n",
    "Nabla is a **tracing-based** framework. Every operation you write — `nb.sin`, `nb.matmul`,\n",
    "`+` — builds a computation graph behind the scenes. This graph is what transforms like\n",
    "`grad`, `vmap`, and `compile` operate on.\n",
    "\n",
    "In this notebook you'll learn to:\n",
    "1. **Trace** a function to see its computation graph\n",
    "2. See how `grad` transforms that graph (adding backward ops)\n",
    "3. See how `vmap` transforms it (adding batch dimensions)\n",
    "4. Understand why tracing matters for `nb.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aec806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla graph tracing example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla.core import trace  # low-level tracing API\n",
    "\n",
    "print(\"Nabla graph tracing example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807ddc4",
   "metadata": {},
   "source": [
    "## 1. Tracing a Simple Function\n",
    "\n",
    "Let's define a small function and **trace** it. Tracing runs the function with\n",
    "symbolic inputs, recording every operation into a graph — without actually\n",
    "computing any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de505e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    \"\"\"A simple function: f(x, y) = sin(x) * y + exp(x).\"\"\"\n",
    "    return nb.sin(x) * y + nb.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b85126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mfn\u001b[0m(\n",
      "    \u001b[96m%a1\u001b[0m: f32[2],\n",
      "    \u001b[96m%a2\u001b[0m: f32[2]\n",
      ") {\n",
      "  \u001b[96m%v1\u001b[0m: f32[2] = sin(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v2\u001b[0m: f32[2] = mul(\u001b[96m%v1\u001b[0m, \u001b[96m%a2\u001b[0m)\n",
      "  \u001b[96m%v3\u001b[0m: f32[2] = exp(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v4\u001b[0m: f32[2] = add(\u001b[96m%v2\u001b[0m, \u001b[96m%v3\u001b[0m)\n",
      "  \u001b[1mreturn\u001b[0m \u001b[96m%v4\u001b[0m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create concrete inputs (their shapes/dtypes matter, not values)\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0], dtype=np.float32))\n",
    "y = nb.Tensor.from_dlpack(np.array([3.0, 4.0], dtype=np.float32))\n",
    "\n",
    "# Trace the function\n",
    "traced_graph = trace(f, x, y)\n",
    "print(traced_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ffc43",
   "metadata": {},
   "source": [
    "### Reading the Trace Output\n",
    "\n",
    "The trace prints an **IR (intermediate representation)** of your function:\n",
    "\n",
    "- `%a1`, `%a2` are the **input arguments** (your `x` and `y`)\n",
    "- `%v1`, `%v2`, ... are **intermediate values** produced by operations\n",
    "- Each line shows: `variable: type = operation(inputs)`\n",
    "- The `return` statement shows which value is the final output\n",
    "\n",
    "This is exactly the graph that gets compiled when you use `@nb.compile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63fcb9",
   "metadata": {},
   "source": [
    "## 2. How `grad` Transforms the Graph\n",
    "\n",
    "When you call `nb.grad(f)`, Nabla doesn't just run backpropagation at runtime.\n",
    "It **transforms the graph itself** — adding reverse-mode differentiation operations.\n",
    "Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec3c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mfn\u001b[0m(\n",
      "    \u001b[96m%a1\u001b[0m: f32[2],\n",
      "    \u001b[96m%a2\u001b[0m: f32[2]\n",
      ") {\n",
      "  \u001b[96m%v1\u001b[0m: f32[2] = ones(device=Device(type=cpu,id=0), dtype=@float32, shape=(2,))\n",
      "  \u001b[96m%v2\u001b[0m: f32[2] = exp(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v3\u001b[0m: f32[2] = mul(\u001b[96m%v1\u001b[0m, \u001b[96m%v2\u001b[0m)\n",
      "  \u001b[96m%v4\u001b[0m: f32[2] = mul(\u001b[96m%v1\u001b[0m, \u001b[96m%a2\u001b[0m)\n",
      "  \u001b[96m%v5\u001b[0m: f32[2] = cos(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v6\u001b[0m: f32[2] = mul(\u001b[96m%v4\u001b[0m, \u001b[96m%v5\u001b[0m)\n",
      "  \u001b[96m%v7\u001b[0m: f32[2] = add(\u001b[96m%v3\u001b[0m, \u001b[96m%v6\u001b[0m)\n",
      "  \u001b[1mreturn\u001b[0m \u001b[96m%v7\u001b[0m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "grad_f = nb.grad(f, argnums=0)  # gradient w.r.t. x\n",
    "\n",
    "traced_grad = trace(grad_f, x, y)\n",
    "print(traced_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea0b70",
   "metadata": {},
   "source": [
    "Notice how the graph is now **larger** — it contains:\n",
    "1. The **forward pass** (same ops as before: `sin`, `mul`, `exp`, `add`)\n",
    "2. The **backward pass** (new ops that implement the chain rule)\n",
    "\n",
    "Each backward op computes a partial derivative. Together, they propagate\n",
    "the gradient from the output back to the input `x`.\n",
    "\n",
    "The key insight: `grad` is not magic — it's a **graph-to-graph transformation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb451573",
   "metadata": {},
   "source": [
    "## 3. How `vmap` Transforms the Graph\n",
    "\n",
    "`vmap` (vectorized map) adds a **batch dimension** to every operation in the graph.\n",
    "Instead of processing one input at a time, the transformed function processes an\n",
    "entire batch in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3429fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mfn\u001b[0m(\n",
      "    \u001b[96m%a1\u001b[0m: f32[3,2],\n",
      "    \u001b[96m%a2\u001b[0m: f32[3,2]\n",
      ") {\n",
      "  \u001b[96m%v1\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = incr_batch_dims(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v2\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = sin(\u001b[96m%v1\u001b[0m)\n",
      "  \u001b[96m%v3\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = incr_batch_dims(\u001b[96m%a2\u001b[0m)\n",
      "  \u001b[96m%v4\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = mul(\u001b[96m%v2\u001b[0m, \u001b[96m%v3\u001b[0m)\n",
      "  \u001b[96m%v5\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = exp(\u001b[96m%v1\u001b[0m)\n",
      "  \u001b[96m%v6\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = add(\u001b[96m%v4\u001b[0m, \u001b[96m%v5\u001b[0m)\n",
      "  \u001b[96m%v7\u001b[0m: f32[3,2] = decr_batch_dims(\u001b[96m%v6\u001b[0m)\n",
      "  \u001b[1mreturn\u001b[0m \u001b[96m%v7\u001b[0m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "batched_f = nb.vmap(f)\n",
    "\n",
    "# Create batched inputs: (batch=3, features=2)\n",
    "x_batch = nb.Tensor.from_dlpack(\n",
    "    np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=np.float32)\n",
    ")\n",
    "y_batch = nb.Tensor.from_dlpack(\n",
    "    np.array([[0.5, 0.5], [1.0, 1.0], [2.0, 2.0]], dtype=np.float32)\n",
    ")\n",
    "\n",
    "traced_vmap = trace(batched_f, x_batch, y_batch)\n",
    "print(traced_vmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243efe74",
   "metadata": {},
   "source": [
    "The operations are the same (`sin`, `mul`, `exp`, `add`), but now each tensor\n",
    "carries an extra **batch dimension**. The trace may show this as a leading\n",
    "dimension in the type signatures.\n",
    "\n",
    "`vmap` doesn't write a loop — it lifts every operation to work on batches natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2febc",
   "metadata": {},
   "source": [
    "## 4. Composing Transforms\n",
    "\n",
    "The real power is **composing transforms**. Since each transform is just a\n",
    "graph-to-graph function, you can stack them:\n",
    "\n",
    "- `grad(vmap(f))` — gradient of a batched function\n",
    "- `vmap(grad(f))` — per-sample gradients\n",
    "- `jacrev(f)` — full Jacobian via batched VJPs\n",
    "\n",
    "Let's trace a composed transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e8395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mfn\u001b[0m(\n",
      "    \u001b[96m%a1\u001b[0m: f32[3,2],\n",
      "    \u001b[96m%a2\u001b[0m: f32[3,2]\n",
      ") {\n",
      "  \u001b[96m%v1\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = ones(device=Device(type=cpu,id=0), dtype=@float32, shape=(3,2))\n",
      "  \u001b[96m%v2\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = incr_batch_dims(\u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v3\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = exp(\u001b[96m%v2\u001b[0m)\n",
      "  \u001b[96m%v4\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = mul(\u001b[96m%v1\u001b[0m, \u001b[96m%v3\u001b[0m)\n",
      "  \u001b[96m%v5\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = incr_batch_dims(\u001b[96m%a2\u001b[0m)\n",
      "  \u001b[96m%v6\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = mul(\u001b[96m%v1\u001b[0m, \u001b[96m%v5\u001b[0m)\n",
      "  \u001b[96m%v7\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = cos(\u001b[96m%v2\u001b[0m)\n",
      "  \u001b[96m%v8\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = mul(\u001b[96m%v6\u001b[0m, \u001b[96m%v7\u001b[0m)\n",
      "  \u001b[96m%v9\u001b[0m: f32[\u001b[90m3\u001b[0m | 2] = add(\u001b[96m%v4\u001b[0m, \u001b[96m%v8\u001b[0m)\n",
      "  \u001b[96m%v10\u001b[0m: f32[3,2] = decr_batch_dims(\u001b[96m%v9\u001b[0m)\n",
      "  \u001b[1mreturn\u001b[0m \u001b[96m%v10\u001b[0m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Per-sample gradients: vmap over grad\n",
    "per_sample_grad = nb.vmap(nb.grad(f, argnums=0))\n",
    "\n",
    "traced_composed = trace(per_sample_grad, x_batch, y_batch)\n",
    "print(traced_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c8da7",
   "metadata": {},
   "source": [
    "This graph contains both the backward ops from `grad` and the batching from\n",
    "`vmap` — composed automatically. Each sample in the batch gets its own gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5a7f3",
   "metadata": {},
   "source": [
    "## 5. Why Tracing Matters for `nb.compile`\n",
    "\n",
    "When you decorate a function with `@nb.compile`, Nabla:\n",
    "1. **Traces** the function (just like we did above)\n",
    "2. **Optimizes** the graph (fusing ops, eliminating redundancy)\n",
    "3. **Compiles** it to run on the target hardware (CPU/GPU)\n",
    "\n",
    "The trace is the bridge between your Python code and efficient compiled execution.\n",
    "Let's verify — a compiled function produces the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b7225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager result:    Tensor([ 5.2427 11.0262] : f32[2])\n",
      "Compiled result: Tensor([ 5.2427 11.0262] : f32[2])\n"
     ]
    }
   ],
   "source": [
    "compiled_f = nb.compile(f)\n",
    "\n",
    "# Compare eager vs compiled\n",
    "eager_result = f(x, y)\n",
    "compiled_result = compiled_f(x, y)\n",
    "\n",
    "print(f\"Eager result:    {eager_result}\")\n",
    "print(f\"Compiled result: {compiled_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326207b",
   "metadata": {},
   "source": [
    "## 6. Tracing a More Realistic Function\n",
    "\n",
    "Let's trace something closer to real ML — a tiny neural network layer with\n",
    "a loss function, then see what `value_and_grad` does to the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f473058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_layer(params, x):\n",
    "    \"\"\"One linear layer + ReLU: f(x) = relu(x @ W + b).\"\"\"\n",
    "    return nb.relu(x @ params[\"W\"] + params[\"b\"])\n",
    "\n",
    "def loss_fn(params, x, target):\n",
    "    \"\"\"MSE loss on the layer output.\"\"\"\n",
    "    pred = simple_layer(params, x)\n",
    "    return nb.mean((pred - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d965e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mfn\u001b[0m(\n",
      "    \u001b[96m%a1\u001b[0m: f32[3,2],\n",
      "    \u001b[96m%a2\u001b[0m: f32[2],\n",
      "    \u001b[96m%a3\u001b[0m: f32[4,3],\n",
      "    \u001b[96m%a4\u001b[0m: f32[4,2]\n",
      ") {\n",
      "  \u001b[96m%v1\u001b[0m: f32[4,2] = matmul(\u001b[96m%a3\u001b[0m, \u001b[96m%a1\u001b[0m)\n",
      "  \u001b[96m%v2\u001b[0m: f32[1,2] = unsqueeze(\u001b[96m%a2\u001b[0m, axis=0)\n",
      "  \u001b[96m%v3\u001b[0m: f32[4,2] = broadcast_to(\u001b[96m%v2\u001b[0m, shape=(4,2))\n",
      "  \u001b[96m%v4\u001b[0m: f32[4,2] = add(\u001b[96m%v1\u001b[0m, \u001b[96m%v3\u001b[0m)\n",
      "  \u001b[96m%v5\u001b[0m: f32[4,2] = relu(\u001b[96m%v4\u001b[0m)\n",
      "  \u001b[96m%v6\u001b[0m: f32[4,2] = sub(\u001b[96m%v5\u001b[0m, \u001b[96m%a4\u001b[0m)\n",
      "  \u001b[96m%v7\u001b[0m: f32[1] = unsqueeze(\u001b[96m?\u001b[0m, axis=0)\n",
      "  \u001b[96m%v8\u001b[0m: f32[1,1] = unsqueeze(\u001b[96m%v7\u001b[0m, axis=0)\n",
      "  \u001b[96m%v9\u001b[0m: f32[4,2] = broadcast_to(\u001b[96m%v8\u001b[0m, shape=(4,2))\n",
      "  \u001b[96m%v10\u001b[0m: f32[4,2] = pow(\u001b[96m%v6\u001b[0m, \u001b[96m%v9\u001b[0m)\n",
      "  \u001b[96m%v11\u001b[0m: f32[4,1] = reduce_sum(\u001b[96m%v10\u001b[0m, axis=1, keepdims=True)\n",
      "  \u001b[96m%v12\u001b[0m: f32[1,1] = reduce_sum(\u001b[96m%v11\u001b[0m, axis=0, keepdims=True)\n",
      "  \u001b[96m%v13\u001b[0m: f32[1] = reshape(\u001b[96m%v12\u001b[0m, shape=(1,))\n",
      "  \u001b[96m%v14\u001b[0m: f32[] = squeeze(\u001b[96m%v13\u001b[0m, axis=0)\n",
      "  \u001b[96m%v15\u001b[0m: f32[] = div(\u001b[96m%v14\u001b[0m, \u001b[96m?\u001b[0m)\n",
      "  \u001b[96m%v16\u001b[0m: f32[3,4] = swap_axes(\u001b[96m%a3\u001b[0m, axis1=-2, axis2=-1)\n",
      "  \u001b[96m%v17\u001b[0m: f32[1] = unsqueeze(\u001b[96m?\u001b[0m, axis=0)\n",
      "  \u001b[96m%v18\u001b[0m: f32[1,1] = unsqueeze(\u001b[96m%v17\u001b[0m, axis=0)\n",
      "  \u001b[96m%v19\u001b[0m: f32[4,2] = broadcast_to(\u001b[96m%v18\u001b[0m, shape=(4,2))\n",
      "  \u001b[96m%v20\u001b[0m: bool[4,2] = greater(\u001b[96m%v4\u001b[0m, \u001b[96m%v19\u001b[0m)\n",
      "  \u001b[96m%v21\u001b[0m: f32[] = ones(device=Device(type=cpu,id=0), dtype=@float32, shape=())\n",
      "  \u001b[96m%v22\u001b[0m: f32[] = div(\u001b[96m%v21\u001b[0m, \u001b[96m?\u001b[0m)\n",
      "  \u001b[96m%v23\u001b[0m: f32[1] = unsqueeze(\u001b[96m%v22\u001b[0m, axis=0)\n",
      "  \u001b[96m%v24\u001b[0m: f32[1,1] = reshape(\u001b[96m%v23\u001b[0m, shape=(1,1))\n",
      "  \u001b[96m%v25\u001b[0m: f32[4,1] = broadcast_to(\u001b[96m%v24\u001b[0m, shape=(4,1))\n",
      "  \u001b[96m%v26\u001b[0m: f32[4,2] = broadcast_to(\u001b[96m%v25\u001b[0m, shape=(4,2))\n",
      "  \u001b[96m%v27\u001b[0m: f32[1] = unsqueeze(\u001b[96m?\u001b[0m, axis=0)\n",
      "  \u001b[96m%v28\u001b[0m: f32[1,1] = unsqueeze(\u001b[96m%v27\u001b[0m, axis=0)\n",
      "  \u001b[96m%v29\u001b[0m: f32[4,2] = broadcast_to(\u001b[96m%v28\u001b[0m, shape=(4,2))\n",
      "  \u001b[96m%v30\u001b[0m: f32[4,2] = sub(\u001b[96m%v9\u001b[0m, \u001b[96m%v29\u001b[0m)\n",
      "  \u001b[96m%v31\u001b[0m: f32[4,2] = pow(\u001b[96m%v6\u001b[0m, \u001b[96m%v30\u001b[0m)\n",
      "  \u001b[96m%v32\u001b[0m: f32[4,2] = mul(\u001b[96m%v9\u001b[0m, \u001b[96m%v31\u001b[0m)\n",
      "  \u001b[96m%v33\u001b[0m: f32[4,2] = mul(\u001b[96m%v26\u001b[0m, \u001b[96m%v32\u001b[0m)\n",
      "  \u001b[96m%v34\u001b[0m: f32[4,2] = zeros(device=Device(type=cpu,id=0), dtype=@float32, shape=(4,2))\n",
      "  \u001b[96m%v35\u001b[0m: f32[4,2] = where(\u001b[96m%v20\u001b[0m, \u001b[96m%v33\u001b[0m, \u001b[96m%v34\u001b[0m)\n",
      "  \u001b[96m%v36\u001b[0m: f32[3,2] = matmul(\u001b[96m%v16\u001b[0m, \u001b[96m%v35\u001b[0m)\n",
      "  \u001b[96m%v37\u001b[0m: f32[1,2] = reduce_sum(\u001b[96m%v35\u001b[0m, axis=0, keepdims=True)\n",
      "  \u001b[96m%v38\u001b[0m: f32[2] = squeeze(\u001b[96m%v37\u001b[0m, axis=0)\n",
      "  \u001b[1mreturn\u001b[0m (\u001b[96m%v15\u001b[0m, \u001b[96m%v36\u001b[0m, \u001b[96m%v38\u001b[0m)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create small inputs\n",
    "params = {\n",
    "    \"W\": nb.Tensor.from_dlpack(np.random.randn(3, 2).astype(np.float32)),\n",
    "    \"b\": nb.Tensor.from_dlpack(np.zeros(2, dtype=np.float32)),\n",
    "}\n",
    "x_in = nb.Tensor.from_dlpack(np.random.randn(4, 3).astype(np.float32))\n",
    "target = nb.Tensor.from_dlpack(np.random.randn(4, 2).astype(np.float32))\n",
    "\n",
    "# Trace the training step\n",
    "train_step = nb.value_and_grad(loss_fn, argnums=0)\n",
    "traced_train = trace(train_step, params, x_in, target)\n",
    "print(traced_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a16f42",
   "metadata": {},
   "source": [
    "You can see the full forward+backward pass for a training step. This is exactly\n",
    "the graph that `@nb.compile` would optimize and execute efficiently.\n",
    "\n",
    "The operations flow: `matmul → add → relu → subtract → square → mean` (forward),\n",
    "then the reverse chain computes gradients for `W` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cb4a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | What it does |\n",
    "|---------|-------------|\n",
    "| `trace(fn, *args)` | Captures the computation graph without executing it |\n",
    "| `grad` | Graph → graph: adds backward (chain rule) operations |\n",
    "| `vmap` | Graph → graph: adds batch dimensions to all ops |\n",
    "| `compile` | Traces → optimizes → compiles the graph for hardware |\n",
    "| Composition | Transforms stack: `vmap(grad(f))` works automatically |\n",
    "\n",
    "Understanding tracing helps you reason about what Nabla does under the hood,\n",
    "debug unexpected behavior, and write more efficient code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
