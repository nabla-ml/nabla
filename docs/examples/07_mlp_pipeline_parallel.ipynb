{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c60be0",
   "metadata": {},
   "source": [
    "# Example 6: Pipeline Parallelism (GPipe)\n",
    "\n",
    "**Pipeline parallelism** splits a model into sequential stages across devices.\n",
    "Instead of running each input through all stages sequentially, we overlap\n",
    "computation by feeding new micro-batches into the pipeline while earlier ones\n",
    "are still being processed at later stages.\n",
    "\n",
    "```\n",
    "Time →  t0    t1    t2    t3    t4    t5    t6\n",
    "Stage 0  [x0]  [x1]  [x2]  [x3]   ·     ·     ·\n",
    "Stage 1   ·    [x0]  [x1]  [x2]  [x3]   ·     ·\n",
    "Stage 2   ·     ·    [x0]  [x1]  [x2]  [x3]   ·\n",
    "Stage 3   ·     ·     ·    [x0]  [x1]  [x2]  [x3]\n",
    "                                  ↑ results start emerging\n",
    "```\n",
    "\n",
    "In this example we'll:\n",
    "1. Shard an MLP across 4 pipeline stages\n",
    "2. Use `ppermute` for stage-to-stage communication\n",
    "3. Compute gradients through the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1a843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Pipeline Parallelism example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec, PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "print(\"Nabla Pipeline Parallelism example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac553",
   "metadata": {},
   "source": [
    "## 1. Setup: Model and Device Mesh\n",
    "\n",
    "We'll use a simple 4-layer MLP, with **each layer on a separate pipeline stage**.\n",
    "A `DeviceMesh` defines the logical layout of devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efee855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mesh: 4 stages\n",
      "Pipeline: 8 micro-batches × 4 samples\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "STAGES = 4           # Number of pipeline stages (= devices)\n",
    "MICRO_BATCHES = 8    # Number of micro-batches to stream through\n",
    "MICRO_BATCH_SIZE = 4 # Samples per micro-batch\n",
    "DIM = 16             # Hidden dimension\n",
    "\n",
    "# Create a 1D device mesh for pipeline parallelism\n",
    "mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "print(f\"Device mesh: {STAGES} stages\")\n",
    "print(f\"Pipeline: {MICRO_BATCHES} micro-batches × {MICRO_BATCH_SIZE} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cc930",
   "metadata": {},
   "source": [
    "## 2. Pipeline Primitives\n",
    "\n",
    "Each pipeline stage applies one linear layer followed by ReLU.\n",
    "We define three small helpers:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `stage_compute` | Apply one stage's weight + bias → ReLU |\n",
    "| `pipeline_step` | One tick: compute → shift → inject next micro-batch |\n",
    "| `pipeline_loop` | Iterate steps, collecting outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w, b):\n",
    "    \"\"\"One pipeline stage: linear + ReLU.\"\"\"\n",
    "    return ops.relu(ops.matmul(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3faf9",
   "metadata": {},
   "source": [
    "`pipeline_step` is the core of GPipe: after computing all stages in parallel,\n",
    "`ppermute` shifts outputs one stage forward (stage 0→1, 1→2, ...).\n",
    "The last stage's result is extracted via a mask, and the fresh micro-batch is\n",
    "injected into stage 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6020d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_step(\n",
    "    current_state, fresh_input, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract result -> inject input.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack, bias_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    # Extract the final stage's output (mask selects stage 0 after the shift)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    # Inject the fresh micro-batch at stage 0, pass shifted activations elsewhere\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d8e7",
   "metadata": {},
   "source": [
    "The **pipeline loop** feeds `MICRO_BATCHES + STAGES` ticks through the pipeline.\n",
    "During the first `STAGES - 1` ticks the pipeline is \"filling up\"; results\n",
    "start emerging at tick `STAGES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ce3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_loop(\n",
    "    padded_inputs, weight_stack, bias_stack, current_state, mask_0,\n",
    "    step_fn, perm, total_steps,\n",
    "):\n",
    "    \"\"\"Stream micro-batches through the pipeline for `total_steps` ticks.\"\"\"\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, bias_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d4067",
   "metadata": {},
   "source": [
    "## 3. Shard Data Across Stages\n",
    "\n",
    "Each weight matrix lives on one stage. We use `ops.shard` with a\n",
    "`PartitionSpec` to place the first dimension on the `\"stage\"` mesh axis.\n",
    "We also need zero-padded inputs (the pipeline needs `STAGES` empty ticks\n",
    "to fill up) and a boolean mask that selects stage 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69787a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights sharded: [Dim(4), Dim(16), Dim(16)], Inputs padded: [Dim(12), Dim(4), Dim(16)]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Random weights (one per stage), inputs, and targets\n",
    "w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "b_np = np.random.randn(STAGES, DIM).astype(np.float32)\n",
    "x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "y_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "# Shard weights: first axis → \"stage\" mesh axis\n",
    "w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "b_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None)]\n",
    "\n",
    "w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec)\n",
    "b_sharded = ops.shard(nb.Tensor.from_dlpack(b_np), mesh, b_spec)\n",
    "\n",
    "# Pad inputs with STAGES zero-slices for pipeline warm-up\n",
    "padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "x_padded = nb.Tensor.from_dlpack(np.concatenate([x_np, padding], axis=0))\n",
    "y_nb = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "# Initial pipeline state: zeros on each stage\n",
    "state_sharded = ops.shard(\n",
    "    nb.zeros((STAGES, MICRO_BATCH_SIZE, DIM)), mesh, w_spec\n",
    ")\n",
    "\n",
    "# Stage-0 mask for injecting fresh inputs\n",
    "mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "mask_0 = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec)\n",
    "\n",
    "nb.realize_all(w_sharded, b_sharded, state_sharded, mask_0)\n",
    "print(f\"Weights sharded: {w_sharded.shape}, Inputs padded: {x_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691456b9",
   "metadata": {},
   "source": [
    "## 4. Communication & Vectorized Stages\n",
    "\n",
    "`ppermute` shifts tensors between devices according to a permutation list.\n",
    "For a 4-stage pipeline, stage *i* sends its output to stage *i+1* (with wrap-around):\n",
    "\n",
    "```\n",
    "perm = [(0,1), (1,2), (2,3), (3,0)]\n",
    "```\n",
    "\n",
    "We then use `vmap` with `spmd_axis_name=\"stage\"` to auto-vectorize\n",
    "`stage_compute` over the stage dimension — each stage computes with its\n",
    "own weight/bias slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ad355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppermute permutation: [(0, 1), (1, 2), (2, 3), (3, 0)]\n",
      "step_fn ready — each stage runs its own weights in parallel\n"
     ]
    }
   ],
   "source": [
    "# Build the circular permutation for ppermute\n",
    "idx = mesh.axis_names.index(\"stage\")\n",
    "size = mesh.shape[idx]\n",
    "perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "print(f\"ppermute permutation: {perm}\")\n",
    "\n",
    "# Vectorize stage_compute over the stage axis\n",
    "step_fn = vmap(\n",
    "    stage_compute,\n",
    "    in_axes=(0, 0, 0),\n",
    "    out_axes=0,\n",
    "    spmd_axis_name=\"stage\",\n",
    "    mesh=mesh,\n",
    ")\n",
    "print(\"step_fn ready — each stage runs its own weights in parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2b9b4",
   "metadata": {},
   "source": [
    "## 5. Define the Pipeline Loss\n",
    "\n",
    "The loss function runs the full pipeline loop, slices out the valid outputs\n",
    "(the first `STAGES` ticks produce incomplete results), and computes MSE\n",
    "against the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258eead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_loss(inputs, weights, biases, state, mask, targets):\n",
    "    \"\"\"MSE loss through the full GPipe pipeline.\"\"\"\n",
    "    total_steps = MICRO_BATCHES + STAGES\n",
    "    stream_outputs, _ = pipeline_loop(\n",
    "        inputs, weights, biases, state, mask, step_fn, perm, total_steps\n",
    "    )\n",
    "\n",
    "    # Slice valid range [STAGES : STAGES + MICRO_BATCHES]\n",
    "    indices = ops.arange(STAGES, STAGES + MICRO_BATCHES)\n",
    "    valid_preds = ops.gather(stream_outputs, indices, axis=0)\n",
    "\n",
    "    # MSE loss\n",
    "    diff = valid_preds - targets\n",
    "    return ops.mean(diff * diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0722dc",
   "metadata": {},
   "source": [
    "## 6. Compute Gradients Through the Pipeline\n",
    "\n",
    "`nb.grad` differentiates through the entire pipeline — including `ppermute`\n",
    "shifts and per-stage `vmap` — computing gradients for inputs, weights, and\n",
    "biases simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c247d150",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to create op 'mo_gather':\nInputs:\n    result = TensorType(dtype=float32, shape=[Dim(8), Dim(4), Dim(16)], device=cpu:0)\n    input = TensorValue(dtype=float32, shape=[Dim(12), Dim(4), Dim(16)], device=cpu:0)\n    indices = TensorValue(dtype=float32, shape=[Dim(8)], device=cpu:0)\n    axis = TensorValue(dtype=int64, shape=[], device=cpu:0)\n\nDiagnostics:\n    \nDiagnostics:\n    \nVerification failed:\nerror: unknown: 'rmo.mo.gather' op operand #1 must be tensor with integer or index elements, but got '!mo.tensor<[8], f32>'\n note: unknown: see current operation: %884 = \"rmo.mo.gather\"(%879, %455, %883) <{outputParamDecls = #kgen<param.decls[]>}> : (!mo.tensor<[12, 4, 16], f32>, !mo.tensor<[8], f32>, !mo.tensor<[], si64>) -> !mo.tensor<[8, 4, 16], f32>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m x_grad, w_grad, b_grad = grad_fn(\n\u001b[32m      4\u001b[39m     x_padded, w_sharded, b_sharded, state_sharded, mask_0, y_nb\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Materialize results as numpy arrays\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m x_grad_np, w_grad_np, b_grad_np = \u001b[43mnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m x_grad_np = x_grad_np[:MICRO_BATCHES]  \u001b[38;5;66;03m# trim padding region\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput gradient shape:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_grad_np.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/tensor/api.py:1042\u001b[39m, in \u001b[36mTensor.to_numpy_all\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unrealized:\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unrealized) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m         \u001b[43mGRAPH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43munrealized\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43munrealized\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1044\u001b[39m         GRAPH.evaluate(unrealized[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:538\u001b[39m, in \u001b[36mComputeGraph.evaluate\u001b[39m\u001b[34m(self, tensor, return_model, *extra_outputs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Replay trace to build MAX graph\u001b[39;00m\n\u001b[32m    537\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: replaying trace to build graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_replay_trace_to_build_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Build graph outputs\u001b[39;00m\n\u001b[32m    541\u001b[39m _debug_eval(\u001b[33m\"\u001b[39m\u001b[33mmiss: building graph outputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/graph/engine.py:793\u001b[39m, in \u001b[36mComputeGraph._replay_trace_to_build_graph\u001b[39m\u001b[34m(self, targets)\u001b[39m\n\u001b[32m    790\u001b[39m op_args = pytree.tree_map(to_tensor, opnode.op_args)\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     raw_result = \u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m executed_ops += \u001b[32m1\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# Extract graph values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/base.py:191\u001b[39m, in \u001b[36mOperation.execute\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m mesh = spmd.get_mesh_from_args(args)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m GRAPH.graph:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     shard_results = \u001b[43mspmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_on_shards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapted_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m output_sharding = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._infer_output_sharding:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/core/sharding/spmd.py:441\u001b[39m, in \u001b[36mexecute_on_shards\u001b[39m\u001b[34m(op_fn, args, kwargs, mesh, input_shardings, op)\u001b[39m\n\u001b[32m    438\u001b[39m     local_kwargs = op._transform_shard_kwargs(kwargs, output_sharding, i, args)\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Execute with unified kernel signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m result = \u001b[43mop_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[32m1\u001b[39m:\n\u001b[32m    446\u001b[39m     results.append(result[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/ops/view/indexing.py:162\u001b[39m, in \u001b[36mGatherOp.kernel\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [ops.gather_nd(x, indices, batch_dims=batch_dims)]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/ops/gather.py:54\u001b[39m, in \u001b[36mgather\u001b[39m\u001b[34m(input, indices, axis)\u001b[39m\n\u001b[32m     52\u001b[39m output_shape = [*shape[:axis], *indices.shape, *shape[axis + \u001b[32m1\u001b[39m :]]\n\u001b[32m     53\u001b[39m assert_same_device(\u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m, indices=indices)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrmo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmo_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDeviceRef\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCPU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/graph.py:788\u001b[39m, in \u001b[36mGraph._add_op\u001b[39m\u001b[34m(self, op, *args, **kwargs)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, op, *args, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[Value[Any]]:  \u001b[38;5;66;03m# noqa: ANN001\u001b[39;00m\n\u001b[32m    787\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for clients that only require the op results.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     results, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_op_get_op_with_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/venv/lib/python3.13/site-packages/max/graph/graph.py:848\u001b[39m, in \u001b[36mGraph._add_op_get_op_with_results\u001b[39m\u001b[34m(self, op, _ip, *args, **kwargs)\u001b[39m\n\u001b[32m    846\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    847\u001b[39m             mapped_args = {\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(args), **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    849\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to create op \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInputs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    850\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    851\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mapped_args.items()\n\u001b[32m    852\u001b[39m             )\n\u001b[32m    853\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    854\u001b[39m             \u001b[38;5;66;03m# Intentionally suppress extra stack traces from max._mlir.\u001b[39;00m\n\u001b[32m    855\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    857\u001b[39m _set_output_param_decls(Operation._from_cmlir(staged_op), \u001b[38;5;28mself\u001b[39m._params)\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, mlir.Operation | mlir.OpView):\n",
      "\u001b[31mValueError\u001b[39m: Failed to create op 'mo_gather':\nInputs:\n    result = TensorType(dtype=float32, shape=[Dim(8), Dim(4), Dim(16)], device=cpu:0)\n    input = TensorValue(dtype=float32, shape=[Dim(12), Dim(4), Dim(16)], device=cpu:0)\n    indices = TensorValue(dtype=float32, shape=[Dim(8)], device=cpu:0)\n    axis = TensorValue(dtype=int64, shape=[], device=cpu:0)\n\nDiagnostics:\n    \nDiagnostics:\n    \nVerification failed:\nerror: unknown: 'rmo.mo.gather' op operand #1 must be tensor with integer or index elements, but got '!mo.tensor<[8], f32>'\n note: unknown: see current operation: %884 = \"rmo.mo.gather\"(%879, %455, %883) <{outputParamDecls = #kgen<param.decls[]>}> : (!mo.tensor<[12, 4, 16], f32>, !mo.tensor<[8], f32>, !mo.tensor<[], si64>) -> !mo.tensor<[8, 4, 16], f32>"
     ]
    }
   ],
   "source": [
    "grad_fn = nb.grad(pipeline_loss, argnums=(0, 1, 2), realize=False)\n",
    "\n",
    "x_grad, w_grad, b_grad = grad_fn(\n",
    "    x_padded, w_sharded, b_sharded, state_sharded, mask_0, y_nb\n",
    ")\n",
    "\n",
    "# Materialize results as numpy arrays\n",
    "x_grad_np, w_grad_np, b_grad_np = nb.Tensor.to_numpy_all(x_grad, w_grad, b_grad)\n",
    "x_grad_np = x_grad_np[:MICRO_BATCHES]  # trim padding region\n",
    "\n",
    "print(f\"Input gradient shape:  {x_grad_np.shape}\")\n",
    "print(f\"Weight gradient shape: {w_grad_np.shape}\")\n",
    "print(f\"Bias gradient shape:   {b_grad_np.shape}\")\n",
    "print(f\"Weight grad range:     [{w_grad_np.min():.4f}, {w_grad_np.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd231e1",
   "metadata": {},
   "source": [
    "## 7. Verify Against JAX Reference\n",
    "\n",
    "As a sanity check, we run the same computation sequentially in JAX and\n",
    "compare gradients. The pipeline scheduling should not change the mathematical\n",
    "result — only how computation is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "    def jax_ref(x, params_w, params_b, y):\n",
    "        def apply(curr, w, b):\n",
    "            return jax.nn.relu(curr @ w + b)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(MICRO_BATCHES):\n",
    "            a = x[i]\n",
    "            for w, b in zip(params_w, params_b, strict=False):\n",
    "                a = apply(a, w, b)\n",
    "            preds.append(a)\n",
    "        preds = jnp.stack(preds)\n",
    "        return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "    grad_ref_fn = jax.jit(jax.grad(jax_ref, argnums=(0, 1, 2)))\n",
    "    x_grad_ref, w_grad_ref, b_grad_ref = grad_ref_fn(x_np, w_np, b_np, y_np)\n",
    "\n",
    "    x_diff = np.max(np.abs(x_grad_np - x_grad_ref))\n",
    "    w_diff = np.max(np.abs(w_grad_np - w_grad_ref))\n",
    "    b_diff = np.max(np.abs(b_grad_np - b_grad_ref))\n",
    "\n",
    "    print(f\"Max input grad diff:  {x_diff:.6f}\")\n",
    "    print(f\"Max weight grad diff: {w_diff:.6f}\")\n",
    "    print(f\"Max bias grad diff:   {b_diff:.6f}\")\n",
    "    assert w_diff < 5e-4 and b_diff < 5e-4 and x_diff < 5e-4, \"Gradient mismatch!\"\n",
    "    print(\"✅ Nabla pipeline gradients match JAX sequential reference\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"JAX not installed — skipping reference comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b67ebf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- `DeviceMesh` + `PartitionSpec` place tensors on specific stages\n",
    "- `ppermute` handles inter-stage communication without explicit send/recv\n",
    "- `vmap` with `spmd_axis_name` vectorizes computation across stages\n",
    "- `nb.grad` differentiates through the entire sharded pipeline\n",
    "\n",
    "**Previous:** [05b — Transformer (JAX-style)](05b_transformer_jax.ipynb) · **Next:** [07 — Pipeline + Data Parallelism](07_mlp_pp_dp_training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
