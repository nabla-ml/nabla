{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657a618b",
   "metadata": {},
   "source": [
    "# Example 2: Automatic Differentiation\n",
    "\n",
    "Nabla provides a **JAX-like functional autodiff** system built on composable\n",
    "transforms. Every transform is a higher-order function: it takes a function\n",
    "and returns a new function that computes derivatives.\n",
    "\n",
    "| Transform | Mode | Computes |\n",
    "|-----------|------|----------|\n",
    "| `grad` | Reverse | Gradient of scalar-valued function |\n",
    "| `value_and_grad` | Reverse | (value, gradient) pair |\n",
    "| `jvp` | Forward | Jacobian-vector product |\n",
    "| `vjp` | Reverse | Vector-Jacobian product |\n",
    "| `jacrev` | Reverse | Full Jacobian matrix |\n",
    "| `jacfwd` | Forward | Full Jacobian matrix |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57275c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla autodiff example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla autodiff example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72441e47",
   "metadata": {},
   "source": [
    "## 1. `grad` — Gradient of a Scalar Function\n",
    "\n",
    "`nb.grad(fn)` returns a function that computes the gradient of `fn`\n",
    "with respect to specified arguments (default: first argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e34f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = x^3 + 2x^2 - 5x + 3\n",
      "f'(x) = 3x^2 + 4x - 5\n",
      "f'(2.0) = 3*4 + 4*2 - 5 = 15\n",
      "Nabla grad: Tensor([15.] : f32[1])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"f(x) = x^3 + 2x^2 - 5x + 3, so f'(x) = 3x^2 + 4x - 5.\"\"\"\n",
    "    return x ** 3 + 2.0 * x ** 2 - 5.0 * x + 3.0\n",
    "\n",
    "df = nb.grad(f)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([2.0], dtype=np.float32))\n",
    "grad_val = df(x)\n",
    "print(f\"f(x) = x^3 + 2x^2 - 5x + 3\")\n",
    "print(f\"f'(x) = 3x^2 + 4x - 5\")\n",
    "print(f\"f'(2.0) = 3*4 + 4*2 - 5 = {3*4 + 4*2 - 5}\")\n",
    "print(f\"Nabla grad: {grad_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468cb38",
   "metadata": {},
   "source": [
    "## 2. `value_and_grad` — Value and Gradient Together\n",
    "\n",
    "Often you need both the function value and its gradient. This is more\n",
    "efficient than calling `f` and `grad(f)` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86db4172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1, 2, 3]\n",
      "f(x) = sum(x^2) = Tensor(14. : f32[])\n",
      "grad(f) = 2x = Tensor([2. 4. 6.] : f32[3])\n"
     ]
    }
   ],
   "source": [
    "def quadratic(x):\n",
    "    \"\"\"f(x) = sum(x^2), so grad = 2x.\"\"\"\n",
    "    return nb.reduce_sum(x * x)\n",
    "\n",
    "val_and_grad_fn = nb.value_and_grad(quadratic)\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "value, gradient = val_and_grad_fn(x)\n",
    "print(f\"x = [1, 2, 3]\")\n",
    "print(f\"f(x) = sum(x^2) = {value}\")\n",
    "print(f\"grad(f) = 2x = {gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89de59e",
   "metadata": {},
   "source": [
    "### Multiple Arguments with `argnums`\n",
    "\n",
    "Use `argnums` to specify which arguments to differentiate with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a8749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad w.r.t. w: Tensor([3. 4.] : f32[2])\n",
      "  (should be x = [3, 4])\n",
      "grad w.r.t. x: Tensor([1. 2.] : f32[2])\n",
      "  (should be w = [1, 2])\n",
      "grad w.r.t. (w, x): (Tensor([3. 4.] : f32[2]), Tensor([1. 2.] : f32[2]))\n"
     ]
    }
   ],
   "source": [
    "def weighted_sum(w, x):\n",
    "    \"\"\"f(w, x) = sum(w * x).\"\"\"\n",
    "    return nb.reduce_sum(w * x)\n",
    "\n",
    "# Gradient w.r.t. first arg (w) only — default\n",
    "grad_w = nb.grad(weighted_sum, argnums=0)\n",
    "w = nb.Tensor.from_dlpack(np.array([1.0, 2.0], dtype=np.float32))\n",
    "x = nb.Tensor.from_dlpack(np.array([3.0, 4.0], dtype=np.float32))\n",
    "print(f\"grad w.r.t. w: {grad_w(w, x)}\")\n",
    "print(f\"  (should be x = [3, 4])\")\n",
    "\n",
    "# Gradient w.r.t. second arg (x)\n",
    "grad_x = nb.grad(weighted_sum, argnums=1)\n",
    "print(f\"grad w.r.t. x: {grad_x(w, x)}\")\n",
    "print(f\"  (should be w = [1, 2])\")\n",
    "\n",
    "# Gradient w.r.t. both — returns a tuple\n",
    "grad_both = nb.grad(weighted_sum, argnums=(0, 1))\n",
    "gw, gx = grad_both(w, x)\n",
    "print(f\"grad w.r.t. (w, x): ({gw}, {gx})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de2a5e",
   "metadata": {},
   "source": [
    "## 3. `jvp` — Forward-Mode (Jacobian-Vector Product)\n",
    "\n",
    "`nb.jvp(fn, primals, tangents)` computes:\n",
    "- The function output `fn(*primals)`\n",
    "- The directional derivative `J @ tangents` (JVP)\n",
    "\n",
    "This is efficient when the number of **inputs** is small (one forward pass\n",
    "per tangent direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85829d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g([3, 2]) = [3^2 + 2, 3*2] = Tensor([11.  6.] : f32[2])\n",
      "JVP with v=[1,0] (column 1 of Jacobian):\n",
      "  J @ v = Tensor([6. 2.] : f32[2])\n",
      "  Expected: [2*3, 2] = [6, 2]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    \"\"\"g(x) = [x0^2 + x1, x0 * x1].\"\"\"\n",
    "    r0 = nb.reshape(x[0] ** 2 + x[1], (1,))\n",
    "    r1 = nb.reshape(x[0] * x[1], (1,))\n",
    "    return nb.concatenate([r0, r1], axis=0)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([3.0, 2.0], dtype=np.float32))\n",
    "v = nb.Tensor.from_dlpack(np.array([1.0, 0.0], dtype=np.float32))\n",
    "\n",
    "output, jvp_val = nb.jvp(g, (x,), (v,))\n",
    "print(f\"g([3, 2]) = [3^2 + 2, 3*2] = {output}\")\n",
    "print(f\"JVP with v=[1,0] (column 1 of Jacobian):\")\n",
    "print(f\"  J @ v = {jvp_val}\")\n",
    "print(f\"  Expected: [2*3, 2] = [6, 2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20f8cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JVP with v=[0,1] (column 2 of Jacobian):\n",
      "  J @ v = Tensor([1. 3.] : f32[2])\n",
      "  Expected: [1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Second column of the Jacobian\n",
    "v2 = nb.Tensor.from_dlpack(np.array([0.0, 1.0], dtype=np.float32))\n",
    "_, jvp_val2 = nb.jvp(g, (x,), (v2,))\n",
    "print(f\"JVP with v=[0,1] (column 2 of Jacobian):\")\n",
    "print(f\"  J @ v = {jvp_val2}\")\n",
    "print(f\"  Expected: [1, 3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4ee0f",
   "metadata": {},
   "source": [
    "## 4. `vjp` — Reverse-Mode (Vector-Jacobian Product)\n",
    "\n",
    "`nb.vjp(fn, *primals)` returns `(output, vjp_fn)` where `vjp_fn(cotangent)`\n",
    "gives the VJP = `cotangent @ J`.\n",
    "\n",
    "This is efficient when the number of **outputs** is small (one backward pass\n",
    "per cotangent direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ce0024",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nabla.core.autograd.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m A @ x  \u001b[38;5;66;03m# (2,3) @ (3,) = (2,)\u001b[39;00m\n\u001b[32m      8\u001b[39m x = nb.Tensor.from_dlpack(np.array([\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m], dtype=np.float32))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m output, vjp_fn = \u001b[43mnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mf(x) = Ax, A = [[1,0,2],[0,3,1]], x = [1,2,3]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mf(x) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CodingProjects/nabla/nabla/transforms/vjp.py:42\u001b[39m, in \u001b[36mvjp\u001b[39m\u001b[34m(fn, has_aux, create_graph, *primals)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvjp\u001b[39m(\n\u001b[32m     29\u001b[39m     fn: Callable[..., Any],\n\u001b[32m     30\u001b[39m     *primals: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     | \u001b[38;5;28mtuple\u001b[39m[Any, Callable[..., \u001b[38;5;28mtuple\u001b[39m[Any, ...]], Any]\n\u001b[32m     36\u001b[39m ):\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute VJP of *fn* at *primals*. Returns ``(output, vjp_fn[, aux])``.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    *create_graph* defaults to ``True`` so the returned pullback always\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    produces differentiable gradients, enabling nested Jacobian compositions.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backward_on_trace\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pytree\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trace \u001b[38;5;28;01mas\u001b[39;00m capture_trace\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nabla.core.autograd.utils'"
     ]
    }
   ],
   "source": [
    "def mat_fn(x):\n",
    "    \"\"\"f(x) = Ax, where A is 2x3 and x is (3,). Output is (2,).\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[1.0, 0.0, 2.0], [0.0, 3.0, 1.0]], dtype=np.float32)\n",
    "    )\n",
    "    return A @ x  # (2,3) @ (3,) = (2,)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "output, vjp_fn = nb.vjp(mat_fn, x)\n",
    "print(f\"f(x) = Ax, A = [[1,0,2],[0,3,1]], x = [1,2,3]\")\n",
    "print(f\"f(x) = {output}\")\n",
    "print(f\"  Expected: [1+0+6, 0+6+3] = [7, 9]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04868eeb",
   "metadata": {},
   "source": [
    "### Pulling Back Cotangent Vectors\n",
    "\n",
    "The VJP returns a **pullback function** `vjp_fn`. Given a cotangent vector $v$ (same shape as the output), it computes $v^\\top J$ — the vector-Jacobian product. Each standard basis cotangent extracts one row of $J^\\top = A^\\top$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a08799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VJP with cotangent [1, 0] — gives first row of A^T\n",
    "v1 = nb.Tensor.from_dlpack(np.array([1.0, 0.0], dtype=np.float32))\n",
    "(vjp1,) = vjp_fn(v1)\n",
    "print(f\"VJP with v=[1,0]: {vjp1}\")\n",
    "print(f\"  Expected: A^T @ [1,0] = [1, 0, 2]\")\n",
    "\n",
    "# VJP with cotangent [0, 1] — gives second row of A^T\n",
    "v2 = nb.Tensor.from_dlpack(np.array([0.0, 1.0], dtype=np.float32))\n",
    "(vjp2,) = vjp_fn(v2)\n",
    "print(f\"VJP with v=[0,1]: {vjp2}\")\n",
    "print(f\"  Expected: A^T @ [0,1] = [0, 3, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81827a",
   "metadata": {},
   "source": [
    "## 5. `jacrev` — Full Jacobian via Reverse Mode\n",
    "\n",
    "`nb.jacrev(fn)` computes the full Jacobian matrix using reverse-mode\n",
    "autodiff (one backward pass per output element, batched via vmap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009773b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian via jacrev:\n",
      "Tensor(\n",
      "  [[ 2.5403 -1.    ]\n",
      "   [ 1.      3.8776]] : f32[2,2]\n",
      ")\n",
      "Expected: A + diag(cos(x))\n",
      "  [[2+cos(1), -1     ],\n",
      "   [1,        3+cos(0.5)]]\n",
      "  ≈ [[2.5403, -1.0000],\n",
      "     [1.0000, 3.8776]]\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    \"\"\"h(x) = Ax + sin(x), nonlinear vector function R^2 -> R^2.\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[2.0, -1.0], [1.0, 3.0]], dtype=np.float32)\n",
    "    )\n",
    "    return A @ x + nb.sin(x)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 0.5], dtype=np.float32))\n",
    "J = nb.jacrev(h)(x)\n",
    "print(\"Jacobian via jacrev:\")\n",
    "print(J)\n",
    "print(\"Expected: A + diag(cos(x))\")\n",
    "print(f\"  [[2+cos(1), -1     ],\")\n",
    "print(f\"   [1,        3+cos(0.5)]]\")\n",
    "print(f\"  ≈ [[{2+np.cos(1):.4f}, {-1:.4f}],\")\n",
    "print(f\"     [{1:.4f}, {3+np.cos(0.5):.4f}]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc00df4",
   "metadata": {},
   "source": [
    "## 6. `jacfwd` — Full Jacobian via Forward Mode\n",
    "\n",
    "`nb.jacfwd(fn)` computes the same Jacobian using forward-mode autodiff\n",
    "(one JVP per input element, batched via vmap). Prefer `jacfwd` when\n",
    "inputs are few and outputs are many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian via jacfwd:\n",
      "Tensor(\n",
      "  [[ 2.5403 -1.    ]\n",
      "   [ 1.      3.8776]] : f32[2,2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "J_fwd = nb.jacfwd(h)(x)\n",
    "print(\"Jacobian via jacfwd:\")\n",
    "print(J_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0ded3",
   "metadata": {},
   "source": [
    "### When to use `jacrev` vs `jacfwd`\n",
    "\n",
    "| Scenario | Prefer |\n",
    "|----------|--------|\n",
    "| Few outputs, many inputs | `jacrev` |\n",
    "| Few inputs, many outputs | `jacfwd` |\n",
    "| Square Jacobian | Either works |\n",
    "| Hessian (second derivative) | Compose both! |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757e80a",
   "metadata": {},
   "source": [
    "## 7. Hessians — Composing Transforms\n",
    "\n",
    "Because Nabla's transforms are **composable**, you can compute Hessians\n",
    "(second-order derivatives) by nesting Jacobian transforms.\n",
    "\n",
    "For a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the Hessian\n",
    "$H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$ can be computed\n",
    "in multiple ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8bc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = x0^2 * x1 + x1^3\n",
      "x = Tensor([2. 3.] : f32[2])\n",
      "f(x) = Tensor(39. : f32[])\n",
      "\n",
      "Analytical Hessian at x=[2,3]:\n",
      "  [[2*x1, 2*x0], [2*x0, 6*x1]] = [[6, 4], [4, 18]]\n",
      "\n",
      "Method 1 — jacfwd(grad(f)):\n",
      "Tensor(\n",
      "  [[ 6.  4.]\n",
      "   [ 4. 18.]] : f32[2,2]\n",
      ")\n",
      "Method 2 — jacrev(grad(f)):\n",
      "Tensor(\n",
      "  [[ 6.  4.]\n",
      "   [ 4. 18.]] : f32[2,2]\n",
      ")\n",
      "Method 3 — jacrev(jacfwd(f)):\n",
      "Tensor(\n",
      "  [[ 6.  4.]\n",
      "   [ 4. 18.]] : f32[2,2]\n",
      ")\n",
      "Method 4 — jacfwd(jacrev(f)):\n",
      "Tensor(\n",
      "  [[ 6.  4.]\n",
      "   [ 4. 18.]] : f32[2,2]\n",
      ")\n",
      "\n",
      "All four methods produce the same Hessian! ✅\n"
     ]
    }
   ],
   "source": [
    "def scalar_fn(x):\n",
    "    \"\"\"f(x) = x0^2 * x1 + x1^3, a polynomial with known Hessian.\"\"\"\n",
    "    return x[0] ** 2 * x[1] + x[1] ** 3\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([2.0, 3.0], dtype=np.float32))\n",
    "print(f\"f(x) = x0^2 * x1 + x1^3\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = {scalar_fn(x)}\")\n",
    "\n",
    "# Analytical Hessian:\n",
    "# df/dx0 = 2*x0*x1,  df/dx1 = x0^2 + 3*x1^2\n",
    "# H = [[2*x1, 2*x0], [2*x0, 6*x1]]\n",
    "# At x=[2,3]:  H = [[6, 4], [4, 18]]\n",
    "print(\"\\nAnalytical Hessian at x=[2,3]:\")\n",
    "print(\"  [[2*x1, 2*x0], [2*x0, 6*x1]] = [[6, 4], [4, 18]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db44ea",
   "metadata": {},
   "source": [
    "### Method 1: `jacfwd(grad(f))` — Forward-over-Reverse\n",
    "\n",
    "The most common approach: first compute the gradient (reverse), then differentiate that gradient function again (forward) to get the full Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = nb.jacfwd(nb.grad(scalar_fn))(x)\n",
    "print(\"Method 1 — jacfwd(grad(f)):\")\n",
    "print(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46070a",
   "metadata": {},
   "source": [
    "### Method 2: `jacrev(grad(f))` — Reverse-over-Reverse\n",
    "\n",
    "Same idea, but use reverse mode for the outer Jacobian. Either combination works — choose based on the shape of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2 = nb.jacrev(nb.grad(scalar_fn))(x)\n",
    "print(\"Method 2 — jacrev(grad(f)):\")\n",
    "print(H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc1fc1",
   "metadata": {},
   "source": [
    "### Method 3: `jacrev(jacfwd(f))` — Full Jacobian Composition\n",
    "\n",
    "You can also compose two Jacobian transforms directly. Since `jacfwd(f)` returns the full Jacobian, wrapping it in `jacrev` differentiates each Jacobian entry — yielding the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3 = nb.jacrev(nb.jacfwd(scalar_fn))(x)\n",
    "print(\"Method 3 — jacrev(jacfwd(f)):\")\n",
    "print(H3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacfe53",
   "metadata": {},
   "source": [
    "### Method 4: `jacfwd(jacrev(f))` — The Reverse Order\n",
    "\n",
    "Swapping the order also works. All four methods produce the identical Hessian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "H4 = nb.jacfwd(nb.jacrev(scalar_fn))(x)\n",
    "print(\"Method 4 — jacfwd(jacrev(f)):\")\n",
    "print(H4)\n",
    "\n",
    "print(\"\\nAll four methods produce the same Hessian! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506a450",
   "metadata": {},
   "source": [
    "## 8. Gradient of a Multi-Variable Loss\n",
    "\n",
    "A more practical example: computing gradients for a simple regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93318933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. weights (shape [Dim(3), Dim(1)]):\n",
      "Tensor(\n",
      "  [[-2.3079]\n",
      "   [ 2.5454]\n",
      "   [-0.4476]] : f32[3,1]\n",
      ")\n",
      "\n",
      "Gradient w.r.t. bias (shape [Dim(1)]):\n",
      "Tensor([0.0037] : f32[1])\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_loss(w, b, X, y):\n",
    "    \"\"\"MSE loss for linear regression: ||Xw + b - y||^2 / n.\"\"\"\n",
    "    predictions = X @ w + b\n",
    "    residuals = predictions - y\n",
    "    return nb.mean(residuals * residuals)\n",
    "\n",
    "# Create data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 3\n",
    "X = nb.Tensor.from_dlpack(np.random.randn(n_samples, n_features).astype(np.float32))\n",
    "w_true = nb.Tensor.from_dlpack(np.array([[2.0], [-1.0], [0.5]], dtype=np.float32))\n",
    "y = X @ w_true + 0.1 * nb.gaussian((n_samples, 1))\n",
    "\n",
    "# Initialize weights\n",
    "w = nb.zeros((n_features, 1))\n",
    "b = nb.zeros((1,))\n",
    "\n",
    "# Compute gradients\n",
    "grad_fn = nb.grad(linear_regression_loss, argnums=(0, 1))\n",
    "dw, db = grad_fn(w, b, X, y)\n",
    "print(f\"Gradient w.r.t. weights (shape {dw.shape}):\")\n",
    "print(dw)\n",
    "print(f\"\\nGradient w.r.t. bias (shape {db.shape}):\")\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f93e64",
   "metadata": {},
   "source": [
    "## 9. A Simple Gradient Descent\n",
    "\n",
    "Using `value_and_grad` in a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f114ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   Loss        \n",
      "--------------------\n",
      "2      2.658563    \n",
      "4      1.408227    \n",
      "6      0.797857    \n",
      "8      0.478955    \n",
      "10     0.301070    \n",
      "\n",
      "Learned weights: Tensor(\n",
      "  [[ 1.3725]\n",
      "   [-1.022 ]\n",
      "   [ 0.3079]] : f32[3,1]\n",
      ")\n",
      "True weights:    Tensor(\n",
      "  [[ 2. ]\n",
      "   [-1. ]\n",
      "   [ 0.5]] : f32[3,1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "w = nb.zeros((n_features, 1))\n",
    "b = nb.zeros((1,))\n",
    "lr = 0.1\n",
    "\n",
    "vg_fn = nb.value_and_grad(linear_regression_loss, argnums=(0, 1))\n",
    "\n",
    "print(f\"{'Step':<6} {'Loss':<12}\")\n",
    "print(\"-\" * 20)\n",
    "for step in range(10):\n",
    "    loss, (dw, db) = vg_fn(w, b, X, y)\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    if (step + 1) % 2 == 0:\n",
    "        print(f\"{step + 1:<6} {loss.item():<12.6f}\")\n",
    "\n",
    "print(f\"\\nLearned weights: {w}\")\n",
    "print(f\"True weights:    {w_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00b23d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Transform | Input | Output | Best for |\n",
    "|-----------|-------|--------|----------|\n",
    "| `grad(f)` | Scalar fn | Gradient vector | Training losses |\n",
    "| `value_and_grad(f)` | Scalar fn | (value, gradient) | Training loops |\n",
    "| `jvp(f, primals, tangents)` | Any fn | (output, J·v) | Few inputs |\n",
    "| `vjp(f, *primals)` | Any fn | (output, vjp_fn) | Few outputs |\n",
    "| `jacrev(f)` | Any fn | Full Jacobian | Few outputs |\n",
    "| `jacfwd(f)` | Any fn | Full Jacobian | Few inputs |\n",
    "| Compose! | — | Hessians, etc. | Higher-order derivatives |\n",
    "\n",
    "**Next:** [03_graph_tracing](03_graph_tracing)\n",
    "— See what happens *under the hood* when you apply these transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Example 02 completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Example 02 completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
