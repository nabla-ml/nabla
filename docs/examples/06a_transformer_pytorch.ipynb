{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af0fdb",
   "metadata": {},
   "source": [
    "# Example 5a: Transformer Training (PyTorch-Style)\n",
    "\n",
    "We'll build a small Transformer encoder for a synthetic **sequence\n",
    "classification** task: given a sequence of token embeddings, predict which\n",
    "class it belongs to.\n",
    "\n",
    "The model uses Nabla's built-in `TransformerEncoderLayer`, `Embedding`,\n",
    "and `MultiHeadAttention` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d597535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Transformer Training — PyTorch-style\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla Transformer Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120857f",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding\n",
    "\n",
    "We'll use sinusoidal positional encoding, computed as a fixed buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4043dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
    "    div_term = np.exp(\n",
    "        np.arange(0, d_model, 2, dtype=np.float32) * -(np.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe  # (max_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f5f71",
   "metadata": {},
   "source": [
    "## 2. Define the Model\n",
    "\n",
    "Our `TransformerClassifier` is an `nb.nn.Module` subclass with these components:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `nb.nn.Embedding` | Maps token IDs → dense vectors |\n",
    "| Sinusoidal PE | Encodes position information (fixed, not learned) |\n",
    "| `nb.nn.TransformerEncoderLayer` × N | Self-attention + feed-forward blocks |\n",
    "| `nb.nn.Linear` | Classification head |\n",
    "\n",
    "The `__init__` method creates these components; `forward` chains them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab2ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nb.nn.Module):\n",
    "    \"\"\"Transformer encoder for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers,\n",
    "                 num_classes, max_len=128, dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        self.embedding = nb.nn.Embedding(vocab_size, d_model)\n",
    "        pe_np = make_positional_encoding(max_len, d_model)\n",
    "        self.pe = nb.Tensor.from_dlpack(pe_np)  # fixed, not learned\n",
    "\n",
    "        # --- Encoder stack ---\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nb.nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads,\n",
    "                dim_feedforward=dim_feedforward, dropout=0.0,\n",
    "            )\n",
    "            setattr(self, f\"encoder_{i}\", layer)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nb.nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Embed + positional encoding\n",
    "        x = self.embedding(token_ids)\n",
    "        seq_len = token_ids.shape[-1]\n",
    "        pe = nb.slice_tensor(self.pe, start=(0, 0), size=(seq_len, self.d_model))\n",
    "        x = x + pe\n",
    "\n",
    "        # Encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Mean pool + classify\n",
    "        return self.classifier(nb.mean(x, axis=-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5612fd5",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Data\n",
    "\n",
    "Generate a simple classification task:\n",
    "- Sequences of random token IDs\n",
    "- Labels based on a rule (e.g., majority token determines class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f792c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 150 sequences of length 8\n",
      "Vocab size: 20, Classes: 3\n",
      "Sample tokens: [ 6 19 14 10  7  6 18 10]\n",
      "Sample label:  0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "num_classes = 3\n",
    "n_samples = 150\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Generate random token sequences\n",
    "token_ids_np = np.random.randint(0, vocab_size, (n_samples, seq_len)).astype(np.int64)\n",
    "\n",
    "# Labels: class = (sum of tokens) mod num_classes\n",
    "labels_np = (token_ids_np.sum(axis=1) % num_classes).astype(np.int64)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_onehot_np = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "labels_onehot_np[np.arange(n_samples), labels_np] = 1.0\n",
    "\n",
    "token_ids = nb.Tensor.from_dlpack(token_ids_np)\n",
    "labels = nb.Tensor.from_dlpack(labels_onehot_np)\n",
    "\n",
    "print(f\"Dataset: {n_samples} sequences of length {seq_len}\")\n",
    "print(f\"Vocab size: {vocab_size}, Classes: {num_classes}\")\n",
    "print(f\"Sample tokens: {token_ids_np[0]}\")\n",
    "print(f\"Sample label:  {labels_np[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebce09c",
   "metadata": {},
   "source": [
    "## 4. Build Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 2 encoder layers, d_model=32, heads=4\n",
      "Total parameters: 17827\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    max_len=seq_len,\n",
    "    dim_feedforward=64,\n",
    ")\n",
    "model.eval()  # Disable dropout\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {num_layers} encoder layers, d_model={d_model}, heads={num_heads}\")\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "\n",
    "opt_state = nb.nn.optim.adamw_init(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fb1e2",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ea97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, tokens, targets):\n",
    "    \"\"\"Cross-entropy loss on model predictions.\"\"\"\n",
    "    logits = model(tokens)\n",
    "    return nb.nn.functional.cross_entropy_loss(logits, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555f365",
   "metadata": {},
   "source": [
    "### Train with `value_and_grad`\n",
    "\n",
    "Note: Even in PyTorch-style, Nabla uses **functional gradient computation** —\n",
    "`value_and_grad` computes gradients of the loss with respect to the model\n",
    "parameters (specified by `argnums=0`). There is no `.backward()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc6650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    Loss         Accuracy  \n",
      "--------------------------------\n",
      "10       1.4213       28.67%    \n",
      "20       1.1596       33.33%    \n",
      "30       1.0961       35.33%    \n",
      "40       1.0887       39.33%    \n",
      "50       1.0870       38.67%    \n",
      "60       1.0870       36.67%    \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "lr = 1e-3\n",
    "\n",
    "print(f\"{'Epoch':<8} {'Loss':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, token_ids, labels)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(model, grads, opt_state, lr=lr)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        logits = model(token_ids)\n",
    "        pred_classes = nb.argmax(logits, axis=-1)\n",
    "        target_classes = nb.Tensor.from_dlpack(labels_np.astype(np.int64))\n",
    "        correct = nb.equal(pred_classes, target_classes)\n",
    "        accuracy = nb.mean(nb.cast(correct, nb.DType.float32)).item()\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.4f} {accuracy:<10.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebdbbe",
   "metadata": {},
   "source": [
    "## 6. Compiled Training (Bonus)\n",
    "\n",
    "For maximum performance, wrap the training step in `@nb.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9306dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.compile\n",
    "def compiled_step(model, opt_state, tokens, targets):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, tokens, targets)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=1e-3\n",
    "    )\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e66ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled training:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       2.9450      \n",
      "20       2.4014      \n",
      "30       2.0130      \n",
      "\n",
      "The full train step (forward + backward + optimizer) runs as one MAX graph!\n"
     ]
    }
   ],
   "source": [
    "# Fresh model for compiled training\n",
    "model2 = TransformerClassifier(\n",
    "    vocab_size=vocab_size, d_model=d_model, num_heads=num_heads,\n",
    "    num_layers=num_layers, num_classes=num_classes,\n",
    "    max_len=seq_len, dim_feedforward=64,\n",
    ")\n",
    "model2.eval()\n",
    "opt_state2 = nb.nn.optim.adamw_init(model2)\n",
    "\n",
    "print(f\"Compiled training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(30):\n",
    "    model2, opt_state2, loss = compiled_step(model2, opt_state2, token_ids, labels)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.4f}\")\n",
    "\n",
    "print(\"\\nThe full train step (forward + backward + optimizer) runs as one MAX graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9aaa32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | API |\n",
    "|-----------|-----|\n",
    "| Token embedding | `nb.nn.Embedding(vocab_size, d_model)` |\n",
    "| Transformer layer | `nb.nn.TransformerEncoderLayer(d_model, heads, ff_dim)` |\n",
    "| Multi-head attention | `nb.nn.MultiHeadAttention(d_model, heads)` |\n",
    "| Cross-entropy | `nb.nn.functional.cross_entropy_loss(logits, targets)` |\n",
    "| Compiled training | `@nb.compile` on the full train step |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
