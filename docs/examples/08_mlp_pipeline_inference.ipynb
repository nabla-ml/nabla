{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e92132",
   "metadata": {},
   "source": [
    "# Example 8: Pipeline Parallel Inference\n",
    "\n",
    "This example demonstrates **inference-only** pipeline execution (no gradients).\n",
    "Compared to training (Examples 6–7), inference is simpler:\n",
    "- No bias terms (to keep the example minimal)\n",
    "- No loss function or backward pass\n",
    "- Results are compared against sequential NumPy to verify correctness\n",
    "\n",
    "This pattern is useful for serving large models that don't fit on a single device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec, PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "# Pipeline configuration\n",
    "STAGES = 4\n",
    "MICRO_BATCHES = 8\n",
    "MICRO_BATCH_SIZE = 4\n",
    "DIM = 16\n",
    "\n",
    "print(\"Nabla Pipeline Inference example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f65f55",
   "metadata": {},
   "source": [
    "## 1. Inference Pipeline Primitives\n",
    "\n",
    "For inference we only need weights (no biases here).\n",
    "The `stage_compute` applies a single linear layer + ReLU per stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5071d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w):\n",
    "    return ops.relu(ops.matmul(x, w))\n",
    "\n",
    "\n",
    "def pipeline_step(current_state, fresh_input, weight_stack, mask_0, step_fn, perm):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract -> inject.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result\n",
    "\n",
    "\n",
    "def pipeline_inference_loop(\n",
    "    padded_inputs, weight_stack, current_state, mask_0, step_fn, perm, total_steps\n",
    "):\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0438b9",
   "metadata": {},
   "source": [
    "## 2. Shard Weights and Prepare Inputs\n",
    "\n",
    "Same pattern as Example 6 — shard the weight stack across pipeline stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPipe Inference Test on Mesh: @pp = <[\"stage\"=4]>\n",
      "Running Reference...\n",
      "Max Diff: 0.000000\n",
      "✅ SUCCESS\n"
     ]
    }
   ],
   "source": [
    "mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "np.random.seed(42)\n",
    "\n",
    "w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "# Shard weights across stages\n",
    "w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
    "\n",
    "# Pad inputs for pipeline warm-up\n",
    "padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "x_padded = nb.Tensor.from_dlpack(np.concatenate([x_np, padding], axis=0))\n",
    "\n",
    "# Initial state and stage-0 mask\n",
    "state_sharded = ops.shard(\n",
    "    nb.Tensor.from_dlpack(np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)),\n",
    "    mesh, w_spec\n",
    ").realize()\n",
    "\n",
    "mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "mask_0 = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec).realize()\n",
    "\n",
    "print(f\"Mesh: {mesh}\")\n",
    "print(f\"Weights: {w_sharded.shape}, Inputs: {x_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4abab",
   "metadata": {},
   "source": [
    "## 3. Run Inference Pipeline\n",
    "\n",
    "Set up the ppermute permutation, vectorize across stages with `vmap`,\n",
    "and run the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication setup\n",
    "idx = mesh.axis_names.index(\"stage\")\n",
    "size = mesh.shape[idx]\n",
    "perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "\n",
    "# Vectorize stage_compute over the stage axis\n",
    "step_fn = vmap(\n",
    "    stage_compute, in_axes=(0, 0), out_axes=0, spmd_axis_name=\"stage\", mesh=mesh\n",
    ")\n",
    "\n",
    "# Run the full inference pipeline\n",
    "total_steps = MICRO_BATCHES + STAGES\n",
    "results, _ = pipeline_inference_loop(\n",
    "    x_padded, w_sharded, state_sharded, mask_0, step_fn, perm, total_steps\n",
    ")\n",
    "\n",
    "# Extract valid predictions (skip warm-up ticks)\n",
    "preds = results[STAGES : STAGES + MICRO_BATCHES]\n",
    "preds_np = preds.to_numpy()\n",
    "print(f\"Predictions shape: {preds_np.shape}\")\n",
    "print(f\"Output range: [{preds_np.min():.4f}, {preds_np.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5aaff8",
   "metadata": {},
   "source": [
    "## 4. Verify Against Sequential NumPy\n",
    "\n",
    "Run the same computation sequentially in NumPy to confirm the pipeline\n",
    "produces identical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential NumPy reference\n",
    "ref_outs = []\n",
    "for i in range(MICRO_BATCHES):\n",
    "    act = x_np[i]\n",
    "    for s in range(STAGES):\n",
    "        act = np.maximum(act @ w_np[s], 0)  # ReLU(x @ W)\n",
    "    ref_outs.append(act)\n",
    "ref = np.stack(ref_outs)\n",
    "\n",
    "diff = np.max(np.abs(preds_np - ref))\n",
    "print(f\"Max difference vs NumPy reference: {diff:.6f}\")\n",
    "assert diff < 1e-4, f\"Mismatch: {diff}\"\n",
    "print(\"✅ Pipeline inference matches sequential computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d41349",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- Pipeline inference uses the same `ppermute` + `vmap` pattern as training\n",
    "- Without gradients, we simply call the loop directly — no `nb.grad` needed\n",
    "- The pipeline produces numerically identical results to sequential execution\n",
    "\n",
    "**Previous:** [07 — 2D Parallelism (PP+DP)](07_mlp_pp_dp_training.ipynb) · **Next:** [09 — JAX Comparison](09_jax_comparison_compiled.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
