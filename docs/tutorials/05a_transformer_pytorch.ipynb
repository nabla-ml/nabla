{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be778c6",
   "metadata": {},
   "source": [
    "# Tutorial 5a: Transformer Training (PyTorch-Style)\n",
    "\n",
    "We'll build a small Transformer encoder for a synthetic **sequence\n",
    "classification** task: given a sequence of token embeddings, predict which\n",
    "class it belongs to.\n",
    "\n",
    "The model uses Nabla's built-in `TransformerEncoderLayer`, `Embedding`,\n",
    "and `MultiHeadAttention` modules."
   ]
  },
  {
   "cell_type": "code",
   "id": "ef5f00ae",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla Transformer Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22063641",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding\n",
    "\n",
    "We'll use sinusoidal positional encoding, computed as a fixed buffer."
   ]
  },
  {
   "cell_type": "code",
   "id": "4a4812ae",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
    "    div_term = np.exp(\n",
    "        np.arange(0, d_model, 2, dtype=np.float32) * -(np.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe  # (max_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e9433",
   "metadata": {},
   "source": [
    "## 2. Define the Model\n",
    "\n",
    "A small Transformer encoder with:\n",
    "- Learned token embeddings\n",
    "- Sinusoidal positional encoding (fixed buffer)\n",
    "- N Transformer encoder layers\n",
    "- Mean pooling + classification head"
   ]
  },
  {
   "cell_type": "code",
   "id": "81dc0941",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nb.nn.Module):\n",
    "    \"\"\"Transformer encoder for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        max_len: int = 128,\n",
    "        dim_feedforward: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token embedding\n",
    "        self.embedding = nb.nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding (fixed, not learned)\n",
    "        pe_np = make_positional_encoding(max_len, d_model)\n",
    "        self.pe = nb.Tensor.from_dlpack(pe_np)\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nb.nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=0.0,  # No dropout for deterministic training\n",
    "            )\n",
    "            setattr(self, f\"encoder_{i}\", layer)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nb.nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            token_ids: Integer tensor of shape (batch, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Logits of shape (batch, num_classes).\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        x = self.embedding(token_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Add positional encoding (sliced to sequence length)\n",
    "        seq_len = token_ids.shape[-1]\n",
    "        pe_slice = nb.slice_tensor(\n",
    "            self.pe, start=(0, 0), size=(seq_len, self.d_model)\n",
    "        )\n",
    "        x = x + pe_slice\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Mean pooling over sequence dimension\n",
    "        x = nb.mean(x, axis=-2)  # (batch, d_model)\n",
    "\n",
    "        # Classify\n",
    "        return self.classifier(x)  # (batch, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d04ca",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Data\n",
    "\n",
    "Generate a simple classification task:\n",
    "- Sequences of random token IDs\n",
    "- Labels based on a rule (e.g., majority token determines class)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5657516e",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "num_classes = 3\n",
    "n_samples = 150\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Generate random token sequences\n",
    "token_ids_np = np.random.randint(0, vocab_size, (n_samples, seq_len)).astype(np.int64)\n",
    "\n",
    "# Labels: class = (sum of tokens) mod num_classes\n",
    "labels_np = (token_ids_np.sum(axis=1) % num_classes).astype(np.int64)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_onehot_np = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "labels_onehot_np[np.arange(n_samples), labels_np] = 1.0\n",
    "\n",
    "token_ids = nb.Tensor.from_dlpack(token_ids_np)\n",
    "labels = nb.Tensor.from_dlpack(labels_onehot_np)\n",
    "\n",
    "print(f\"Dataset: {n_samples} sequences of length {seq_len}\")\n",
    "print(f\"Vocab size: {vocab_size}, Classes: {num_classes}\")\n",
    "print(f\"Sample tokens: {token_ids_np[0]}\")\n",
    "print(f\"Sample label:  {labels_np[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba96ed",
   "metadata": {},
   "source": [
    "## 4. Build Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "4da62c97",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    max_len=seq_len,\n",
    "    dim_feedforward=64,\n",
    ")\n",
    "model.eval()  # Disable dropout\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {num_layers} encoder layers, d_model={d_model}, heads={num_heads}\")\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "\n",
    "opt_state = nb.nn.optim.adamw_init(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3321119",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc43e649",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, tokens, targets):\n",
    "    logits = model(tokens)\n",
    "    return nb.nn.functional.cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "num_epochs = 60\n",
    "lr = 1e-3\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, token_ids, labels)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=lr\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Compute accuracy\n",
    "        logits = model(token_ids)\n",
    "        pred_classes = nb.argmax(logits, axis=-1)\n",
    "        target_classes = nb.Tensor.from_dlpack(labels_np.astype(np.int64))\n",
    "        correct = nb.equal(pred_classes, target_classes)\n",
    "        accuracy = nb.mean(nb.cast(correct, nb.DType.float32)).item()\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.4f} {accuracy:<10.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caea57f",
   "metadata": {},
   "source": [
    "## 6. Compiled Training (Bonus)\n",
    "\n",
    "For maximum performance, wrap the training step in `@nb.compile`."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2aa6167",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    max_len=seq_len,\n",
    "    dim_feedforward=64,\n",
    ")\n",
    "model2.eval()\n",
    "opt_state2 = nb.nn.optim.adamw_init(model2)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def compiled_step(model, opt_state, tokens, targets):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, tokens, targets)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=1e-3\n",
    "    )\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "print(f\"\\nCompiled training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(30):\n",
    "    model2, opt_state2, loss = compiled_step(model2, opt_state2, token_ids, labels)\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.4f}\")\n",
    "\n",
    "print(\"\\nCompiled step executes forward + backward + optimizer in a single MAX graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9317b01",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | API |\n",
    "|-----------|-----|\n",
    "| Token embedding | `nb.nn.Embedding(vocab_size, d_model)` |\n",
    "| Transformer layer | `nb.nn.TransformerEncoderLayer(d_model, heads, ff_dim)` |\n",
    "| Multi-head attention | `nb.nn.MultiHeadAttention(d_model, heads)` |\n",
    "| Cross-entropy | `nb.nn.functional.cross_entropy_loss(logits, targets)` |\n",
    "| Compiled training | `@nb.compile` on the full train step |\n",
    "\n",
    "**Next:** [05b_transformer_jax](05b_transformer_jax)\n",
    "— The same Transformer, built in a purely functional style."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e1dd45d",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Tutorial 05a completed!\")"
   ]
  }
 ]
}
