{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Tensors and Operations\n",
    "\n",
    "Welcome to Nabla! This tutorial introduces the core building block of\n",
    "the library: the **Tensor**. Nabla tensors are lazy by default — operations\n",
    "build a computation graph that is evaluated only when you request the result\n",
    "(e.g., by printing or calling `.realize()`).\n",
    "\n",
    "Let's start by importing Nabla and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Tensors\n",
    "\n",
    "There are several ways to create tensors in Nabla:\n",
    "\n",
    "1. **From NumPy arrays** via `nb.Tensor.from_dlpack()` (works with any DLPack source)\n",
    "2. **Factory functions** like `nb.zeros()`, `nb.ones()`, `nb.arange()`, `nb.uniform()`\n",
    "3. **Constants** via `nb.constant()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From NumPy:\n",
      "Tensor(\n",
      "  [[1. 2. 3.]\n",
      "   [4. 5. 6.]] : f32[2,3]\n",
      ")\n",
      "  Shape: [Dim(2), Dim(3)], Dtype: DType.float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From NumPy arrays\n",
    "np_array = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32)\n",
    "x = nb.Tensor.from_dlpack(np_array)\n",
    "print(\"From NumPy:\")\n",
    "print(x)\n",
    "print(f\"  Shape: {x.shape}, Dtype: {x.dtype}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros: Tensor(\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]] : f32[2,3]\n",
      ")\n",
      "Ones:  Tensor(\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]] : f32[2,3]\n",
      ")\n",
      "Full:  Tensor(\n",
      "  [[3.14 3.14 3.14]\n",
      "   [3.14 3.14 3.14]] : f32[2,3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Factory functions — zeros, ones, full\n",
    "z = nb.zeros((2, 3))\n",
    "o = nb.ones((2, 3))\n",
    "f = nb.full((2, 3), 3.14)\n",
    "print(\"Zeros:\", z)\n",
    "print(\"Ones: \", o)\n",
    "print(\"Full: \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arange:   Tensor([0. 1. 2. 3. 4. 5.] : f32[6])\n",
      "Uniform:  Tensor(\n",
      "  [[ 0.5962  0.889  -0.9222]\n",
      "   [ 0.1495  0.7381  0.8013]] : f32[2,3]\n",
      ")\n",
      "Gaussian: Tensor(\n",
      "  [[ 1.6811  2.3331 -0.2512]\n",
      "   [ 0.8896  1.6362 -1.9282]] : f32[2,3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ranges and random tensors\n",
    "r = nb.arange(0, 6, dtype=nb.DType.float32)\n",
    "u = nb.uniform((2, 3), low=-1.0, high=1.0)\n",
    "g = nb.gaussian((2, 3), mean=0.0, std=1.0)\n",
    "print(\"Arange:  \", r)\n",
    "print(\"Uniform: \", u)\n",
    "print(\"Gaussian:\", g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Tensor([10. 20. 30.] : f32[3])\n"
     ]
    }
   ],
   "source": [
    "# Constants from python lists (via numpy)\n",
    "c = nb.constant(np.array([10.0, 20.0, 30.0], dtype=np.float32))\n",
    "print(\"Constant:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Properties\n",
    "\n",
    "Every tensor carries metadata about its shape, dtype, and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  [Dim(3), Dim(4), Dim(5)]\n",
      "Dtype:  DType.float32\n",
      "Device: Device(type=cpu,id=0)\n",
      "Rank:   3\n"
     ]
    }
   ],
   "source": [
    "x = nb.uniform((3, 4, 5))\n",
    "print(f\"Shape:  {x.shape}\")\n",
    "print(f\"Dtype:  {x.dtype}\")\n",
    "print(f\"Device: {x.device}\")\n",
    "print(f\"Rank:   {x.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arithmetic Operations\n",
    "\n",
    "Nabla supports standard arithmetic via Python operators and named functions.\n",
    "All operations are lazy — they build a graph that is evaluated on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:     Tensor([1. 2. 3.] : f32[3])\n",
      "b:     Tensor([4. 5. 6.] : f32[3])\n",
      "a + b: Tensor([5. 7. 9.] : f32[3])\n",
      "a - b: Tensor([-3. -3. -3.] : f32[3])\n",
      "a * b: Tensor([ 4. 10. 18.] : f32[3])\n",
      "a / b: Tensor([0.25 0.4  0.5 ] : f32[3])\n",
      "a ** 2: Tensor([1. 4. 9.] : f32[3])\n"
     ]
    }
   ],
   "source": [
    "a = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "b = nb.Tensor.from_dlpack(np.array([4.0, 5.0, 6.0], dtype=np.float32))\n",
    "\n",
    "print(\"a:    \", a)\n",
    "print(\"b:    \", b)\n",
    "print(\"a + b:\", a + b)\n",
    "print(\"a - b:\", a - b)\n",
    "print(\"a * b:\", a * b)\n",
    "print(\"a / b:\", a / b)\n",
    "print(\"a ** 2:\", a ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb.add(a, b): Tensor([5. 7. 9.] : f32[3])\n",
      "nb.mul(a, b): Tensor([ 4. 10. 18.] : f32[3])\n",
      "nb.sub(a, b): Tensor([-3. -3. -3.] : f32[3])\n",
      "nb.div(a, b): Tensor([0.25 0.4  0.5 ] : f32[3])\n"
     ]
    }
   ],
   "source": [
    "# Named function equivalents\n",
    "print(\"nb.add(a, b):\", nb.add(a, b))\n",
    "print(\"nb.mul(a, b):\", nb.mul(a, b))\n",
    "print(\"nb.sub(a, b):\", nb.sub(a, b))\n",
    "print(\"nb.div(a, b):\", nb.div(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Element-wise Unary Operations\n",
    "\n",
    "Nabla provides a rich set of element-wise functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:        Tensor([0.  0.5 1.  1.5 2. ] : f32[5])\n",
      "exp(x):   Tensor([1.     1.6487 2.7183 4.4817 7.3891] : f32[5])\n",
      "log(x+1): Tensor([0.     0.4055 0.6931 0.9163 1.0986] : f32[5])\n",
      "sqrt(x):  Tensor([0.     0.7071 1.     1.2247 1.4142] : f32[5])\n",
      "sin(x):   Tensor([0.     0.4794 0.8415 0.9975 0.9093] : f32[5])\n",
      "cos(x):   Tensor([ 1.      0.8776  0.5403  0.0707 -0.4161] : f32[5])\n",
      "tanh(x):  Tensor([0.     0.4621 0.7616 0.9051 0.964 ] : f32[5])\n"
     ]
    }
   ],
   "source": [
    "x = nb.Tensor.from_dlpack(np.array([0.0, 0.5, 1.0, 1.5, 2.0], dtype=np.float32))\n",
    "print(\"x:       \", x)\n",
    "print(\"exp(x):  \", nb.exp(x))\n",
    "print(\"log(x+1):\", nb.log(x + 1.0))\n",
    "print(\"sqrt(x): \", nb.sqrt(x))\n",
    "print(\"sin(x):  \", nb.sin(x))\n",
    "print(\"cos(x):  \", nb.cos(x))\n",
    "print(\"tanh(x): \", nb.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:        Tensor([-2. -1.  0.  1.  2.] : f32[5])\n",
      "relu(x):  Tensor([0. 0. 0. 1. 2.] : f32[5])\n",
      "sigmoid: Tensor([0.1192 0.2689 0.5    0.7311 0.8808] : f32[5])\n",
      "gelu(x):  Tensor([-0.0455 -0.1587  0.      0.8413  1.9545] : f32[5])\n",
      "silu(x):  Tensor([-0.2384 -0.2689  0.      0.7311  1.7616] : f32[5])\n"
     ]
    }
   ],
   "source": [
    "# Activation functions\n",
    "x = nb.Tensor.from_dlpack(np.array([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=np.float32))\n",
    "print(\"x:       \", x)\n",
    "print(\"relu(x): \", nb.relu(x))\n",
    "print(\"sigmoid:\", nb.sigmoid(x))\n",
    "print(\"gelu(x): \", nb.gelu(x))\n",
    "print(\"silu(x): \", nb.silu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Operations\n",
    "\n",
    "Matrix multiplication is a first-class operation in Nabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: [Dim(3), Dim(4)]\n",
      "B shape: [Dim(4), Dim(5)]\n",
      "A @ B shape: [Dim(3), Dim(5)]\n",
      "A @ B:\n",
      " Tensor(\n",
      "  [[1.3086 1.1829 0.6232 1.087  1.0454]\n",
      "   [2.0064 1.206  1.0642 1.251  1.3388]\n",
      "   [1.5473 0.8017 0.8984 1.0166 1.047 ]] : f32[3,5]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "A = nb.uniform((3, 4))\n",
    "B = nb.uniform((4, 5))\n",
    "C = nb.matmul(A, B)  # or A @ B\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"A @ B shape: {C.shape}\")\n",
    "print(\"A @ B:\\n\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched matmul: [Dim(2), Dim(3), Dim(4)] @ [Dim(2), Dim(4), Dim(5)] = [Dim(2), Dim(3), Dim(5)]\n"
     ]
    }
   ],
   "source": [
    "# Batched matmul\n",
    "batch_A = nb.uniform((2, 3, 4))\n",
    "batch_B = nb.uniform((2, 4, 5))\n",
    "batch_C = batch_A @ batch_B\n",
    "print(f\"Batched matmul: {batch_A.shape} @ {batch_B.shape} = {batch_C.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer product ([Dim(3)] x [Dim(2)]):\n",
      "Tensor(\n",
      "  [[ 4.  5.]\n",
      "   [ 8. 10.]\n",
      "   [12. 15.]] : f32[3,2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Outer product via broadcasting: v1[:, None] * v2[None, :]\n",
    "v1 = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "v2 = nb.Tensor.from_dlpack(np.array([4.0, 5.0], dtype=np.float32))\n",
    "outer = nb.unsqueeze(v1, axis=1) * nb.unsqueeze(v2, axis=0)\n",
    "print(f\"Outer product ({v1.shape} x {v2.shape}):\")\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reduction Operations\n",
    "\n",
    "Reduce along one or more axes (or all axes for a scalar result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " Tensor(\n",
      "  [[1. 2. 3.]\n",
      "   [4. 5. 6.]] : f32[2,3]\n",
      ")\n",
      "\n",
      "sum(x):   Tensor(21. : f32[])\n",
      "mean(x):  Tensor(3.5 : f32[])\n",
      "max(x):   Tensor(6. : f32[])\n",
      "min(x):   Tensor(1. : f32[])\n"
     ]
    }
   ],
   "source": [
    "x = nb.Tensor.from_dlpack(\n",
    "    np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32)\n",
    ")\n",
    "print(\"x:\\n\", x)\n",
    "print()\n",
    "\n",
    "# Full reductions\n",
    "print(\"sum(x):  \", nb.reduce_sum(x))\n",
    "print(\"mean(x): \", nb.mean(x))\n",
    "print(\"max(x):  \", nb.reduce_max(x))\n",
    "print(\"min(x):  \", nb.reduce_min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(axis=0): Tensor([5. 7. 9.] : f32[3])\n",
      "sum(axis=1): Tensor([ 6. 15.] : f32[2])\n",
      "mean(axis=1): Tensor([2. 5.] : f32[2])\n",
      "max(axis=0):  Tensor([4. 5. 6.] : f32[3])\n"
     ]
    }
   ],
   "source": [
    "# Axis-specific reductions\n",
    "print(\"sum(axis=0):\", nb.reduce_sum(x, axis=0))  # Sum columns\n",
    "print(\"sum(axis=1):\", nb.reduce_sum(x, axis=1))  # Sum rows\n",
    "print(\"mean(axis=1):\", nb.mean(x, axis=1))\n",
    "print(\"max(axis=0): \", nb.reduce_max(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(axis=1, keepdims=True): Tensor(\n",
      "  [[ 6.]\n",
      "   [15.]] : f32[2,1]\n",
      ")\n",
      "  Shape: [Dim(2), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "# keepdims preserves the reduced dimension\n",
    "print(\"sum(axis=1, keepdims=True):\", nb.reduce_sum(x, axis=1, keepdims=True))\n",
    "print(f\"  Shape: {nb.reduce_sum(x, axis=1, keepdims=True).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax(axis=1): Tensor([2 2] : i64[2])\n",
      "argmin(axis=0): Tensor([0 0 0] : i64[3])\n"
     ]
    }
   ],
   "source": [
    "# Argmax / Argmin\n",
    "print(\"argmax(axis=1):\", nb.argmax(x, axis=1))\n",
    "print(\"argmin(axis=0):\", nb.argmin(x, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Shape Manipulation\n",
    "\n",
    "Nabla supports reshaping, transposing, squeezing, and more — all as lazy ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: shape=[Dim(12)]\n",
      "Tensor([ 0.  1.  2. ...  9. 10. 11.] : f32[12])\n",
      "\n",
      "Reshaped to (3, 4):\n",
      "Tensor(\n",
      "  [[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]] : f32[3,4]\n",
      ")\n",
      "\n",
      "Flattened back: shape=[Dim(12)]\n"
     ]
    }
   ],
   "source": [
    "x = nb.arange(0, 12, dtype=nb.DType.float32)\n",
    "print(f\"Original: shape={x.shape}\")\n",
    "print(x)\n",
    "\n",
    "# Reshape\n",
    "r = nb.reshape(x, (3, 4))\n",
    "print(f\"\\nReshaped to (3, 4):\")\n",
    "print(r)\n",
    "\n",
    "# Flatten\n",
    "f = nb.flatten(r)\n",
    "print(f\"\\nFlattened back: shape={f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:   [Dim(2), Dim(3), Dim(4)]\n",
      "Swap axes (1,2):  [Dim(2), Dim(4), Dim(3)]\n",
      "Permute (2,0,1):  [Dim(4), Dim(2), Dim(3)]\n",
      "Move axis 2→0:    [Dim(4), Dim(2), Dim(3)]\n"
     ]
    }
   ],
   "source": [
    "# Transpose and permute\n",
    "m = nb.uniform((2, 3, 4))\n",
    "print(f\"Original shape:   {m.shape}\")\n",
    "print(f\"Swap axes (1,2):  {nb.swap_axes(m, 1, 2).shape}\")\n",
    "print(f\"Permute (2,0,1):  {nb.permute(m, (2, 0, 1)).shape}\")\n",
    "print(f\"Move axis 2→0:    {nb.moveaxis(m, 2, 0).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       [Dim(1), Dim(3), Dim(1), Dim(4)]\n",
      "Squeeze(0):     [Dim(3), Dim(1), Dim(4)]\n",
      "Squeeze(2):     [Dim(1), Dim(3), Dim(4)]\n",
      "Unsqueeze(0):   [Dim(1), Dim(3), Dim(4)]\n",
      "Unsqueeze(1):   [Dim(3), Dim(1), Dim(4)]\n"
     ]
    }
   ],
   "source": [
    "# Squeeze and unsqueeze\n",
    "x = nb.ones((1, 3, 1, 4))\n",
    "print(f\"Original:       {x.shape}\")\n",
    "print(f\"Squeeze(0):     {nb.squeeze(x, axis=0).shape}\")\n",
    "print(f\"Squeeze(2):     {nb.squeeze(x, axis=2).shape}\")\n",
    "\n",
    "y = nb.ones((3, 4))\n",
    "print(f\"Unsqueeze(0):   {nb.unsqueeze(y, axis=0).shape}\")\n",
    "print(f\"Unsqueeze(1):   {nb.unsqueeze(y, axis=1).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Concatenation and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenate (axis=0): [Dim(4), Dim(3)]\n",
      "Concatenate (axis=1): [Dim(2), Dim(6)]\n",
      "Stack (axis=0):       [Dim(2), Dim(2), Dim(3)]\n",
      "Stack (axis=1):       [Dim(2), Dim(2), Dim(3)]\n"
     ]
    }
   ],
   "source": [
    "a = nb.ones((2, 3))\n",
    "b = nb.zeros((2, 3))\n",
    "print(\"Concatenate (axis=0):\", nb.concatenate([a, b], axis=0).shape)\n",
    "print(\"Concatenate (axis=1):\", nb.concatenate([a, b], axis=1).shape)\n",
    "print(\"Stack (axis=0):      \", nb.stack([a, b], axis=0).shape)\n",
    "print(\"Stack (axis=1):      \", nb.stack([a, b], axis=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Broadcasting\n",
    "\n",
    "Nabla follows NumPy broadcasting rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [Dim(3), Dim(1)] + y: [Dim(1), Dim(4)] = z: [Dim(3), Dim(4)]\n",
      "Tensor(\n",
      "  [[1.5786 1.5875 0.9695 1.2206]\n",
      "   [1.725  1.7339 1.1159 1.367 ]\n",
      "   [0.8194 0.8283 0.2104 0.4614]] : f32[3,4]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = nb.uniform((3, 1))\n",
    "y = nb.uniform((1, 4))\n",
    "z = x + y  # Broadcasts to (3, 4)\n",
    "print(f\"x: {x.shape} + y: {y.shape} = z: {z.shape}\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast [Dim(3)] → [Dim(4), Dim(3)]:\n",
      "Tensor(\n",
      "  [[1. 2. 3.]\n",
      "   [1. 2. 3.]\n",
      "   [1. 2. 3.]\n",
      "   [1. 2. 3.]] : f32[4,3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Explicit broadcast\n",
    "v = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "b = nb.broadcast_to(v, (4, 3))\n",
    "print(f\"Broadcast {v.shape} → {b.shape}:\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dtype: DType.float32\n",
      "Cast to int32:  DType.int32\n",
      "Cast to float64: DType.float64\n"
     ]
    }
   ],
   "source": [
    "x = nb.ones((3,), dtype=nb.DType.float32)\n",
    "print(f\"Original dtype: {x.dtype}\")\n",
    "\n",
    "x_int = nb.cast(x, nb.DType.int32)\n",
    "print(f\"Cast to int32:  {x_int.dtype}\")\n",
    "\n",
    "x_f64 = nb.cast(x, nb.DType.float64)\n",
    "print(f\"Cast to float64: {x_f64.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparisons and Logical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: Tensor([1. 2. 3. 4.] : f32[4])\n",
      "b: Tensor([2. 2. 4. 3.] : f32[4])\n",
      "a == b: Tensor([False  True False False] : bool[4])\n",
      "a > b:  Tensor([False False False  True] : bool[4])\n",
      "a < b:  Tensor([ True False  True False] : bool[4])\n",
      "a >= b: Tensor([False  True False  True] : bool[4])\n"
     ]
    }
   ],
   "source": [
    "a = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32))\n",
    "b = nb.Tensor.from_dlpack(np.array([2.0, 2.0, 4.0, 3.0], dtype=np.float32))\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a == b:\", nb.equal(a, b))\n",
    "print(\"a > b: \", nb.greater(a, b))\n",
    "print(\"a < b: \", nb.less(a, b))\n",
    "print(\"a >= b:\", nb.greater_equal(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where(a > b, a, b): Tensor([2. 2. 4. 4.] : f32[4])\n"
     ]
    }
   ],
   "source": [
    "# Where (conditional select)\n",
    "mask = nb.greater(a, b)\n",
    "result = nb.where(mask, a, b)  # Pick a where a > b, else b\n",
    "print(\"where(a > b, a, b):\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " Tensor(\n",
      "  [[2.  1.  0.1]\n",
      "   [0.5 2.  0.3]] : f32[2,3]\n",
      ")\n",
      "Softmax:\n",
      " Tensor(\n",
      "  [[0.659  0.2424 0.0986]\n",
      "   [0.1587 0.7113 0.1299]] : f32[2,3]\n",
      ")\n",
      "Row sums: Tensor([1. 1.] : f32[2])\n"
     ]
    }
   ],
   "source": [
    "logits = nb.Tensor.from_dlpack(\n",
    "    np.array([[2.0, 1.0, 0.1], [0.5, 2.0, 0.3]], dtype=np.float32)\n",
    ")\n",
    "probs = nb.softmax(logits, axis=-1)\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"Softmax:\\n\", probs)\n",
    "print(\"Row sums:\", nb.reduce_sum(probs, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial you learned how to:\n",
    "- Create tensors from NumPy arrays, factory functions, and constants\n",
    "- Perform arithmetic, element-wise, and matrix operations\n",
    "- Reduce tensors along axes (sum, mean, max, min, argmax)\n",
    "- Manipulate shapes (reshape, transpose, squeeze, unsqueeze)\n",
    "- Use broadcasting, type casting, comparisons, and softmax\n",
    "\n",
    "All operations are **lazy** — they build a computation graph that's evaluated\n",
    "on demand. This enables powerful optimizations when combined with `@nb.compile`.\n",
    "\n",
    "**Next:** [02_autodiff.py](02_autodiff.py) — Automatic differentiation with Nabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Tutorial 01 completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Tutorial 01 completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
