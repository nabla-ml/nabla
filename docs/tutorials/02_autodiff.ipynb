{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710cd8e",
   "metadata": {},
   "source": [
    "# Tutorial 2: Automatic Differentiation\n",
    "\n",
    "Nabla provides a **JAX-like functional autodiff** system built on composable\n",
    "transforms. Every transform is a higher-order function: it takes a function\n",
    "and returns a new function that computes derivatives.\n",
    "\n",
    "| Transform | Mode | Computes |\n",
    "|-----------|------|----------|\n",
    "| `grad` | Reverse | Gradient of scalar-valued function |\n",
    "| `value_and_grad` | Reverse | (value, gradient) pair |\n",
    "| `jvp` | Forward | Jacobian-vector product |\n",
    "| `vjp` | Reverse | Vector-Jacobian product |\n",
    "| `jacrev` | Reverse | Full Jacobian matrix |\n",
    "| `jacfwd` | Forward | Full Jacobian matrix |"
   ]
  },
  {
   "cell_type": "code",
   "id": "8aefc43b",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla autodiff tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36459",
   "metadata": {},
   "source": [
    "## 1. `grad` — Gradient of a Scalar Function\n",
    "\n",
    "`nb.grad(fn)` returns a function that computes the gradient of `fn`\n",
    "with respect to specified arguments (default: first argument)."
   ]
  },
  {
   "cell_type": "code",
   "id": "6a9e009d",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"f(x) = x^3 + 2x^2 - 5x + 3, so f'(x) = 3x^2 + 4x - 5.\"\"\"\n",
    "    return x ** 3 + 2.0 * x ** 2 - 5.0 * x + 3.0\n",
    "\n",
    "df = nb.grad(f)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([2.0], dtype=np.float32))\n",
    "grad_val = df(x)\n",
    "print(f\"f(x) = x^3 + 2x^2 - 5x + 3\")\n",
    "print(f\"f'(x) = 3x^2 + 4x - 5\")\n",
    "print(f\"f'(2.0) = 3*4 + 4*2 - 5 = {3*4 + 4*2 - 5}\")\n",
    "print(f\"Nabla grad: {grad_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafb239",
   "metadata": {},
   "source": [
    "## 2. `value_and_grad` — Value and Gradient Together\n",
    "\n",
    "Often you need both the function value and its gradient. This is more\n",
    "efficient than calling `f` and `grad(f)` separately."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac164b9b",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x):\n",
    "    \"\"\"f(x) = sum(x^2), so grad = 2x.\"\"\"\n",
    "    return nb.reduce_sum(x * x)\n",
    "\n",
    "val_and_grad_fn = nb.value_and_grad(quadratic)\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "value, gradient = val_and_grad_fn(x)\n",
    "print(f\"x = [1, 2, 3]\")\n",
    "print(f\"f(x) = sum(x^2) = {value}\")\n",
    "print(f\"grad(f) = 2x = {gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785e3e3",
   "metadata": {},
   "source": [
    "### Multiple Arguments with `argnums`\n",
    "\n",
    "Use `argnums` to specify which arguments to differentiate with respect to."
   ]
  },
  {
   "cell_type": "code",
   "id": "9010c8c3",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(w, x):\n",
    "    \"\"\"f(w, x) = sum(w * x).\"\"\"\n",
    "    return nb.reduce_sum(w * x)\n",
    "\n",
    "# Gradient w.r.t. first arg (w) only — default\n",
    "grad_w = nb.grad(weighted_sum, argnums=0)\n",
    "w = nb.Tensor.from_dlpack(np.array([1.0, 2.0], dtype=np.float32))\n",
    "x = nb.Tensor.from_dlpack(np.array([3.0, 4.0], dtype=np.float32))\n",
    "print(f\"grad w.r.t. w: {grad_w(w, x)}\")\n",
    "print(f\"  (should be x = [3, 4])\")\n",
    "\n",
    "# Gradient w.r.t. second arg (x)\n",
    "grad_x = nb.grad(weighted_sum, argnums=1)\n",
    "print(f\"grad w.r.t. x: {grad_x(w, x)}\")\n",
    "print(f\"  (should be w = [1, 2])\")\n",
    "\n",
    "# Gradient w.r.t. both — returns a tuple\n",
    "grad_both = nb.grad(weighted_sum, argnums=(0, 1))\n",
    "gw, gx = grad_both(w, x)\n",
    "print(f\"grad w.r.t. (w, x): ({gw}, {gx})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb7e42",
   "metadata": {},
   "source": [
    "## 3. `jvp` — Forward-Mode (Jacobian-Vector Product)\n",
    "\n",
    "`nb.jvp(fn, primals, tangents)` computes:\n",
    "- The function output `fn(*primals)`\n",
    "- The directional derivative `J @ tangents` (JVP)\n",
    "\n",
    "This is efficient when the number of **inputs** is small (one forward pass\n",
    "per tangent direction)."
   ]
  },
  {
   "cell_type": "code",
   "id": "13e78bf4",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    \"\"\"g(x) = [x0^2 + x1, x0 * x1].\"\"\"\n",
    "    r0 = nb.reshape(x[0] ** 2 + x[1], (1,))\n",
    "    r1 = nb.reshape(x[0] * x[1], (1,))\n",
    "    return nb.concatenate([r0, r1], axis=0)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([3.0, 2.0], dtype=np.float32))\n",
    "v = nb.Tensor.from_dlpack(np.array([1.0, 0.0], dtype=np.float32))\n",
    "\n",
    "output, jvp_val = nb.jvp(g, (x,), (v,))\n",
    "print(f\"g([3, 2]) = [3^2 + 2, 3*2] = {output}\")\n",
    "print(f\"JVP with v=[1,0] (column 1 of Jacobian):\")\n",
    "print(f\"  J @ v = {jvp_val}\")\n",
    "print(f\"  Expected: [2*3, 2] = [6, 2]\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "22890d7b",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second column of the Jacobian\n",
    "v2 = nb.Tensor.from_dlpack(np.array([0.0, 1.0], dtype=np.float32))\n",
    "_, jvp_val2 = nb.jvp(g, (x,), (v2,))\n",
    "print(f\"JVP with v=[0,1] (column 2 of Jacobian):\")\n",
    "print(f\"  J @ v = {jvp_val2}\")\n",
    "print(f\"  Expected: [1, 3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d9a76",
   "metadata": {},
   "source": [
    "## 4. `vjp` — Reverse-Mode (Vector-Jacobian Product)\n",
    "\n",
    "`nb.vjp(fn, *primals)` returns `(output, vjp_fn)` where `vjp_fn(cotangent)`\n",
    "gives the VJP = `cotangent @ J`.\n",
    "\n",
    "This is efficient when the number of **outputs** is small (one backward pass\n",
    "per cotangent direction)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b37b6e91",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(x):\n",
    "    \"\"\"f(x) = Ax where A = [[1, 2], [3, 4], [5, 6]].\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=np.float32)\n",
    "    )\n",
    "    return x @ A  # (3,) @ (3,2) isn't quite right — let's use matmul properly\n",
    "\n",
    "# For vjp demo, use a scalar-to-vector function via matrix multiply\n",
    "def mat_fn(x):\n",
    "    \"\"\"f(x) = Ax, where A is 2x3 and x is (3,). Output is (2,).\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[1.0, 0.0, 2.0], [0.0, 3.0, 1.0]], dtype=np.float32)\n",
    "    )\n",
    "    return A @ x  # (2,3) @ (3,) = (2,)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "output, vjp_fn = nb.vjp(mat_fn, x)\n",
    "print(f\"f(x) = Ax, A = [[1,0,2],[0,3,1]], x = [1,2,3]\")\n",
    "print(f\"f(x) = {output}\")\n",
    "print(f\"  Expected: [1+0+6, 0+6+3] = [7, 9]\")\n",
    "\n",
    "# VJP with cotangent [1, 0] — gives first row of A^T\n",
    "v1 = nb.Tensor.from_dlpack(np.array([1.0, 0.0], dtype=np.float32))\n",
    "(vjp1,) = vjp_fn(v1)\n",
    "print(f\"\\nVJP with v=[1,0]: {vjp1}\")\n",
    "print(f\"  Expected: A^T @ [1,0] = [1, 0, 2]\")\n",
    "\n",
    "# VJP with cotangent [0, 1] — gives second row of A^T\n",
    "v2 = nb.Tensor.from_dlpack(np.array([0.0, 1.0], dtype=np.float32))\n",
    "(vjp2,) = vjp_fn(v2)\n",
    "print(f\"VJP with v=[0,1]: {vjp2}\")\n",
    "print(f\"  Expected: A^T @ [0,1] = [0, 3, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2cd59f",
   "metadata": {},
   "source": [
    "## 5. `jacrev` — Full Jacobian via Reverse Mode\n",
    "\n",
    "`nb.jacrev(fn)` computes the full Jacobian matrix using reverse-mode\n",
    "autodiff (one backward pass per output element, batched via vmap)."
   ]
  },
  {
   "cell_type": "code",
   "id": "39887d93",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    \"\"\"h(x) = Ax + sin(x), nonlinear vector function R^2 -> R^2.\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[2.0, -1.0], [1.0, 3.0]], dtype=np.float32)\n",
    "    )\n",
    "    return A @ x + nb.sin(x)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 0.5], dtype=np.float32))\n",
    "J = nb.jacrev(h)(x)\n",
    "print(\"Jacobian via jacrev:\")\n",
    "print(J)\n",
    "print(\"Expected: A + diag(cos(x))\")\n",
    "print(f\"  [[2+cos(1), -1     ],\")\n",
    "print(f\"   [1,        3+cos(0.5)]]\")\n",
    "print(f\"  ≈ [[{2+np.cos(1):.4f}, {-1:.4f}],\")\n",
    "print(f\"     [{1:.4f}, {3+np.cos(0.5):.4f}]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d3e00",
   "metadata": {},
   "source": [
    "## 6. `jacfwd` — Full Jacobian via Forward Mode\n",
    "\n",
    "`nb.jacfwd(fn)` computes the same Jacobian using forward-mode autodiff\n",
    "(one JVP per input element, batched via vmap). Prefer `jacfwd` when\n",
    "inputs are few and outputs are many."
   ]
  },
  {
   "cell_type": "code",
   "id": "9dc2060b",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_fwd = nb.jacfwd(h)(x)\n",
    "print(\"Jacobian via jacfwd:\")\n",
    "print(J_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64115964",
   "metadata": {},
   "source": [
    "### When to use `jacrev` vs `jacfwd`\n",
    "\n",
    "| Scenario | Prefer |\n",
    "|----------|--------|\n",
    "| Few outputs, many inputs | `jacrev` |\n",
    "| Few inputs, many outputs | `jacfwd` |\n",
    "| Square Jacobian | Either works |\n",
    "| Hessian (second derivative) | Compose both! |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b05c20",
   "metadata": {},
   "source": [
    "## 7. Hessians — Composing Transforms\n",
    "\n",
    "Because Nabla's transforms are **composable**, you can compute Hessians\n",
    "(second-order derivatives) by nesting Jacobian transforms.\n",
    "\n",
    "For a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the Hessian\n",
    "$H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$ can be computed\n",
    "in multiple ways:"
   ]
  },
  {
   "cell_type": "code",
   "id": "33daa9b1",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_fn(x):\n",
    "    \"\"\"f(x) = x0^2 * x1 + x1^3, a polynomial with known Hessian.\"\"\"\n",
    "    return x[0] ** 2 * x[1] + x[1] ** 3\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([2.0, 3.0], dtype=np.float32))\n",
    "print(f\"f(x) = x0^2 * x1 + x1^3\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = {scalar_fn(x)}\")\n",
    "print()\n",
    "\n",
    "# The Hessian of f:\n",
    "# df/dx0 = 2*x0*x1,  df/dx1 = x0^2 + 3*x1^2\n",
    "# d^2f/dx0dx0 = 2*x1,    d^2f/dx0dx1 = 2*x0\n",
    "# d^2f/dx1dx0 = 2*x0,    d^2f/dx1dx1 = 6*x1\n",
    "# At x = [2, 3]:\n",
    "# H = [[6, 4], [4, 18]]\n",
    "print(\"Analytical Hessian at x=[2,3]:\")\n",
    "print(\"  [[2*x1, 2*x0], [2*x0, 6*x1]] = [[6, 4], [4, 18]]\")\n",
    "print()\n",
    "\n",
    "# Method 1: jacfwd(grad(f))\n",
    "H1 = nb.jacfwd(nb.grad(scalar_fn))(x)\n",
    "print(\"Method 1 — jacfwd(grad(f)):\")\n",
    "print(H1)\n",
    "\n",
    "# Method 2: jacrev(grad(f))\n",
    "H2 = nb.jacrev(nb.grad(scalar_fn))(x)\n",
    "print(\"Method 2 — jacrev(grad(f)):\")\n",
    "print(H2)\n",
    "\n",
    "# Method 3: jacrev(jacfwd(f))\n",
    "H3 = nb.jacrev(nb.jacfwd(scalar_fn))(x)\n",
    "print(\"Method 3 — jacrev(jacfwd(f)):\")\n",
    "print(H3)\n",
    "\n",
    "# Method 4: jacfwd(jacrev(f))\n",
    "H4 = nb.jacfwd(nb.jacrev(scalar_fn))(x)\n",
    "print(\"Method 4 — jacfwd(jacrev(f)):\")\n",
    "print(H4)\n",
    "\n",
    "print(\"\\nAll four methods produce the same Hessian! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2bbc6",
   "metadata": {},
   "source": [
    "## 8. Gradient of a Multi-Variable Loss\n",
    "\n",
    "A more practical example: computing gradients for a simple regression loss."
   ]
  },
  {
   "cell_type": "code",
   "id": "7750bd88",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_loss(w, b, X, y):\n",
    "    \"\"\"MSE loss for linear regression: ||Xw + b - y||^2 / n.\"\"\"\n",
    "    predictions = X @ w + b\n",
    "    residuals = predictions - y\n",
    "    return nb.mean(residuals * residuals)\n",
    "\n",
    "# Create data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 3\n",
    "X = nb.Tensor.from_dlpack(np.random.randn(n_samples, n_features).astype(np.float32))\n",
    "w_true = nb.Tensor.from_dlpack(np.array([[2.0], [-1.0], [0.5]], dtype=np.float32))\n",
    "y = X @ w_true + 0.1 * nb.gaussian((n_samples, 1))\n",
    "\n",
    "# Initialize weights\n",
    "w = nb.zeros((n_features, 1))\n",
    "b = nb.zeros((1,))\n",
    "\n",
    "# Compute gradients\n",
    "grad_fn = nb.grad(linear_regression_loss, argnums=(0, 1))\n",
    "dw, db = grad_fn(w, b, X, y)\n",
    "print(f\"Gradient w.r.t. weights (shape {dw.shape}):\")\n",
    "print(dw)\n",
    "print(f\"\\nGradient w.r.t. bias (shape {db.shape}):\")\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36af6b",
   "metadata": {},
   "source": [
    "## 9. A Simple Gradient Descent\n",
    "\n",
    "Using `value_and_grad` in a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "id": "066ac2ad",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nb.zeros((n_features, 1))\n",
    "b = nb.zeros((1,))\n",
    "lr = 0.1\n",
    "\n",
    "vg_fn = nb.value_and_grad(linear_regression_loss, argnums=(0, 1))\n",
    "\n",
    "print(f\"{'Step':<6} {'Loss':<12}\")\n",
    "print(\"-\" * 20)\n",
    "for step in range(10):\n",
    "    loss, (dw, db) = vg_fn(w, b, X, y)\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    if (step + 1) % 2 == 0:\n",
    "        print(f\"{step + 1:<6} {loss.item():<12.6f}\")\n",
    "\n",
    "print(f\"\\nLearned weights: {w}\")\n",
    "print(f\"True weights:    {w_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b864522",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Transform | Input | Output | Best for |\n",
    "|-----------|-------|--------|----------|\n",
    "| `grad(f)` | Scalar fn | Gradient vector | Training losses |\n",
    "| `value_and_grad(f)` | Scalar fn | (value, gradient) | Training loops |\n",
    "| `jvp(f, primals, tangents)` | Any fn | (output, J·v) | Few inputs |\n",
    "| `vjp(f, *primals)` | Any fn | (output, vjp_fn) | Few outputs |\n",
    "| `jacrev(f)` | Any fn | Full Jacobian | Few outputs |\n",
    "| `jacfwd(f)` | Any fn | Full Jacobian | Few inputs |\n",
    "| Compose! | — | Hessians, etc. | Higher-order derivatives |\n",
    "\n",
    "**Next:** [03a_mlp_training_pytorch](03a_mlp_training_pytorch)\n",
    "— Training an MLP with Nabla's PyTorch-style API."
   ]
  },
  {
   "cell_type": "code",
   "id": "8f479aa5",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Tutorial 02 completed!\")"
   ]
  }
 ]
}
