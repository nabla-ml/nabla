{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb3b084",
   "metadata": {},
   "source": [
    "# Example 8: Pipeline Parallel Inference\n",
    "\n",
    "This example focuses on inference-only execution:\n",
    "- 4-stage GPipe-style forward pipeline\n",
    "- staged communication via `ppermute`\n",
    "- output parity checks against sequential NumPy"
   ]
  },
  {
   "cell_type": "code",
   "id": "8af8e8ec",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla import ops\n",
    "from nabla.core.sharding import DeviceMesh, DimSpec\n",
    "from nabla.core.sharding import PartitionSpec as P\n",
    "from nabla.ops import communication\n",
    "from nabla.transforms import vmap\n",
    "\n",
    "STAGES = 4\n",
    "MICRO_BATCHES = 8\n",
    "MICRO_BATCH_SIZE = 4\n",
    "DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403233a6",
   "metadata": {},
   "source": [
    "## 1. Define Inference Pipeline Helpers\n",
    "\n",
    "These helpers run staged forward-only pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca6ce1b6",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_compute(x, w):\n",
    "    return ops.relu(ops.matmul(x, w))\n",
    "\n",
    "\n",
    "def pipeline_step(current_state, fresh_input, weight_stack, mask_0, step_fn, perm):\n",
    "    \"\"\"Single GPipe step: compute -> shift -> extract -> inject.\"\"\"\n",
    "    computed = step_fn(current_state, weight_stack)\n",
    "    shifted = communication.ppermute(computed, perm)\n",
    "    res_part = ops.where(mask_0, shifted, ops.zeros_like(shifted))\n",
    "    result = ops.reduce_sum(res_part, axis=0)\n",
    "    next_state = ops.where(mask_0, fresh_input, shifted)\n",
    "    return next_state, result\n",
    "\n",
    "\n",
    "def pipeline_inference_loop(\n",
    "    padded_inputs, weight_stack, current_state, mask_0, step_fn, perm, total_steps\n",
    "):\n",
    "    results = []\n",
    "    for t in range(total_steps):\n",
    "        start_idx = (t, 0, 0)\n",
    "        slice_size = (1, MICRO_BATCH_SIZE, DIM)\n",
    "        fraction = ops.slice_tensor(padded_inputs, start=start_idx, size=slice_size)\n",
    "        fresh = ops.squeeze(fraction, axis=0)\n",
    "\n",
    "        current_state, res = pipeline_step(\n",
    "            current_state, fresh, weight_stack, mask_0, step_fn, perm\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return ops.stack(results, axis=0), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7702aff",
   "metadata": {},
   "source": [
    "## 2. Run Inference Parity Check\n",
    "\n",
    "Trace the pipeline graph and compare outputs to a sequential NumPy baseline."
   ]
  },
  {
   "cell_type": "code",
   "id": "d02279f1",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pp_inference_clean():\n",
    "    mesh = DeviceMesh(\"pp\", (STAGES,), (\"stage\",))\n",
    "    print(f\"Running GPipe Inference Test on Mesh: {mesh}\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    w_np = np.random.randn(STAGES, DIM, DIM).astype(np.float32)\n",
    "    x_np = np.random.randn(MICRO_BATCHES, MICRO_BATCH_SIZE, DIM).astype(np.float32)\n",
    "\n",
    "    w_spec = [DimSpec.from_raw(d) for d in P(\"stage\", None, None)]\n",
    "    w_sharded = ops.shard(nb.Tensor.from_dlpack(w_np), mesh, w_spec).realize()\n",
    "\n",
    "    padding = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "    x_padded_np = np.concatenate([x_np, padding], axis=0)\n",
    "    x_padded_nb = nb.Tensor.from_dlpack(x_padded_np)\n",
    "\n",
    "    state_np = np.zeros((STAGES, MICRO_BATCH_SIZE, DIM), dtype=np.float32)\n",
    "    state_sharded = ops.shard(nb.Tensor.from_dlpack(state_np), mesh, w_spec).realize()\n",
    "\n",
    "    mask_np = np.eye(STAGES, 1).reshape(STAGES, 1, 1).astype(bool)\n",
    "    mask_0_sharded = ops.shard(nb.Tensor.from_dlpack(mask_np), mesh, w_spec).realize()\n",
    "\n",
    "    idx = mesh.axis_names.index(\"stage\")\n",
    "    size = mesh.shape[idx]\n",
    "    perm = [(i, (i + 1) % size) for i in range(size)]\n",
    "\n",
    "    step_fn = vmap(\n",
    "        stage_compute, in_axes=(0, 0), out_axes=0, spmd_axis_name=\"stage\", mesh=mesh\n",
    "    )\n",
    "\n",
    "    def trace_wrapper(inputs, weights, state, mask):\n",
    "        total_steps = MICRO_BATCHES + STAGES\n",
    "        return pipeline_inference_loop(\n",
    "            inputs, weights, state, mask, step_fn, perm, total_steps\n",
    "        )\n",
    "\n",
    "    traced = nb.core.graph.tracing.trace(\n",
    "        trace_wrapper, x_padded_nb, w_sharded, state_sharded, mask_0_sharded\n",
    "    )\n",
    "\n",
    "    results_np = nb.core.tree_map(lambda x: x.to_numpy(), traced.outputs)\n",
    "    preds_all = results_np[0]\n",
    "    vals = preds_all[STAGES : STAGES + MICRO_BATCHES]\n",
    "\n",
    "    print(\"Running Reference...\")\n",
    "    outs = []\n",
    "    for i in range(MICRO_BATCHES):\n",
    "        act = x_np[i]\n",
    "        for s in range(STAGES):\n",
    "            act = np.maximum(act @ w_np[s], 0)\n",
    "        outs.append(act)\n",
    "    ref = np.stack(outs)\n",
    "\n",
    "    diff = np.max(np.abs(vals - ref))\n",
    "    print(f\"Max Diff: {diff:.6f}\")\n",
    "\n",
    "    if diff < 1e-4:\n",
    "        print(\"✅ SUCCESS\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pp_inference_clean()"
   ]
  }
 ]
}
