{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX vs. Nabla: Training a Transformer (CPU)\n",
    "\n",
    "This notebook provides a detailed, from-scratch implementation of a Transformer model for a sequence-reversal task. The goal is to compare the APIs and programming models of two deep learning frameworks: **JAX** and **Nabla**.\n",
    "\n",
    "Each core component of the Transformer is implemented for both frameworks in the same code cell, allowing for a direct comparison of their syntax and approach. The implementations are kept as architecturally similar as possible to highlight the differences in the libraries themselves.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental building blocks of the Transformer architecture.\n",
    "- Compare the functional, JIT-centric approaches of JAX and Nabla.\n",
    "- See how a complete sequence-to-sequence model is built and trained in both frameworks.\n",
    "\n",
    "At the end of the notebook, you will find two separate code cells that run the complete training loops, one for JAX and one for Nabla, followed by visualization of the training loss curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "First, we import the necessary libraries and define the configuration parameters for our model and training task. These parameters are identical for both the JAX and Nabla implementations to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def install_packages():\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"modular\",\n",
    "            \"--extra-index-url\",\n",
    "            \"https://download.pytorch.org/whl/cpu\",\n",
    "            \"--index-url\",\n",
    "            \"https://dl.modular.com/public/nightly/python/simple/\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"nabla-ml\",\n",
    "            \"jax\",\n",
    "            \"jaxlib\",\n",
    "            \"matplotlib\",\n",
    "            \"numpy\",\n",
    "            \"--upgrade\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    import time\n",
    "    from typing import Any, List\n",
    "\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    import nabla as nb\n",
    "\n",
    "    print(\"âœ… All packages imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(\"ðŸ”„ Installing required packages...\")\n",
    "    install_packages()\n",
    "\n",
    "    # Retry imports after installation\n",
    "    import time\n",
    "    from typing import Any\n",
    "\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    import nabla as nb\n",
    "\n",
    "    print(\"âœ… All packages installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION (Shared for both JAX and Nabla)\n",
    "# ============================================================================\n",
    "\n",
    "# Task Configuration\n",
    "VOCAB_SIZE = 20  # Total vocabulary size (0=PAD, 1=START, 2=END, 3-19=content)\n",
    "SOURCE_SEQ_LEN = 9  # Length of input sequences to reverse\n",
    "TARGET_SEQ_LEN = SOURCE_SEQ_LEN + 2  # +1 for END token, +1 for START token in decoder\n",
    "MAX_SEQ_LEN = TARGET_SEQ_LEN\n",
    "\n",
    "# Model Architecture\n",
    "NUM_LAYERS = 2  # Number of encoder and decoder layers\n",
    "D_MODEL = 64  # Model dimension (embedding size)\n",
    "NUM_HEADS = 4  # Number of attention heads (must divide D_MODEL)\n",
    "D_FF = 128  # Feed-forward network hidden dimension\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 64  # Number of sequences per training batch\n",
    "LEARNING_RATE = 0.0005  # AdamW learning rate\n",
    "NUM_EPOCHS = 500  # Total training epochs\n",
    "PRINT_INTERVAL = 10  # Print progress every N epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding\n",
    "\n",
    "Since Transformers do not have inherent knowledge of sequence order (unlike RNNs), we inject positional information using sinusoidal positional encodings. These are fixed (non-learned) vectors added to the input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_jax(max_seq_len: int, d_model: int) -> jnp.ndarray:\n",
    "    \"\"\"Create sinusoidal positional encodings for JAX.\"\"\"\n",
    "    position = jnp.arange(max_seq_len).reshape((max_seq_len, 1))\n",
    "    half_d_model = d_model // 2\n",
    "    dim_indices = jnp.arange(half_d_model).reshape((1, half_d_model))\n",
    "    scaling_factors = 10000.0 ** (2.0 * dim_indices / d_model)\n",
    "    angles = position / scaling_factors\n",
    "    sin_vals = jnp.sin(angles)\n",
    "    cos_vals = jnp.cos(angles)\n",
    "    stacked = jnp.stack([sin_vals, cos_vals], axis=2)\n",
    "    pe = stacked.reshape((max_seq_len, d_model))\n",
    "    return pe.reshape((1, max_seq_len, d_model))\n",
    "\n",
    "\n",
    "def positional_encoding_nabla(max_seq_len: int, d_model: int) -> nb.Array:\n",
    "    \"\"\"Create sinusoidal positional encodings for Nabla.\"\"\"\n",
    "    position = nb.ndarange((max_seq_len,)).reshape((max_seq_len, 1))\n",
    "    half_d_model = d_model // 2\n",
    "    dim_indices = nb.ndarange((half_d_model,)).reshape((1, half_d_model))\n",
    "    scaling_factors = 10000.0 ** (2.0 * dim_indices / d_model)\n",
    "    angles = position / scaling_factors\n",
    "    sin_vals = nb.sin(angles)\n",
    "    cos_vals = nb.cos(angles)\n",
    "    stacked = nb.stack([sin_vals, cos_vals], axis=2)\n",
    "    pe = stacked.reshape((max_seq_len, d_model))\n",
    "    return pe.reshape((1, max_seq_len, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "This is the core mechanism of the Transformer. It computes attention scores by taking the dot product of a query vector with all key vectors, scaling the result, applying a softmax, and then using these weights to create a weighted sum of the value vectors.\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_jax(q, k, v, mask=None):\n",
    "    \"\"\"Scaled dot-product attention for JAX.\"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(\n",
    "        jnp.array([d_k], dtype=jnp.float32)\n",
    "    )\n",
    "    if mask is not None:\n",
    "        scores = jnp.where(mask == 0, -1e9, scores)\n",
    "    attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "    output = jnp.matmul(attention_weights, v)\n",
    "    return output\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention_nabla(q, k, v, mask=None):\n",
    "    \"\"\"Scaled dot-product attention for Nabla.\"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = nb.matmul(q, k.permute((0, 1, 3, 2))) / nb.sqrt(\n",
    "        nb.array([d_k], dtype=nb.DType.float32)\n",
    "    )\n",
    "    if mask is not None:\n",
    "        scores = nb.where(mask, scores, nb.full_like(scores, -1e9))\n",
    "    attention_weights = nb.softmax(scores, axis=-1)\n",
    "    output = nb.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, we project the queries, keys, and values into multiple lower-dimensional spaces (\"heads\"). Attention is computed in parallel for each head, and the results are concatenated and projected back to the original dimension. This allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_jax(x, xa, params, mask=None):\n",
    "    \"\"\"Multi-head attention for JAX.\"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    d_head = d_model // NUM_HEADS\n",
    "\n",
    "    q_linear = jnp.matmul(x, params[\"w_q\"])\n",
    "    k_linear = jnp.matmul(xa, params[\"w_k\"])\n",
    "    v_linear = jnp.matmul(xa, params[\"w_v\"])\n",
    "\n",
    "    q = q_linear.reshape(batch_size, seq_len, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "    k = k_linear.reshape(batch_size, -1, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "    v = v_linear.reshape(batch_size, -1, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "\n",
    "    attention_output = scaled_dot_product_attention_jax(q, k, v, mask)\n",
    "\n",
    "    attention_output = attention_output.transpose((0, 2, 1, 3)).reshape(\n",
    "        batch_size, seq_len, d_model\n",
    "    )\n",
    "    return jnp.matmul(attention_output, params[\"w_o\"])\n",
    "\n",
    "\n",
    "def multi_head_attention_nabla(x, xa, params, mask=None):\n",
    "    \"\"\"Multi-head attention for Nabla.\"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    d_head = d_model // NUM_HEADS\n",
    "\n",
    "    q_linear = nb.matmul(x, params[\"w_q\"])\n",
    "    k_linear = nb.matmul(xa, params[\"w_k\"])\n",
    "    v_linear = nb.matmul(xa, params[\"w_v\"])\n",
    "\n",
    "    q = q_linear.reshape((batch_size, seq_len, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "    k = k_linear.reshape((batch_size, -1, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "    v = v_linear.reshape((batch_size, -1, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "\n",
    "    attention_output = scaled_dot_product_attention_nabla(q, k, v, mask)\n",
    "\n",
    "    attention_output = attention_output.permute((0, 2, 1, 3)).reshape(\n",
    "        (batch_size, seq_len, d_model)\n",
    "    )\n",
    "    return nb.matmul(attention_output, params[\"w_o\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Position-wise Feed-Forward Network\n",
    "\n",
    "Each encoder and decoder layer contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_jax(x, params):\n",
    "    \"\"\"Position-wise feed-forward network for JAX.\"\"\"\n",
    "    hidden = jax.nn.relu(jnp.matmul(x, params[\"w1\"]) + params[\"b1\"])\n",
    "    output = jnp.matmul(hidden, params[\"w2\"]) + params[\"b2\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "def feed_forward_nabla(x, params):\n",
    "    \"\"\"Position-wise feed-forward network for Nabla.\"\"\"\n",
    "    hidden = nb.relu(nb.matmul(x, params[\"w1\"]) + params[\"b1\"])\n",
    "    output = nb.matmul(hidden, params[\"w2\"]) + params[\"b2\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer Normalization\n",
    "\n",
    "Layer Normalization is used to stabilize the network and speed up training. We use a \"Pre-Norm\" architecture, where normalization is applied *before* each sub-layer (attention and FFN), followed by a residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_jax(x, params, eps=1e-6):\n",
    "    \"\"\"Layer normalization for JAX.\"\"\"\n",
    "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "    variance = jnp.mean((x - mean) * (x - mean), axis=-1, keepdims=True)\n",
    "    normalized = (x - mean) / jnp.sqrt(variance + eps)\n",
    "    return params[\"gamma\"] * normalized + params[\"beta\"]\n",
    "\n",
    "\n",
    "def layer_norm_nabla(x, params, eps=1e-6):\n",
    "    \"\"\"Layer normalization for Nabla.\"\"\"\n",
    "    mean = nb.mean(x, axes=[-1], keep_dims=True)\n",
    "    variance = nb.mean((x - mean) * (x - mean), axes=[-1], keep_dims=True)\n",
    "    normalized = (x - mean) / nb.sqrt(variance + eps)\n",
    "    return params[\"gamma\"] * normalized + params[\"beta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encoder and Decoder Layers\n",
    "\n",
    "We now assemble the building blocks into complete Encoder and Decoder layers.\n",
    "\n",
    "- **Encoder Layer**: Contains a self-attention mechanism and a feed-forward network.\n",
    "- **Decoder Layer**: Contains a masked self-attention mechanism, a cross-attention mechanism (attending to the encoder's output), and a feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Layer Implementations ---\n",
    "def encoder_layer_jax(x, params, mask):\n",
    "    norm_x = layer_norm_jax(x, params[\"norm1\"])\n",
    "    attention_output = multi_head_attention_jax(norm_x, norm_x, params[\"mha\"], mask)\n",
    "    x = x + attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm2\"])\n",
    "    ffn_output = feed_forward_jax(norm_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder_layer_jax(x, encoder_output, params, look_ahead_mask, padding_mask):\n",
    "    norm_x = layer_norm_jax(x, params[\"norm1\"])\n",
    "    masked_attention_output = multi_head_attention_jax(\n",
    "        norm_x, norm_x, params[\"masked_mha\"], look_ahead_mask\n",
    "    )\n",
    "    x = x + masked_attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm2\"])\n",
    "    cross_attention_output = multi_head_attention_jax(\n",
    "        norm_x, encoder_output, params[\"cross_mha\"], padding_mask\n",
    "    )\n",
    "    x = x + cross_attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm3\"])\n",
    "    ffn_output = feed_forward_jax(norm_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "# --- Nabla Layer Implementations ---\n",
    "def encoder_layer_nabla(x, params, mask):\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm1\"])\n",
    "    attention_output = multi_head_attention_nabla(\n",
    "        normed_x, normed_x, params[\"mha\"], mask\n",
    "    )\n",
    "    x = x + attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm2\"])\n",
    "    ffn_output = feed_forward_nabla(normed_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder_layer_nabla(x, encoder_output, params, look_ahead_mask, padding_mask):\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm1\"])\n",
    "    masked_attention_output = multi_head_attention_nabla(\n",
    "        normed_x, normed_x, params[\"masked_mha\"], look_ahead_mask\n",
    "    )\n",
    "    x = x + masked_attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm2\"])\n",
    "    cross_attention_output = multi_head_attention_nabla(\n",
    "        normed_x, encoder_output, params[\"cross_mha\"], padding_mask\n",
    "    )\n",
    "    x = x + cross_attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm3\"])\n",
    "    ffn_output = feed_forward_nabla(normed_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Embedding Lookup\n",
    "\n",
    "Standard libraries provide a simple `embedding[ids]` lookup. To maintain a \"from-scratch\" feel and ensure a direct comparison, we implement this lookup manually using `where` operations. This converts integer token IDs into their corresponding dense vector representations from an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup_jax(token_ids, embedding_matrix):\n",
    "    \"\"\"Manual embedding lookup for JAX.\"\"\"\n",
    "    return embedding_matrix[token_ids]\n",
    "\n",
    "\n",
    "def embedding_lookup_nabla(token_ids, embedding_matrix):\n",
    "    \"\"\"Manual embedding lookup for Nabla.\"\"\"\n",
    "    batch_size, seq_len = token_ids.shape\n",
    "    vocab_size, d_model = embedding_matrix.shape\n",
    "    output = nb.zeros((batch_size, seq_len, d_model))\n",
    "    for token_idx in range(vocab_size):\n",
    "        token_idx_array = nb.array([token_idx], dtype=nb.DType.int32)\n",
    "        condition = nb.equal(\n",
    "            token_ids, nb.broadcast_to(token_idx_array, token_ids.shape)\n",
    "        )\n",
    "        condition_expanded = nb.broadcast_to(\n",
    "            condition.reshape((batch_size, seq_len, 1)), (batch_size, seq_len, d_model)\n",
    "        )\n",
    "        token_embedding = embedding_matrix[token_idx : token_idx + 1, :].reshape(\n",
    "            (1, 1, d_model)\n",
    "        )\n",
    "        token_embedding_expanded = nb.broadcast_to(\n",
    "            token_embedding, (batch_size, seq_len, d_model)\n",
    "        )\n",
    "        output = nb.where(condition_expanded, token_embedding_expanded, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full Transformer Forward Pass\n",
    "\n",
    "Here, we combine all the preceding components into the complete encoder-decoder forward pass. This function takes the source and target token sequences, processes them through the respective stacks of layers, and produces the final output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Full Forward Pass ---\n",
    "def transformer_forward_jax(encoder_inputs, decoder_inputs, params):\n",
    "    target_seq_len = decoder_inputs.shape[1]\n",
    "    positions = jnp.arange(target_seq_len)\n",
    "    causal_mask = positions.reshape((target_seq_len, 1)) >= positions.reshape(\n",
    "        (1, target_seq_len)\n",
    "    )\n",
    "    look_ahead_mask = causal_mask.reshape((1, 1, target_seq_len, target_seq_len))\n",
    "\n",
    "    encoder_seq_len = encoder_inputs.shape[1]\n",
    "    decoder_seq_len = decoder_inputs.shape[1]\n",
    "\n",
    "    encoder_embeddings = embedding_lookup_jax(\n",
    "        encoder_inputs, params[\"encoder\"][\"embedding\"]\n",
    "    )\n",
    "    encoder_pos_enc = params[\"pos_encoding\"][:, :encoder_seq_len, :]\n",
    "    encoder_x = encoder_embeddings + encoder_pos_enc\n",
    "\n",
    "    encoder_output = encoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        encoder_output = encoder_layer_jax(\n",
    "            encoder_output, params[\"encoder\"][f\"layer_{i}\"], mask=None\n",
    "        )\n",
    "    encoder_output = layer_norm_jax(encoder_output, params[\"encoder\"][\"final_norm\"])\n",
    "\n",
    "    decoder_embeddings = embedding_lookup_jax(\n",
    "        decoder_inputs, params[\"decoder\"][\"embedding\"]\n",
    "    )\n",
    "    decoder_pos_enc = params[\"pos_encoding\"][:, :decoder_seq_len, :]\n",
    "    decoder_x = decoder_embeddings + decoder_pos_enc\n",
    "\n",
    "    decoder_output = decoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        decoder_output = decoder_layer_jax(\n",
    "            decoder_output,\n",
    "            encoder_output,\n",
    "            params[\"decoder\"][f\"layer_{i}\"],\n",
    "            look_ahead_mask,\n",
    "            padding_mask=None,\n",
    "        )\n",
    "    decoder_output = layer_norm_jax(decoder_output, params[\"decoder\"][\"final_norm\"])\n",
    "\n",
    "    logits = jnp.matmul(decoder_output, params[\"output_linear\"])\n",
    "    return logits\n",
    "\n",
    "\n",
    "# --- Nabla Full Forward Pass ---\n",
    "def transformer_forward_nabla(encoder_inputs, decoder_inputs, params):\n",
    "    target_seq_len = decoder_inputs.shape[1]\n",
    "    positions = nb.ndarange((target_seq_len,))\n",
    "    causal_mask = nb.greater_equal(\n",
    "        nb.reshape(positions, (target_seq_len, 1)),\n",
    "        nb.reshape(positions, (1, target_seq_len)),\n",
    "    )\n",
    "    look_ahead_mask = nb.reshape(causal_mask, (1, 1, target_seq_len, target_seq_len))\n",
    "\n",
    "    encoder_seq_len = encoder_inputs.shape[1]\n",
    "    decoder_seq_len = decoder_inputs.shape[1]\n",
    "\n",
    "    encoder_embeddings = embedding_lookup_nabla(\n",
    "        encoder_inputs, params[\"encoder\"][\"embedding\"]\n",
    "    )\n",
    "    encoder_pos_enc = params[\"pos_encoding\"][:, :encoder_seq_len, :]\n",
    "    encoder_x = encoder_embeddings + encoder_pos_enc\n",
    "\n",
    "    encoder_output = encoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        encoder_output = encoder_layer_nabla(\n",
    "            encoder_output, params[\"encoder\"][f\"layer_{i}\"], mask=None\n",
    "        )\n",
    "    encoder_output = layer_norm_nabla(encoder_output, params[\"encoder\"][\"final_norm\"])\n",
    "\n",
    "    decoder_embeddings = embedding_lookup_nabla(\n",
    "        decoder_inputs, params[\"decoder\"][\"embedding\"]\n",
    "    )\n",
    "    decoder_pos_enc = params[\"pos_encoding\"][:, :decoder_seq_len, :]\n",
    "    decoder_x = decoder_embeddings + decoder_pos_enc\n",
    "\n",
    "    decoder_output = decoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        decoder_output = decoder_layer_nabla(\n",
    "            decoder_output,\n",
    "            encoder_output,\n",
    "            params[\"decoder\"][f\"layer_{i}\"],\n",
    "            look_ahead_mask,\n",
    "            padding_mask=None,\n",
    "        )\n",
    "    decoder_output = layer_norm_nabla(decoder_output, params[\"decoder\"][\"final_norm\"])\n",
    "\n",
    "    logits = nb.matmul(decoder_output, params[\"output_linear\"])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loss Function\n",
    "\n",
    "We use the standard cross-entropy loss to train our model. To keep the implementations comparable, we manually create one-hot encoded targets from the integer labels and then compute the loss against the model's log-softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_log_softmax_jax(x, axis=-1):\n",
    "    \"\"\"Manual log softmax implementation for JAX.\"\"\"\n",
    "    x_max = jnp.max(x, axis=axis, keepdims=True)\n",
    "    x_shifted = x - x_max\n",
    "    log_sum_exp = jnp.log(jnp.sum(jnp.exp(x_shifted), axis=axis, keepdims=True))\n",
    "    return x_shifted - log_sum_exp\n",
    "\n",
    "\n",
    "def manual_log_softmax_nabla(x, axis=-1):\n",
    "    \"\"\"Manual log softmax implementation for Nabla.\"\"\"\n",
    "    x_max = nb.max(x, axes=[axis], keep_dims=True)\n",
    "    x_shifted = x - x_max\n",
    "    log_sum_exp = nb.log(nb.sum(nb.exp(x_shifted), axes=[axis], keep_dims=True))\n",
    "    return x_shifted - log_sum_exp\n",
    "\n",
    "\n",
    "def cross_entropy_loss_jax(logits, targets):\n",
    "    \"\"\"Cross-entropy loss for JAX.\"\"\"\n",
    "    batch_size, seq_len = targets.shape\n",
    "    vocab_size = logits.shape[-1]\n",
    "    targets_expanded = jnp.expand_dims(targets, -1)\n",
    "    vocab_indices = jnp.arange(vocab_size, dtype=jnp.int32).reshape((1, 1, vocab_size))\n",
    "    one_hot_targets = jnp.equal(targets_expanded, vocab_indices).astype(jnp.float32)\n",
    "    log_probs = manual_log_softmax_jax(logits)\n",
    "    cross_entropy = -jnp.sum(one_hot_targets * log_probs)\n",
    "    return cross_entropy / batch_size\n",
    "\n",
    "\n",
    "def cross_entropy_loss_nabla(logits, targets):\n",
    "    \"\"\"Cross-entropy loss for Nabla.\"\"\"\n",
    "    batch_size, seq_len = targets.shape\n",
    "    vocab_size = logits.shape[-1]\n",
    "    targets_expanded = targets.reshape((batch_size, seq_len, 1))\n",
    "    vocab_indices = nb.ndarange((vocab_size,), dtype=nb.DType.int32).reshape(\n",
    "        (1, 1, vocab_size)\n",
    "    )\n",
    "    one_hot_targets = nb.equal(targets_expanded, vocab_indices).astype(nb.DType.float32)\n",
    "    log_probs = manual_log_softmax_nabla(logits)\n",
    "    cross_entropy = -nb.sum(one_hot_targets * log_probs)\n",
    "    return cross_entropy / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Parameter Initialization\n",
    "\n",
    "We initialize the model's weights and biases. Linear layer weights are initialized using Glorot (Xavier) uniform initialization, while biases and normalization parameters are initialized to zeros and ones, respectively. Embeddings are initialized from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Parameter Initialization ---\n",
    "def _init_encoder_layer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": glorot((D_MODEL, D_FF)),\n",
    "            \"b1\": jnp.zeros(D_FF),\n",
    "            \"w2\": glorot((D_FF, D_MODEL)),\n",
    "            \"b2\": jnp.zeros(D_MODEL),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm2\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "    }\n",
    "\n",
    "\n",
    "def _init_decoder_layer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"masked_mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"cross_mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": glorot((D_MODEL, D_FF)),\n",
    "            \"b1\": jnp.zeros(D_FF),\n",
    "            \"w2\": glorot((D_FF, D_MODEL)),\n",
    "            \"b2\": jnp.zeros(D_MODEL),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm2\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm3\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "    }\n",
    "\n",
    "\n",
    "def init_transformer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    def randn(shape):\n",
    "        return jax.random.normal(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    params: dict[str, Any] = {\"encoder\": {}, \"decoder\": {}}\n",
    "    params[\"encoder\"][\"embedding\"] = randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"decoder\"][\"embedding\"] = randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"pos_encoding\"] = positional_encoding_jax(MAX_SEQ_LEN, D_MODEL)\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"encoder\"][f\"layer_{i}\"] = _init_encoder_layer_params_jax()\n",
    "    params[\"encoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": jnp.ones(D_MODEL),\n",
    "        \"beta\": jnp.zeros(D_MODEL),\n",
    "    }\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"decoder\"][f\"layer_{i}\"] = _init_decoder_layer_params_jax()\n",
    "    params[\"decoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": jnp.ones(D_MODEL),\n",
    "        \"beta\": jnp.zeros(D_MODEL),\n",
    "    }\n",
    "    params[\"output_linear\"] = glorot((D_MODEL, VOCAB_SIZE))\n",
    "    return params\n",
    "\n",
    "\n",
    "# --- Nabla Parameter Initialization ---\n",
    "def _init_encoder_layer_params_nabla():\n",
    "    return {\n",
    "        \"mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": nb.glorot_uniform((D_MODEL, D_FF)),\n",
    "            \"b1\": nb.zeros((D_FF,)),\n",
    "            \"w2\": nb.glorot_uniform((D_FF, D_MODEL)),\n",
    "            \"b2\": nb.zeros((D_MODEL,)),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm2\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "    }\n",
    "\n",
    "\n",
    "def _init_decoder_layer_params_nabla():\n",
    "    return {\n",
    "        \"masked_mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"cross_mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": nb.glorot_uniform((D_MODEL, D_FF)),\n",
    "            \"b1\": nb.zeros((D_FF,)),\n",
    "            \"w2\": nb.glorot_uniform((D_FF, D_MODEL)),\n",
    "            \"b2\": nb.zeros((D_MODEL,)),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm2\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm3\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "    }\n",
    "\n",
    "\n",
    "def init_transformer_params_nabla():\n",
    "    params: dict[str, Any] = {\"encoder\": {}, \"decoder\": {}}\n",
    "    params[\"encoder\"][\"embedding\"] = nb.randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"decoder\"][\"embedding\"] = nb.randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"pos_encoding\"] = positional_encoding_nabla(MAX_SEQ_LEN, D_MODEL)\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"encoder\"][f\"layer_{i}\"] = _init_encoder_layer_params_nabla()\n",
    "    params[\"encoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": nb.ones((D_MODEL,)),\n",
    "        \"beta\": nb.zeros((D_MODEL,)),\n",
    "    }\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"decoder\"][f\"layer_{i}\"] = _init_decoder_layer_params_nabla()\n",
    "    params[\"decoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": nb.ones((D_MODEL,)),\n",
    "        \"beta\": nb.zeros((D_MODEL,)),\n",
    "    }\n",
    "    params[\"output_linear\"] = nb.glorot_uniform((D_MODEL, VOCAB_SIZE))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Generation\n",
    "\n",
    "This function generates a batch of data for our sequence reversal task. For an input sequence like `[a, b, c]`, it produces:\n",
    "- **Encoder Input:** `[a, b, c]`\n",
    "- **Decoder Input:** `[<START>, c, b, a]` (for teacher-forcing during training)\n",
    "- **Target:** `[c, b, a, <END>]` (the ground truth for the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(batch_size):\n",
    "    \"\"\"Generates a dataset batch using NumPy first, then converts to framework-specific arrays.\"\"\"\n",
    "    base_sequences_np = np.random.randint(\n",
    "        3, VOCAB_SIZE, size=(batch_size, SOURCE_SEQ_LEN), dtype=np.int32\n",
    "    )\n",
    "    reversed_sequences_np = np.flip(base_sequences_np, axis=1)\n",
    "    encoder_input_np = base_sequences_np\n",
    "    start_tokens_np = np.ones((batch_size, 1), dtype=np.int32)  # <START> token (1)\n",
    "    decoder_input_np = np.concatenate([start_tokens_np, reversed_sequences_np], axis=1)\n",
    "    end_tokens_np = np.full((batch_size, 1), 2, dtype=np.int32)  # <END> token (2)\n",
    "    target_np = np.concatenate([reversed_sequences_np, end_tokens_np], axis=1)\n",
    "    return encoder_input_np, decoder_input_np, target_np\n",
    "\n",
    "\n",
    "def create_reverse_dataset_jax(batch_size):\n",
    "    encoder_input_np, decoder_input_np, target_np = create_reverse_dataset(batch_size)\n",
    "    return (\n",
    "        jnp.array(encoder_input_np),\n",
    "        jnp.array(decoder_input_np),\n",
    "        jnp.array(target_np),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_reverse_dataset_nabla(batch_size):\n",
    "    encoder_input_np, decoder_input_np, target_np = create_reverse_dataset(batch_size)\n",
    "    return (\n",
    "        nb.Array.from_numpy(encoder_input_np),\n",
    "        nb.Array.from_numpy(decoder_input_np),\n",
    "        nb.Array.from_numpy(target_np),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Optimizer (AdamW)\n",
    "\n",
    "We implement the AdamW optimizer, which is a variant of Adam that decouples weight decay from the gradient update. This often leads to better performance. The implementation includes state initialization (`m` and `v` vectors) and the update step, which also features gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_adamw_state_recursive(params, m_states, v_states, zeros_like_fn):\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            m_states[key], v_states[key] = {}, {}\n",
    "            _init_adamw_state_recursive(\n",
    "                value, m_states[key], v_states[key], zeros_like_fn\n",
    "            )\n",
    "        else:\n",
    "            m_states[key] = zeros_like_fn(value)\n",
    "            v_states[key] = zeros_like_fn(value)\n",
    "\n",
    "\n",
    "def init_adamw_state_jax(params):\n",
    "    m_states, v_states = {}, {}\n",
    "    _init_adamw_state_recursive(params, m_states, v_states, jnp.zeros_like)\n",
    "    return m_states, v_states\n",
    "\n",
    "\n",
    "def init_adamw_state_nabla(params):\n",
    "    m_states, v_states = {}, {}\n",
    "    _init_adamw_state_recursive(params, m_states, v_states, nb.zeros_like)\n",
    "    return m_states, v_states\n",
    "\n",
    "\n",
    "def adamw_step(params, grads, m, v, step, lr, framework_lib):\n",
    "    \"\"\"A generic AdamW step that can be used by both frameworks.\"\"\"\n",
    "    beta1, beta2, eps, weight_decay = 0.9, 0.999, 1e-8, 0.01\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    total_grad_norm_sq = [0.0]\n",
    "\n",
    "    def _calc_norm(g_dict):\n",
    "        for g in g_dict.values():\n",
    "            if isinstance(g, dict):\n",
    "                _calc_norm(g)\n",
    "            else:\n",
    "                total_grad_norm_sq[0] += framework_lib.sum(g * g)\n",
    "\n",
    "    _calc_norm(grads)\n",
    "    grad_norm = framework_lib.sqrt(total_grad_norm_sq[0])\n",
    "    clip_factor = framework_lib.minimum(1.0, max_grad_norm / (grad_norm + 1e-8))\n",
    "\n",
    "    updated_params, updated_m, updated_v = {}, {}, {}\n",
    "\n",
    "    def _update(p_dict, g_dict, m_dict, v_dict, up_p, up_m, up_v):\n",
    "        for key in p_dict:\n",
    "            if isinstance(p_dict[key], dict):\n",
    "                up_p[key], up_m[key], up_v[key] = {}, {}, {}\n",
    "                _update(\n",
    "                    p_dict[key],\n",
    "                    g_dict[key],\n",
    "                    m_dict[key],\n",
    "                    v_dict[key],\n",
    "                    up_p[key],\n",
    "                    up_m[key],\n",
    "                    up_v[key],\n",
    "                )\n",
    "            else:\n",
    "                p, g, m_val, v_val = (\n",
    "                    p_dict[key],\n",
    "                    g_dict[key] * clip_factor,\n",
    "                    m_dict[key],\n",
    "                    v_dict[key],\n",
    "                )\n",
    "                up_m[key] = beta1 * m_val + (1.0 - beta1) * g\n",
    "                up_v[key] = beta2 * v_val + (1.0 - beta2) * (g * g)\n",
    "                m_corr = up_m[key] / (1.0 - beta1**step)\n",
    "                v_corr = up_v[key] / (1.0 - beta2**step)\n",
    "                up_p[key] = p - lr * (\n",
    "                    m_corr / (framework_lib.sqrt(v_corr) + eps) + weight_decay * p\n",
    "                )\n",
    "\n",
    "    _update(params, grads, m, v, updated_params, updated_m, updated_v)\n",
    "    return updated_params, updated_m, updated_v\n",
    "\n",
    "\n",
    "def adamw_step_jax(params, grads, m, v, step, lr):\n",
    "    return adamw_step(params, grads, m, v, step, lr, jnp)\n",
    "\n",
    "\n",
    "def adamw_step_nabla(params, grads, m, v, step, lr):\n",
    "    return adamw_step(params, grads, m, v, step, lr, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. JIT-Compiled Training Step & Inference\n",
    "\n",
    "We define the core logic for a single training step and the inference (prediction) process.\n",
    "\n",
    "- **`complete_training_step`**: This function encapsulates the forward pass, loss calculation, backpropagation (gradient computation), and optimizer update. We decorate it with `@jax.jit` or `@nb.jit` to compile the entire sequence of operations into a single, highly optimized kernel for maximum performance.\n",
    "- **`predict_sequence`**: This function performs autoregressive inference. It generates the output sequence one token at a time, feeding its own prediction from the previous step back into the model as input for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Implementations ---\n",
    "@jax.jit\n",
    "def complete_training_step_jax(\n",
    "    encoder_in, decoder_in, targets, params, m_states, v_states, step\n",
    "):\n",
    "    def loss_fn(p):\n",
    "        return cross_entropy_loss_jax(\n",
    "            transformer_forward_jax(encoder_in, decoder_in, p), targets\n",
    "        )\n",
    "\n",
    "    loss_value, grads = value_and_grad(loss_fn)(params)\n",
    "    updated_params, updated_m, updated_v = adamw_step_jax(\n",
    "        params, grads, m_states, v_states, step, LEARNING_RATE\n",
    "    )\n",
    "    return updated_params, updated_m, updated_v, loss_value\n",
    "\n",
    "\n",
    "def predict_sequence_jax(encoder_input, params):\n",
    "    if encoder_input.ndim == 1:\n",
    "        encoder_input = jnp.expand_dims(encoder_input, axis=0)\n",
    "    decoder_tokens = [jnp.ones((1,), dtype=jnp.int32)]\n",
    "    decoder_input = jnp.zeros((1, TARGET_SEQ_LEN), dtype=jnp.int32)\n",
    "    decoder_input = decoder_input.at[:, 0].set(1)\n",
    "\n",
    "    for pos in range(1, TARGET_SEQ_LEN):\n",
    "        logits = transformer_forward_jax(encoder_input, decoder_input, params)\n",
    "        next_token_logits = logits[:, pos - 1, :]\n",
    "        predicted_token = jnp.argmax(next_token_logits, axis=-1).astype(jnp.int32)\n",
    "        if pos < TARGET_SEQ_LEN:\n",
    "            decoder_input = decoder_input.at[:, pos].set(predicted_token[0])\n",
    "\n",
    "    return decoder_input[0]\n",
    "\n",
    "\n",
    "# --- Nabla Implementations ---\n",
    "@nb.jit\n",
    "def complete_training_step_nabla(\n",
    "    encoder_in, decoder_in, targets, params, m_states, v_states, step\n",
    "):\n",
    "    def loss_fn(p):\n",
    "        return cross_entropy_loss_nabla(\n",
    "            transformer_forward_nabla(encoder_in, decoder_in, p), targets\n",
    "        )\n",
    "\n",
    "    loss_value, grads = nb.value_and_grad(loss_fn)(params)\n",
    "    updated_params, updated_m, updated_v = adamw_step_nabla(\n",
    "        params, grads, m_states, v_states, step, LEARNING_RATE\n",
    "    )\n",
    "    return updated_params, updated_m, updated_v, loss_value\n",
    "\n",
    "\n",
    "def predict_sequence_nabla(encoder_input, params):\n",
    "    if len(encoder_input.shape) == 1:\n",
    "        encoder_input = encoder_input.reshape((1, encoder_input.shape[0]))\n",
    "\n",
    "    decoder_tokens = [nb.ones((1,), dtype=nb.DType.int32)]\n",
    "\n",
    "    for pos in range(1, TARGET_SEQ_LEN):\n",
    "        current_decoder_input = nb.stack(decoder_tokens, axis=1)\n",
    "\n",
    "        logits = transformer_forward_nabla(encoder_input, current_decoder_input, params)\n",
    "\n",
    "        next_token_logits = logits[:, pos - 1, :]\n",
    "\n",
    "        predicted_token = nb.argmax(next_token_logits, axes=-1).astype(nb.DType.int32)\n",
    "\n",
    "        decoder_tokens.append(predicted_token)\n",
    "\n",
    "    final_sequence = nb.stack(decoder_tokens, axis=1)\n",
    "\n",
    "    return final_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. JAX Training Run\n",
    "\n",
    "This cell contains the complete training loop for the **JAX** implementation. It initializes the parameters and optimizer state, then iterates through the training epochs, calling the JIT-compiled `complete_training_step_jax` function. Finally, it evaluates the trained model on a few test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_jax():\n",
    "    \"\"\"Main training loop for the JAX transformer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸ¤– TRAINING TRANSFORMER FROM SCRATCH WITH JAX\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"ðŸ”§ Initializing transformer parameters...\")\n",
    "    params = init_transformer_params_jax()\n",
    "    print(\"ðŸ“ˆ Initializing AdamW optimizer...\")\n",
    "    m_states, v_states = init_adamw_state_jax(params)\n",
    "\n",
    "    print(\"ðŸ”¥ JIT warmup (3 steps)...\")\n",
    "    for i in range(3):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_jax(BATCH_SIZE)\n",
    "        params, m_states, v_states, _ = complete_training_step_jax(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, i + 1\n",
    "        )\n",
    "    print(\"âœ… Warmup complete! Starting timed training...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_history = []\n",
    "    time_history = [start_time]\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_jax(BATCH_SIZE)\n",
    "        params, m_states, v_states, loss = complete_training_step_jax(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, epoch\n",
    "        )\n",
    "        loss_history.append(float(loss))\n",
    "        time_history.append(time.time())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d} | Loss: {float(loss):.4f} | Time: {time_history[-1] - start_time:.1f}s\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"\\nâœ… JAX Training complete! Total time: {time_history[-1] - start_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    return params, loss_history, time_history\n",
    "\n",
    "\n",
    "def plot_loss_curves(\n",
    "    jax_loss_history, jax_time_history, nabla_loss_history, nabla_time_history\n",
    "):\n",
    "    \"\"\"Plot the loss curves for both JAX and Nabla implementations.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Plot loss vs epochs\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(jax_loss_history, label=\"JAX\")\n",
    "    plt.plot(nabla_loss_history, label=\"Nabla\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot loss vs time - ensure equal length by truncating the longer one\n",
    "    plt.subplot(1, 2, 2)\n",
    "    min_len = min(\n",
    "        len(jax_loss_history),\n",
    "        len(jax_time_history),\n",
    "        len(nabla_loss_history),\n",
    "        len(nabla_time_history),\n",
    "    )\n",
    "    jax_times = [t - jax_time_history[0] for t in jax_time_history[:min_len]]\n",
    "    nabla_times = [t - nabla_time_history[0] for t in nabla_time_history[:min_len]]\n",
    "    plt.plot(jax_times, jax_loss_history[:min_len], label=\"JAX\")\n",
    "    plt.plot(nabla_times, nabla_loss_history[:min_len], label=\"Nabla\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ¤– TRAINING TRANSFORMER FROM SCRATCH WITH JAX\n",
      "============================================================\n",
      "ðŸ”§ Initializing transformer parameters...\n",
      "ðŸ“ˆ Initializing AdamW optimizer...\n",
      "ðŸ”¥ JIT warmup (3 steps)...\n",
      "âœ… Warmup complete! Starting timed training...\n",
      "\n",
      "Epoch    10 | Loss: 27.5245 | Time: 0.1s\n",
      "Epoch    20 | Loss: 25.4663 | Time: 0.2s\n",
      "Epoch    30 | Loss: 23.5076 | Time: 0.3s\n",
      "Epoch    40 | Loss: 21.9884 | Time: 0.4s\n",
      "Epoch    50 | Loss: 19.9668 | Time: 0.5s\n",
      "Epoch    60 | Loss: 18.9467 | Time: 0.6s\n",
      "Epoch    70 | Loss: 17.0701 | Time: 0.7s\n",
      "Epoch    80 | Loss: 15.6757 | Time: 0.7s\n",
      "Epoch    90 | Loss: 13.7750 | Time: 0.8s\n",
      "Epoch   100 | Loss: 12.0263 | Time: 0.9s\n",
      "Epoch   110 | Loss: 9.4990 | Time: 1.0s\n",
      "Epoch   120 | Loss: 7.9771 | Time: 1.1s\n",
      "Epoch   130 | Loss: 6.1163 | Time: 1.2s\n",
      "Epoch   140 | Loss: 4.7434 | Time: 1.3s\n",
      "Epoch   150 | Loss: 3.2581 | Time: 1.3s\n",
      "Epoch   160 | Loss: 2.2719 | Time: 1.4s\n",
      "Epoch   170 | Loss: 1.7311 | Time: 1.5s\n",
      "Epoch   180 | Loss: 1.1432 | Time: 1.6s\n",
      "Epoch   190 | Loss: 0.8442 | Time: 1.7s\n",
      "Epoch   200 | Loss: 0.6001 | Time: 1.8s\n",
      "Epoch   210 | Loss: 0.3414 | Time: 1.8s\n",
      "Epoch   220 | Loss: 0.2645 | Time: 1.9s\n",
      "Epoch   230 | Loss: 0.1745 | Time: 2.0s\n",
      "Epoch   240 | Loss: 0.1230 | Time: 2.1s\n",
      "Epoch   250 | Loss: 0.1135 | Time: 2.2s\n",
      "Epoch   260 | Loss: 0.0844 | Time: 2.3s\n",
      "Epoch   270 | Loss: 0.0735 | Time: 2.4s\n",
      "Epoch   280 | Loss: 0.0531 | Time: 2.5s\n",
      "Epoch   290 | Loss: 0.0445 | Time: 2.6s\n",
      "Epoch   300 | Loss: 0.0298 | Time: 2.7s\n",
      "Epoch   310 | Loss: 0.0273 | Time: 2.9s\n",
      "Epoch   320 | Loss: 0.0262 | Time: 3.0s\n",
      "Epoch   330 | Loss: 0.0187 | Time: 3.2s\n",
      "Epoch   340 | Loss: 0.0190 | Time: 3.3s\n",
      "Epoch   350 | Loss: 0.0286 | Time: 3.4s\n",
      "Epoch   360 | Loss: 0.0536 | Time: 3.5s\n",
      "Epoch   370 | Loss: 0.0610 | Time: 3.6s\n",
      "Epoch   380 | Loss: 0.0141 | Time: 3.7s\n",
      "Epoch   390 | Loss: 0.0137 | Time: 3.9s\n",
      "Epoch   400 | Loss: 0.0141 | Time: 4.0s\n",
      "Epoch   410 | Loss: 0.0284 | Time: 4.1s\n",
      "Epoch   420 | Loss: 0.0110 | Time: 4.2s\n",
      "Epoch   430 | Loss: 0.0096 | Time: 4.3s\n",
      "Epoch   440 | Loss: 0.0140 | Time: 4.4s\n",
      "Epoch   450 | Loss: 0.0265 | Time: 4.5s\n",
      "Epoch   460 | Loss: 0.0087 | Time: 4.5s\n",
      "Epoch   470 | Loss: 0.0089 | Time: 4.6s\n",
      "Epoch   480 | Loss: 0.0163 | Time: 4.7s\n",
      "Epoch   490 | Loss: 0.0137 | Time: 4.8s\n",
      "Epoch   500 | Loss: 0.0110 | Time: 4.9s\n",
      "\n",
      "âœ… JAX Training complete! Total time: 4.9s\n"
     ]
    }
   ],
   "source": [
    "# Run the JAX training and store results\n",
    "jax_params, jax_loss_history, jax_time_history = train_transformer_jax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Nabla Training Run\n",
    "\n",
    "This cell contains the complete training loop for the **Nabla** implementation. It follows the same structure as the JAX version, using the `_nabla` suffixed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_nabla():\n",
    "    \"\"\"Main training loop for the Nabla transformer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸ¤– TRAINING TRANSFORMER FROM SCRATCH WITH NABLA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"ðŸ”§ Initializing transformer parameters...\")\n",
    "    params = init_transformer_params_nabla()\n",
    "    print(\"ðŸ“ˆ Initializing AdamW optimizer...\")\n",
    "    m_states, v_states = init_adamw_state_nabla(params)\n",
    "\n",
    "    print(\"ðŸ”¥ JIT warmup (3 steps)...\")\n",
    "    for i in range(3):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_nabla(BATCH_SIZE)\n",
    "        params, m_states, v_states, _ = complete_training_step_nabla(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, i + 1\n",
    "        )\n",
    "    print(\"âœ… Warmup complete! Starting timed training...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_history = []\n",
    "    time_history = [start_time]\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_nabla(BATCH_SIZE)\n",
    "        params, m_states, v_states, loss = complete_training_step_nabla(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, epoch\n",
    "        )\n",
    "        loss_history.append(float(loss.to_numpy()))\n",
    "        time_history.append(time.time())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d} | Loss: {loss.to_numpy():.4f} | Time: {time_history[-1] - start_time:.1f}s\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"\\nâœ… Nabla Training complete! Total time: {time_history[-1] - start_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    return params, loss_history, time_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ¤– TRAINING TRANSFORMER FROM SCRATCH WITH NABLA\n",
      "============================================================\n",
      "ðŸ”§ Initializing transformer parameters...\n",
      "ðŸ“ˆ Initializing AdamW optimizer...\n",
      "ðŸ”¥ JIT warmup (3 steps)...\n",
      "âœ… Warmup complete! Starting timed training...\n",
      "\n",
      "Epoch    10 | Loss: 28.5808 | Time: 0.1s\n",
      "Epoch    20 | Loss: 27.7783 | Time: 0.2s\n",
      "Epoch    30 | Loss: 26.5810 | Time: 0.3s\n",
      "Epoch    40 | Loss: 24.3916 | Time: 0.4s\n",
      "Epoch    50 | Loss: 22.2699 | Time: 0.4s\n",
      "Epoch    60 | Loss: 20.6258 | Time: 0.5s\n",
      "Epoch    70 | Loss: 19.4857 | Time: 0.6s\n",
      "Epoch    80 | Loss: 17.4208 | Time: 0.7s\n",
      "Epoch    90 | Loss: 15.8058 | Time: 0.8s\n",
      "Epoch   100 | Loss: 13.9562 | Time: 0.9s\n",
      "Epoch   110 | Loss: 11.5999 | Time: 1.0s\n",
      "Epoch   120 | Loss: 9.1855 | Time: 1.1s\n",
      "Epoch   130 | Loss: 6.5512 | Time: 1.2s\n",
      "Epoch   140 | Loss: 3.6793 | Time: 1.3s\n",
      "Epoch   150 | Loss: 1.9361 | Time: 1.4s\n",
      "Epoch   160 | Loss: 1.0085 | Time: 1.5s\n",
      "Epoch   170 | Loss: 0.7367 | Time: 1.6s\n",
      "Epoch   180 | Loss: 0.3662 | Time: 1.7s\n",
      "Epoch   190 | Loss: 0.2475 | Time: 1.7s\n",
      "Epoch   200 | Loss: 0.1554 | Time: 1.8s\n",
      "Epoch   210 | Loss: 0.1893 | Time: 1.9s\n",
      "Epoch   220 | Loss: 0.0946 | Time: 2.0s\n",
      "Epoch   230 | Loss: 0.1244 | Time: 2.1s\n",
      "Epoch   240 | Loss: 0.1072 | Time: 2.2s\n",
      "Epoch   250 | Loss: 0.0600 | Time: 2.3s\n",
      "Epoch   260 | Loss: 0.0586 | Time: 2.3s\n",
      "Epoch   270 | Loss: 0.0534 | Time: 2.4s\n",
      "Epoch   280 | Loss: 0.0279 | Time: 2.5s\n",
      "Epoch   290 | Loss: 0.0254 | Time: 2.6s\n",
      "Epoch   300 | Loss: 0.0194 | Time: 2.7s\n",
      "Epoch   310 | Loss: 0.0178 | Time: 2.8s\n",
      "Epoch   320 | Loss: 0.0201 | Time: 2.9s\n",
      "Epoch   330 | Loss: 0.0170 | Time: 3.0s\n",
      "Epoch   340 | Loss: 0.0211 | Time: 3.0s\n",
      "Epoch   350 | Loss: 0.0158 | Time: 3.1s\n",
      "Epoch   360 | Loss: 0.0133 | Time: 3.2s\n",
      "Epoch   370 | Loss: 0.0132 | Time: 3.3s\n",
      "Epoch   380 | Loss: 0.0107 | Time: 3.4s\n",
      "Epoch   390 | Loss: 0.0115 | Time: 3.5s\n",
      "Epoch   400 | Loss: 0.0411 | Time: 3.6s\n",
      "Epoch   410 | Loss: 0.0114 | Time: 3.7s\n",
      "Epoch   420 | Loss: 0.0182 | Time: 3.8s\n",
      "Epoch   430 | Loss: 0.0091 | Time: 3.9s\n",
      "Epoch   440 | Loss: 0.0102 | Time: 3.9s\n",
      "Epoch   450 | Loss: 0.0080 | Time: 4.0s\n",
      "Epoch   460 | Loss: 0.0081 | Time: 4.1s\n",
      "Epoch   470 | Loss: 0.0084 | Time: 4.2s\n",
      "Epoch   480 | Loss: 0.0074 | Time: 4.3s\n",
      "Epoch   490 | Loss: 0.0089 | Time: 4.4s\n",
      "Epoch   500 | Loss: 0.0075 | Time: 4.4s\n",
      "\n",
      "âœ… Nabla Training complete! Total time: 4.4s\n"
     ]
    }
   ],
   "source": [
    "# Run the Nabla training and store results\n",
    "nabla_params, nabla_loss_history, nabla_time_history = train_transformer_nabla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Loss Curves Visualization\n",
    "\n",
    "Now that we've trained both models, let's visualize and compare their training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves for both implementations\n",
    "plot_loss_curves(\n",
    "    jax_loss_history, jax_time_history, nabla_loss_history, nabla_time_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Evaluation\n",
    "\n",
    "Let's evaluate both models on the same test examples to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ§ª FINAL JAX EVALUATION\n",
      "============================================================\n",
      "Example 1:\n",
      "  Input:           [ 3 16  6  6  3  4 10  4  8]\n",
      "  Expected output: [ 8  4 10  4  3  6  6 16  3  2]\n",
      "  Predicted:       [ 8  4 10  4  3  6  6 16  3  2]\n",
      "  Correct:         âœ… YES\n",
      "Example 2:\n",
      "  Input:           [ 6  7 14  6  3 17  6 15  4]\n",
      "  Expected output: [ 4 15  6 17  3  6 14  7  6  2]\n",
      "  Predicted:       [ 4 15  6 17  3  6 14  7  6  2]\n",
      "  Correct:         âœ… YES\n",
      "Example 3:\n",
      "  Input:           [15 12 13 15 13  8  7  4  3]\n",
      "  Expected output: [ 3  4  7  8 13 15 13 12 15  2]\n",
      "  Predicted:       [ 3  4  7  8 13 15 13 12 15  2]\n",
      "  Correct:         âœ… YES\n",
      "Example 4:\n",
      "  Input:           [ 8  4 17  3 12 10  8  5 14]\n",
      "  Expected output: [14  5  8 10 12  3 17  4  8  2]\n",
      "  Predicted:       [14  5  8 10 12  3 17  4  8  2]\n",
      "  Correct:         âœ… YES\n",
      "Example 5:\n",
      "  Input:           [ 4 19 17 18  6 12 11 15 10]\n",
      "  Expected output: [10 15 11 12  6 18 17 19  4  2]\n",
      "  Predicted:       [10 15 11 12  6 18 17 19  4  2]\n",
      "  Correct:         âœ… YES\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª FINAL NABLA EVALUATION\n",
      "============================================================\n",
      "Example 1:\n",
      "  Input:           [ 5  3  4  9  4 13 14 11 14]:\u001b[95mi32[9]\u001b[0m\n",
      "  Expected output: [14 11 14 13  4  9  4  3  5  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Predicted:       [14 11 14 13  4  9  4  3  5  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Correct:         âœ… YES\n",
      "Example 2:\n",
      "  Input:           [16 18  6 17  9 17  3  6  7]:\u001b[95mi32[9]\u001b[0m\n",
      "  Expected output: [ 7  6  3 17  9 17  6 18 16  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Predicted:       [ 7  6  3 17  9 17  6 18 16  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Correct:         âœ… YES\n",
      "Example 3:\n",
      "  Input:           [12  4  3 14 11 12 16 15  8]:\u001b[95mi32[9]\u001b[0m\n",
      "  Expected output: [ 8 15 16 12 11 14  3  4 12  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Predicted:       [ 8 15 16 12 11 14  3  4 12  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Correct:         âœ… YES\n",
      "Example 4:\n",
      "  Input:           [17 18  9  5  4  8  9  3 19]:\u001b[95mi32[9]\u001b[0m\n",
      "  Expected output: [19  3  9  8  4  5  9 18 17  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Predicted:       [19  3  9  8  4  5  9 18 17  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Correct:         âœ… YES\n",
      "Example 5:\n",
      "  Input:           [ 4 16  7  6  7 16 16 12 11]:\u001b[95mi32[9]\u001b[0m\n",
      "  Expected output: [11 12 16 16  7  6  7 16  4  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Predicted:       [11 12 16 16  7  6  7 16  4  2]:\u001b[95mi32[10]\u001b[0m\n",
      "  Correct:         âœ… YES\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(params, predict_fn, create_dataset_fn, framework_name):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ§ª FINAL {framework_name.upper()} EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test on 5 random examples\n",
    "    for i in range(5):\n",
    "        test_enc_in, _, test_target = create_dataset_fn(1)\n",
    "        prediction = predict_fn(test_enc_in[0], params)\n",
    "\n",
    "        if framework_name == \"jax\":\n",
    "            is_correct = jnp.array_equal(prediction[1:], test_target[0])\n",
    "            print(f\"Example {i + 1}:\")\n",
    "            print(f\"  Input:           {test_enc_in[0]}\")\n",
    "            print(f\"  Expected output: {test_target[0]}\")\n",
    "            print(f\"  Predicted:       {prediction[1:]}\")\n",
    "            print(f\"  Correct:         {'âœ… YES' if is_correct else 'âŒ NO'}\")\n",
    "        else:  # nabla\n",
    "            is_correct = np.array_equal(\n",
    "                prediction[1:].to_numpy(), test_target[0].to_numpy()\n",
    "            )\n",
    "            print(f\"Example {i + 1}:\")\n",
    "            print(f\"  Input:           {test_enc_in[0]}\")\n",
    "            print(f\"  Expected output: {test_target[0]}\")\n",
    "            print(f\"  Predicted:       {prediction[1:]}\")\n",
    "            print(f\"  Correct:         {'âœ… YES' if is_correct else 'âŒ NO'}\")\n",
    "\n",
    "\n",
    "# Evaluate JAX model\n",
    "evaluate_model(jax_params, predict_sequence_jax, create_reverse_dataset_jax, \"jax\")\n",
    "\n",
    "# Evaluate Nabla model\n",
    "evaluate_model(\n",
    "    nabla_params, predict_sequence_nabla, create_reverse_dataset_nabla, \"nabla\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nabla Development)",
   "language": "python",
   "name": "nabla-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
