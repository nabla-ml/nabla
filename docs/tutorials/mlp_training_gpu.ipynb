{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Training (GPU)\n",
    "\n",
    "In this tutorial, we'll walk through how to use Nabla with GPU acceleration to train a neural network to learn a complex sin function. We'll cover installation, device setup, and the training loop with jitting for GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Nabla is ready! Running on Python 3.10\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "try:\n",
    "    import nabla as nb\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"modular\",\n",
    "            \"--extra-index-url\",\n",
    "            \"https://download.pytorch.org/whl/cpu\",\n",
    "            \"--index-url\",\n",
    "            \"https://dl.modular.com/public/nightly/python/simple/\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"nabla-ml\", \"--upgrade\"], check=True\n",
    "    )\n",
    "    import nabla as nb\n",
    "\n",
    "# Import other required libraries\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\n",
    "    f\"ðŸŽ‰ Nabla is ready! Running on Python {sys.version_info.major}.{sys.version_info.minor}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Nabla and GPU Acceleration\n",
    "\n",
    "Nabla is a deep learning library that leverages the Modular MLIR compiler for high-performance computation. One of its key features is GPU acceleration, which is achieved through jitting (Just-In-Time compilation). This means that functions decorated with `@nb.jit` are compiled into optimized GPU code by the Modular compiler.\n",
    "\n",
    "### Why `to(device)`?\n",
    "\n",
    "In Nabla, tensors need to be explicitly moved to the desired device (CPU or GPU) using the `to(device)` method. This is because Nabla's GPU mode is only accessible within jitted functions. The device is determined at runtime, and tensors must be on the correct device for operations to execute efficiently.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Jitting**: Functions decorated with `@nb.jit` are compiled and optimized for GPU execution.\n",
    "2. **Device Placement**: Tensors must be moved to the appropriate device using `to(device)`.\n",
    "3. **Training Loop**: The training loop involves creating datasets, computing gradients, and updating parameters using an optimizer.\n",
    "\n",
    "Let's dive into the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device(type=gpu,id=0) device\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 4\n",
    "LAYERS = [1, 64, 128, 256, 128, 64, 1]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "PRINT_INTERVAL = 100\n",
    "SIN_PERIODS = 8\n",
    "\n",
    "device = nb.cpu() if nb.accelerator_count() == 0 else nb.accelerator()\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(x: nb.Array, params: list[nb.Array]) -> nb.Array:\n",
    "    \"\"\"MLP forward pass through all layers.\"\"\"\n",
    "    output = x\n",
    "    for i in range(0, len(params) - 1, 2):\n",
    "        w, b = params[i], params[i + 1]\n",
    "        output = nb.matmul(output, w) + b\n",
    "        # Apply ReLU to all layers except the last\n",
    "        if i < len(params) - 2:\n",
    "            output = nb.relu(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def mean_squared_error(predictions: nb.Array, targets: nb.Array) -> nb.Array:\n",
    "    \"\"\"Compute mean squared error loss.\"\"\"\n",
    "    diff = predictions - targets\n",
    "    squared_errors = diff * diff\n",
    "    batch_size = nb.array(predictions.shape[0], dtype=nb.DType.float32).to(device)\n",
    "    loss = nb.sum(squared_errors) / batch_size\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mlp_forward_and_loss(inputs: list[nb.Array]) -> nb.Array:\n",
    "    \"\"\"Combined forward pass and loss computation for VJP with leaky ReLU.\"\"\"\n",
    "    x, targets, *params = inputs\n",
    "    predictions = mlp_forward(x, params)\n",
    "    loss = mean_squared_error(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sin_dataset(batch_size: int = 256) -> tuple[nb.Array, nb.Array]:\n",
    "    \"\"\"Create the COMPLEX 8-period sin dataset.\"\"\"\n",
    "    x = nb.rand((batch_size, 1), lower=0.0, upper=1.0, dtype=nb.DType.float32).to(\n",
    "        device\n",
    "    )\n",
    "    targets = nb.sin(SIN_PERIODS * 2.0 * np.pi * x) / 2.0 + 0.5\n",
    "    return x, targets\n",
    "\n",
    "\n",
    "def initialize_for_complex_function(\n",
    "    layers: list[int], seed: int = 42\n",
    ") -> list[nb.Array]:\n",
    "    \"\"\"Initialize specifically for learning complex high-frequency functions.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    params = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        fan_in, fan_out = layers[i], layers[i + 1]\n",
    "        w = nb.he_normal((fan_in, fan_out), seed=seed).to(device)\n",
    "        b = nb.zeros((fan_out,)).to(device)\n",
    "        params.append(w)\n",
    "        params.append(b)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step(\n",
    "    params: list[nb.Array],\n",
    "    gradients: list[nb.Array],\n",
    "    m_states: list[nb.Array],\n",
    "    v_states: list[nb.Array],\n",
    "    step: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    beta1: float = 0.9,\n",
    "    beta2: float = 0.999,\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> tuple[list[nb.Array], list[nb.Array], list[nb.Array]]:\n",
    "    \"\"\"AdamW optimizer step with weight decay - OPTIMIZED to match JAX efficiency.\"\"\"\n",
    "    updated_params = []\n",
    "    updated_m = []\n",
    "    updated_v = []\n",
    "\n",
    "    for param, grad, m, v in zip(params, gradients, m_states, v_states, strict=False):\n",
    "        # Update moments\n",
    "        new_m = beta1 * m + (1.0 - beta1) * grad\n",
    "        new_v = beta2 * v + (1.0 - beta2) * (grad * grad)\n",
    "\n",
    "        # Bias correction\n",
    "        bias_correction1 = 1.0 - beta1**step\n",
    "        bias_correction2 = 1.0 - beta2**step\n",
    "\n",
    "        # Corrected moments\n",
    "        m_corrected = new_m / bias_correction1\n",
    "        v_corrected = new_v / bias_correction2\n",
    "\n",
    "        # Parameter update with weight decay\n",
    "        new_param = param - learning_rate * (\n",
    "            m_corrected / (v_corrected**0.5 + eps) + weight_decay * param\n",
    "        )\n",
    "\n",
    "        # Append updated values\n",
    "        updated_params.append(new_param)\n",
    "        updated_m.append(new_m)\n",
    "        updated_v.append(new_v)\n",
    "\n",
    "    return updated_params, updated_m, updated_v\n",
    "\n",
    "\n",
    "def init_adamw_state(params: list[nb.Array]) -> tuple[list[nb.Array], list[nb.Array]]:\n",
    "    \"\"\"Initialize AdamW state - optimized version.\"\"\"\n",
    "    m_states = []\n",
    "    v_states = []\n",
    "    for param in params:\n",
    "        # Use zeros_like for more efficient initialization\n",
    "        m_np = np.zeros_like(param.to_numpy())\n",
    "        v_np = np.zeros_like(param.to_numpy())\n",
    "        m_states.append(nb.Array.from_numpy(m_np).to(device))\n",
    "        v_states.append(nb.Array.from_numpy(v_np).to(device))\n",
    "    return m_states, v_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(\n",
    "    epoch: int,\n",
    "    initial_lr: float = 0.001,\n",
    "    decay_factor: float = 0.95,\n",
    "    decay_every: int = 1000,\n",
    ") -> float:\n",
    "    \"\"\"Learning rate schedule for complex function learning.\"\"\"\n",
    "    return initial_lr * (decay_factor ** (epoch // decay_every))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(show_graph=False)\n",
    "def train_step(\n",
    "    x: nb.Array,\n",
    "    targets: nb.Array,\n",
    "    params: list[nb.Array],\n",
    "    m_states: list[nb.Array],\n",
    "    v_states: list[nb.Array],\n",
    "    step: int,\n",
    "    learning_rate: float,\n",
    ") -> tuple[list[nb.Array], list[nb.Array], list[nb.Array], nb.Array]:\n",
    "    \"\"\"JIT-compiled training step combining gradient computation and optimizer update.\"\"\"\n",
    "\n",
    "    # Define loss function that takes separate arguments (JAX style)\n",
    "    def loss_fn(*inner_params):\n",
    "        predictions = mlp_forward(x, inner_params)\n",
    "        loss = mean_squared_error(predictions, targets)\n",
    "        return loss\n",
    "\n",
    "    loss_value, param_gradients = nb.value_and_grad(\n",
    "        loss_fn, argnums=list(range(len(params)))\n",
    "    )(*params)\n",
    "\n",
    "    # AdamW optimizer update\n",
    "    updated_params, updated_m, updated_v = adamw_step(\n",
    "        params, param_gradients, m_states, v_states, step, learning_rate\n",
    "    )\n",
    "\n",
    "    return updated_params, updated_m, updated_v, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit\n",
    "def compute_predictions_and_loss(\n",
    "    x_test: nb.Array, targets_test: nb.Array, params: list[nb.Array]\n",
    ") -> tuple[nb.Array, nb.Array]:\n",
    "    \"\"\"JIT-compiled function to compute predictions and loss.\"\"\"\n",
    "    predictions_test = mlp_forward(x_test, params)\n",
    "    test_loss = mean_squared_error(predictions_test, targets_test)\n",
    "    return predictions_test, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Learning COMPLEX 8-Period Sin Function with Nabla JIT ===\n",
      "Architecture: [1, 64, 128, 256, 128, 64, 1]\n",
      "Initial learning rate: 0.001\n",
      "Sin periods: 8\n",
      "Batch size: 4\n",
      "Initial loss: 2.015263\n",
      "Initial predictions range: [-1.115, -0.850]\n",
      "Targets range: [0.008, 0.887]\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def test_nabla_complex_sin():\n",
    "    \"\"\"Test Nabla implementation with JIT for complex sin learning.\"\"\"\n",
    "    print(\"=== Learning COMPLEX 8-Period Sin Function with Nabla JIT ===\")\n",
    "    print(f\"Architecture: {LAYERS}\")\n",
    "    print(f\"Initial learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Sin periods: {SIN_PERIODS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "    # Initialize for complex function learning\n",
    "    params = initialize_for_complex_function(LAYERS)\n",
    "    m_states, v_states = init_adamw_state(params)\n",
    "\n",
    "    # Initial analysis\n",
    "    x_init, targets_init = create_sin_dataset(BATCH_SIZE)\n",
    "    predictions_init = mlp_forward(x_init, params)\n",
    "    initial_loss = mean_squared_error(predictions_init, targets_init)\n",
    "\n",
    "    pred_init_np = predictions_init.to_numpy()\n",
    "    target_init_np = targets_init.to_numpy()\n",
    "\n",
    "    print(f\"Initial loss: {initial_loss.to_numpy().item():.6f}\")\n",
    "    print(\n",
    "        f\"Initial predictions range: [{pred_init_np.min():.3f}, {pred_init_np.max():.3f}]\"\n",
    "    )\n",
    "    print(f\"Targets range: [{target_init_np.min():.3f}, {target_init_np.max():.3f}]\")\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    # Training loop\n",
    "    avg_loss = 0.0\n",
    "    avg_time = 0.0\n",
    "    avg_data_time = 0.0\n",
    "    avg_vjp_time = 0.0\n",
    "    avg_adamw_time = 0.0\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Learning rate schedule\n",
    "        current_lr = learning_rate_schedule(epoch, LEARNING_RATE)\n",
    "\n",
    "        # Create fresh batch\n",
    "        data_start = time.time()\n",
    "        x, targets = create_sin_dataset(BATCH_SIZE)\n",
    "        data_time = time.time() - data_start\n",
    "\n",
    "        # Training step using JIT-compiled function\n",
    "        vjp_start = time.time()\n",
    "\n",
    "        # Use JIT-compiled training step (combines gradient computation and optimizer update)\n",
    "        updated_params, updated_m, updated_v, loss_values = train_step(\n",
    "            x, targets, params, m_states, v_states, epoch, current_lr\n",
    "        )\n",
    "\n",
    "        vjp_time = time.time() - vjp_start\n",
    "\n",
    "        # Update return values (no separate AdamW step needed)\n",
    "        params, m_states, v_states = updated_params, updated_m, updated_v\n",
    "        adamw_time = 0.0  # Already included in the JIT step\n",
    "\n",
    "        # Loss extraction and conversion\n",
    "        loss_value = loss_values.to_numpy().item()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        avg_loss += loss_value\n",
    "        avg_time += epoch_time\n",
    "        avg_data_time += data_time\n",
    "        avg_vjp_time += vjp_time\n",
    "        avg_adamw_time += adamw_time\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Loss: {avg_loss / PRINT_INTERVAL:.6f} | Time: {avg_time / PRINT_INTERVAL:.4f}s\"\n",
    "            )\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(\n",
    "                f\"  â”œâ”€ Data Gen:   {avg_data_time / PRINT_INTERVAL:.4f}s ({avg_data_time / avg_time * 100:.1f}%)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  â””â”€ JIT Step:   {avg_vjp_time / PRINT_INTERVAL:.4f}s ({avg_vjp_time / avg_time * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "            avg_loss = 0.0\n",
    "            avg_time = 0.0\n",
    "            avg_data_time = 0.0\n",
    "            avg_vjp_time = 0.0\n",
    "            avg_adamw_time = 0.0\n",
    "\n",
    "    print(\"\\nNabla JIT training completed!\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    x_test_np = np.linspace(0, 1, 1000).reshape(-1, 1).astype(np.float32)\n",
    "    targets_test_np = (\n",
    "        np.sin(SIN_PERIODS * 2.0 * np.pi * x_test_np) / 2.0 + 0.5\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    x_test = nb.Array.from_numpy(x_test_np).to(device)\n",
    "    targets_test = nb.Array.from_numpy(targets_test_np).to(device)\n",
    "\n",
    "    # Use JIT-compiled function for evaluation\n",
    "    predictions_test, test_loss = compute_predictions_and_loss(\n",
    "        x_test, targets_test, params\n",
    "    )\n",
    "\n",
    "    pred_final_np = predictions_test.to_numpy()\n",
    "\n",
    "    final_test_loss = test_loss.to_numpy().item()\n",
    "\n",
    "    print(f\"Final test loss: {final_test_loss:.6f}\")\n",
    "    print(\n",
    "        f\"Final predictions range: [{pred_final_np.min():.3f}, {pred_final_np.max():.3f}]\"\n",
    "    )\n",
    "    print(f\"Target range: [{targets_test_np.min():.3f}, {targets_test_np.max():.3f}]\")\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(pred_final_np.flatten(), targets_test_np.flatten())[0, 1]\n",
    "    print(f\"Prediction-target correlation: {correlation:.4f}\")\n",
    "\n",
    "    return final_test_loss, correlation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_loss, correlation = test_nabla_complex_sin()\n",
    "    print(\"\\n=== Nabla JIT Summary ===\")\n",
    "    print(f\"Final test loss: {final_loss:.6f}\")\n",
    "    print(f\"Correlation with true function: {correlation:.4f}\")\n",
    "\n",
    "    if correlation > 0.95:\n",
    "        print(\"SUCCESS: Nabla JIT learned the complex function very well! ðŸŽ‰\")\n",
    "    elif correlation > 0.8:\n",
    "        print(\"GOOD: Nabla JIT learned the general shape well! ðŸ‘\")\n",
    "    elif correlation > 0.5:\n",
    "        print(\"PARTIAL: Some learning but needs improvement ðŸ¤”\")\n",
    "    else:\n",
    "        print(\"POOR: Nabla JIT failed to learn the complex function ðŸ˜ž\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Installation**: Setting up Nabla with GPU support in Google Colab.\n",
    "2. **Device Setup**: Understanding and using `to(device)` for GPU acceleration.\n",
    "3. **Training Loop**: Implementing a neural network to learn a complex sin function with Nabla's jitting for GPU acceleration.\n",
    "\n",
    "By following this tutorial, you should now have a good understanding of how to use Nabla for GPU-accelerated deep learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
