{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ce03ce",
   "metadata": {},
   "source": [
    "# Tutorial 4: Transforms and `@nb.compile`\n",
    "\n",
    "Nabla's transforms are **higher-order functions** that take a function and\n",
    "return a new function with modified behavior. They are fully composable\n",
    "and work with any Nabla operation, including nn.Modules.\n",
    "\n",
    "| Transform | What it does |\n",
    "|-----------|-------------|\n",
    "| `vmap` | Auto-vectorize over a batch dimension |\n",
    "| `grad` | Compute gradients (reverse-mode) |\n",
    "| `jacrev` | Full Jacobian via reverse-mode |\n",
    "| `jacfwd` | Full Jacobian via forward-mode |\n",
    "| `compile` | Compile computation graph to MAX graph |"
   ]
  },
  {
   "cell_type": "code",
   "id": "bea20f5e",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla Transforms & Compile Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bccb6",
   "metadata": {},
   "source": [
    "## 1. `vmap` — Automatic Vectorization\n",
    "\n",
    "`vmap` transforms a function that operates on a single example into one\n",
    "that operates on a batch — without writing any batching logic yourself."
   ]
  },
  {
   "cell_type": "code",
   "id": "2458d9c9",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_dot(x, y):\n",
    "    \"\"\"Dot product of two vectors (no batch dimension).\"\"\"\n",
    "    return nb.reduce_sum(x * y)\n",
    "\n",
    "# Without vmap: manual loop\n",
    "x_batch = nb.uniform((5, 3))\n",
    "y_batch = nb.uniform((5, 3))\n",
    "\n",
    "# With vmap: automatic vectorization!\n",
    "batched_dot = nb.vmap(single_dot, in_axes=(0, 0))\n",
    "result = batched_dot(x_batch, y_batch)\n",
    "print(f\"Batched dot products (5 pairs of 3D vectors):\")\n",
    "print(result)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60a2a8",
   "metadata": {},
   "source": [
    "### `in_axes` and `out_axes`\n",
    "\n",
    "`in_axes` controls which axis of each argument is the batch axis.\n",
    "`out_axes` controls where to place the batch axis in the output.\n",
    "Use `None` for arguments that should be broadcast (not batched)."
   ]
  },
  {
   "cell_type": "code",
   "id": "96769bb2",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, w):\n",
    "    \"\"\"Weighted sum: w * x, summed.\"\"\"\n",
    "    return nb.reduce_sum(w * x)\n",
    "\n",
    "# x is batched (axis 0), w is shared across the batch\n",
    "batch_fn = nb.vmap(weighted_sum, in_axes=(0, None))\n",
    "\n",
    "x_batch = nb.uniform((4, 3))\n",
    "w = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "\n",
    "result = batch_fn(x_batch, w)\n",
    "print(f\"Batched weighted sum (shared weights):\")\n",
    "print(result)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bae22e",
   "metadata": {},
   "source": [
    "## 2. `vmap` of `grad` — Per-Example Gradients\n",
    "\n",
    "Composing `vmap` with `grad` gives per-example gradients — something that's\n",
    "difficult to do efficiently in most frameworks."
   ]
  },
  {
   "cell_type": "code",
   "id": "8821f440",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_loss(x, w):\n",
    "    \"\"\"Loss for a single sample: (w @ x)^2.\"\"\"\n",
    "    return nb.reduce_sum(w * x) ** 2\n",
    "\n",
    "# grad of the loss w.r.t. w for a single sample\n",
    "grad_single = nb.grad(per_sample_loss, argnums=1)\n",
    "\n",
    "# vmap over samples — per-example gradients!\n",
    "per_example_grad = nb.vmap(grad_single, in_axes=(0, None))\n",
    "\n",
    "x_batch = nb.Tensor.from_dlpack(\n",
    "    np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=np.float32)\n",
    ")\n",
    "w = nb.Tensor.from_dlpack(np.array([2.0, 3.0], dtype=np.float32))\n",
    "\n",
    "grads = per_example_grad(x_batch, w)\n",
    "print(\"Per-example gradients (3 samples, 2 weights):\")\n",
    "print(grads)\n",
    "print(f\"Shape: {grads.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413bcf1",
   "metadata": {},
   "source": [
    "## 3. `jacrev` and `jacfwd` — Full Jacobians\n",
    "\n",
    "Recall from Tutorial 2: `jacrev` and `jacfwd` compute full Jacobian matrices.\n",
    "Here we show them applied to a more interesting function."
   ]
  },
  {
   "cell_type": "code",
   "id": "70bc3cbb",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_layer(x):\n",
    "    \"\"\"A simple neural network layer: tanh(xW + b).\"\"\"\n",
    "    W = nb.Tensor.from_dlpack(\n",
    "        np.array([[1.0, 0.3, -0.2], [-0.5, 0.8, 0.6]], dtype=np.float32)\n",
    "    )\n",
    "    b = nb.Tensor.from_dlpack(np.array([0.1, -0.1, 0.2], dtype=np.float32))\n",
    "    return nb.tanh(x @ W + b)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 0.5], dtype=np.float32))\n",
    "\n",
    "J_rev = nb.jacrev(neural_layer)(x)\n",
    "J_fwd = nb.jacfwd(neural_layer)(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {neural_layer(x).shape}\")\n",
    "print(f\"\\nJacobian via jacrev (shape {J_rev.shape}):\")\n",
    "print(J_rev)\n",
    "print(f\"\\nJacobian via jacfwd (shape {J_fwd.shape}):\")\n",
    "print(J_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c77c0",
   "metadata": {},
   "source": [
    "## 4. Composing Jacobians — Hessians\n",
    "\n",
    "Since transforms compose, we can compute Hessians by nesting:"
   ]
  },
  {
   "cell_type": "code",
   "id": "10de31c4",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(x):\n",
    "    \"\"\"Energy function: E(x) = 0.5 * x^T A x where A = [[2, 1], [1, 3]].\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[2.0, 1.0], [1.0, 3.0]], dtype=np.float32)\n",
    "    )\n",
    "    return 0.5 * nb.reduce_sum(x * (A @ x))\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0], dtype=np.float32))\n",
    "print(f\"E(x) = 0.5 * x^T @ A @ x, where A = [[2,1],[1,3]]\")\n",
    "print(f\"E([1,2]) = {energy(x)}\")\n",
    "print(f\"Gradient: {nb.grad(energy)(x)}\")\n",
    "print(f\"  (should be Ax = [4, 7])\")\n",
    "\n",
    "H = nb.jacfwd(nb.grad(energy))(x)\n",
    "print(f\"\\nHessian (should be A = [[2,1],[1,3]]):\")\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6b3cd",
   "metadata": {},
   "source": [
    "## 5. `@nb.compile` — Graph Compilation\n",
    "\n",
    "`@nb.compile` traces a function, captures its computation graph, and\n",
    "compiles it into an optimized MAX graph. Subsequent calls with the same\n",
    "tensor shapes/dtypes hit a cache — dramatically speeding up execution."
   ]
  },
  {
   "cell_type": "code",
   "id": "b8af1dc1",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_fn(x, y):\n",
    "    \"\"\"A function with many operations.\"\"\"\n",
    "    for _ in range(5):\n",
    "        x = nb.relu(x @ y + x)\n",
    "    return nb.reduce_sum(x)\n",
    "\n",
    "@nb.compile\n",
    "def fast_fn(x, y):\n",
    "    \"\"\"Same function, but compiled.\"\"\"\n",
    "    for _ in range(5):\n",
    "        x = nb.relu(x @ y + x)\n",
    "    return nb.reduce_sum(x)\n",
    "\n",
    "x = nb.uniform((32, 32))\n",
    "y = nb.uniform((32, 32))\n",
    "\n",
    "# Warmup compiled version (first call traces and compiles)\n",
    "_ = fast_fn(x, y)\n",
    "\n",
    "# Benchmark eager\n",
    "start = time.perf_counter()\n",
    "for _ in range(20):\n",
    "    _ = slow_fn(x, y)\n",
    "eager_time = time.perf_counter() - start\n",
    "\n",
    "# Benchmark compiled\n",
    "start = time.perf_counter()\n",
    "for _ in range(20):\n",
    "    _ = fast_fn(x, y)\n",
    "compiled_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Eager:    {eager_time:.4f}s\")\n",
    "print(f\"Compiled: {compiled_time:.4f}s\")\n",
    "print(f\"Speedup:  {eager_time / max(compiled_time, 1e-9):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9150c5",
   "metadata": {},
   "source": [
    "## 6. Compiled Training Loop\n",
    "\n",
    "The real power of `@nb.compile` is compiling entire training steps.\n",
    "When used with `value_and_grad` and `adamw_update`, the forward pass,\n",
    "backward pass, and optimizer step are all fused into a single compiled graph."
   ]
  },
  {
   "cell_type": "code",
   "id": "501e2b84",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyMLP(nb.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nb.nn.Linear(4, 16)\n",
    "        self.fc2 = nb.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(nb.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "def my_loss_fn(model, x, y):\n",
    "    return nb.nn.functional.mse_loss(model(x), y)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def train_step(model, opt_state, x, y):\n",
    "    \"\"\"Compiled training step: forward + backward + optimizer update.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(my_loss_fn, argnums=0)(model, x, y)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "# Setup\n",
    "np.random.seed(0)\n",
    "X = nb.Tensor.from_dlpack(np.random.randn(100, 4).astype(np.float32))\n",
    "y = nb.Tensor.from_dlpack(np.random.randn(100, 1).astype(np.float32))\n",
    "\n",
    "model = TinyMLP()\n",
    "opt_state = nb.nn.optim.adamw_init(model)\n",
    "\n",
    "print(f\"\\nCompiled training loop:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(50):\n",
    "    model, opt_state, loss = train_step(model, opt_state, X, y)\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725d16d",
   "metadata": {},
   "source": [
    "## 7. Compiled Training with JAX-Style Params\n",
    "\n",
    "`@nb.compile` works equally well with dict-based parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "96a32c53",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nabla.nn.functional import xavier_normal\n",
    "\n",
    "\n",
    "def init_params():\n",
    "    params = {\n",
    "        \"w1\": xavier_normal((4, 16)),\n",
    "        \"b1\": nb.zeros((1, 16)),\n",
    "        \"w2\": xavier_normal((16, 1)),\n",
    "        \"b2\": nb.zeros((1, 1)),\n",
    "    }\n",
    "    for p in params.values():\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward(params, x):\n",
    "    h = nb.relu(x @ params[\"w1\"] + params[\"b1\"])\n",
    "    return h @ params[\"w2\"] + params[\"b2\"]\n",
    "\n",
    "\n",
    "def jax_loss_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    diff = pred - y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def jax_train_step(params, opt_state, x, y):\n",
    "    loss, grads = nb.value_and_grad(jax_loss_fn, argnums=0)(params, x, y)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "params = init_params()\n",
    "opt_state = nb.nn.optim.adamw_init(params)\n",
    "\n",
    "print(f\"\\nCompiled JAX-style training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(50):\n",
    "    params, opt_state, loss = jax_train_step(params, opt_state, X, y)\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc38d00",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Transform | Usage | Key benefit |\n",
    "|-----------|-------|------------|\n",
    "| `vmap(f)` | Auto-batch any function | No manual batching |\n",
    "| `vmap(grad(f))` | Per-example gradients | Efficient |\n",
    "| `jacrev(f)` / `jacfwd(f)` | Full Jacobians | Compose for Hessians |\n",
    "| `@nb.compile` | Compile train step | 5–50x speedup |\n",
    "\n",
    "All transforms compose freely with each other:\n",
    "`compile(vmap(grad(f)))`, `jacfwd(jacrev(f))`, etc.\n",
    "\n",
    "**Next:** [05a_transformer_pytorch](05a_transformer_pytorch)\n",
    "— Building and training a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "id": "e90f6d6f",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Tutorial 04 completed!\")"
   ]
  }
 ]
}
