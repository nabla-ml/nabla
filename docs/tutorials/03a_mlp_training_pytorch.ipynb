{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3a: MLP Training (PyTorch-Style)\n",
    "\n",
    "Nabla provides a **PyTorch-style** `nn.Module` API for building and training\n",
    "neural networks. Models are defined as classes with `forward()` methods,\n",
    "parameters are automatically tracked, and training uses functional transforms\n",
    "(`value_and_grad`) combined with the AdamW optimizer.\n",
    "\n",
    "This tutorial trains a 2-layer MLP on a synthetic regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla MLP Training — PyTorch-style\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla MLP Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Model\n",
    "\n",
    "Subclass `nb.nn.Module` and define layers in `__init__`. The `forward()`\n",
    "method specifies the computation. Parameters (from `nb.nn.Linear`, etc.)\n",
    "are automatically registered and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "  fc1: Linear([Dim(4), Dim(32)])\n",
      "  fc2: Linear([Dim(32), Dim(1)])\n",
      "  Total parameters: 193\n"
     ]
    }
   ],
   "source": [
    "class MLP(nb.nn.Module):\n",
    "    \"\"\"Two-layer MLP with ReLU activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nb.nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nb.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nb.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MLP(4, 32, 1)\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  fc1: Linear({model.fc1.weight.shape})\")\n",
    "print(f\"  fc2: Linear({model.fc2.weight.shape})\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Data\n",
    "\n",
    "We'll create a regression dataset: predict `y = sin(x0) + cos(x1) + 0.5*x2 - x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: X [Dim(200), Dim(4)], y [Dim(200), Dim(1)]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_np = np.random.randn(n_samples, 4).astype(np.float32)\n",
    "y_np = (\n",
    "    np.sin(X_np[:, 0])\n",
    "    + np.cos(X_np[:, 1])\n",
    "    + 0.5 * X_np[:, 2]\n",
    "    - X_np[:, 3]\n",
    ").reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "print(f\"Dataset: X {X.shape}, y {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Loss Function\n",
    "\n",
    "The loss function takes the model as the first argument (so we can use\n",
    "`argnums=0` to differentiate w.r.t. model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: Tensor(2.5328 : f32[])\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(model, X, y):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "    predictions = model(X)\n",
    "    return nb.nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "# Test it\n",
    "initial_loss = loss_fn(model, X, y)\n",
    "print(f\"Initial loss: {initial_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the Optimizer\n",
    "\n",
    "Nabla provides a **functional optimizer API** that works seamlessly with\n",
    "`value_and_grad`. The optimizer state is a pytree (dict of tensors), and\n",
    "updates return new model + new state — no mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer state keys: ['m', 'v', 'step']\n"
     ]
    }
   ],
   "source": [
    "opt_state = nb.nn.optim.adamw_init(model)\n",
    "print(f\"Optimizer state keys: {list(opt_state.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Each step:\n",
    "1. `value_and_grad` computes the loss and gradients w.r.t. the model\n",
    "2. `adamw_update` returns a new model and optimizer state with parameters updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch    Loss        \n",
      "----------------------\n",
      "10       0.625402    \n",
      "20       0.242037    \n",
      "30       0.159796    \n",
      "40       0.118573    \n",
      "50       0.082741    \n",
      "60       0.064866    \n",
      "70       0.054502    \n",
      "80       0.047352    \n",
      "90       0.041529    \n",
      "100      0.037468    \n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, X, y)\n",
    "\n",
    "    # Update model parameters\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=learning_rate\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Let's see how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final loss: Tensor(0.0371 : f32[])\n",
      "\n",
      "Sample predictions vs targets:\n",
      "Prediction     Target        \n",
      "----------------------------\n",
      "0.1029         0.2678        \n",
      "0.7215         0.7629        \n",
      "0.7124         0.6380        \n",
      "-0.3226        -0.3964       \n",
      "1.2237         1.0610        \n"
     ]
    }
   ],
   "source": [
    "final_loss = loss_fn(model, X, y)\n",
    "print(f\"\\nFinal loss: {final_loss}\")\n",
    "\n",
    "# Compare predictions vs targets on a few samples\n",
    "predictions = model(X)\n",
    "print(f\"\\nSample predictions vs targets:\")\n",
    "print(f\"{'Prediction':<14} {'Target':<14}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(5):\n",
    "    pred_i = nb.gather(predictions, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    true_i = nb.gather(y, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    print(f\"{pred_i.item():<14.4f} {true_i.item():<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using the Stateful Optimizer (Alternative)\n",
    "\n",
    "Nabla also supports a stateful optimizer API closer to PyTorch's style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stateful AdamW training:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       0.625402    \n",
      "20       0.242037    \n",
      "30       0.159796    \n",
      "40       0.118573    \n",
      "50       0.082741    \n"
     ]
    }
   ],
   "source": [
    "# Reset model\n",
    "model2 = MLP(4, 32, 1)\n",
    "optimizer = nb.nn.optim.AdamW(model2, lr=1e-2)\n",
    "\n",
    "print(f\"\\nStateful AdamW training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(50):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model2, X, y)\n",
    "\n",
    "    # Stateful update — mutates the optimizer's internal state\n",
    "    model2 = optimizer.step(grads)\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | API |\n",
    "|---------|-----|\n",
    "| Define models | `class MyModel(nb.nn.Module)` |\n",
    "| Linear layer | `nb.nn.Linear(in_dim, out_dim)` |\n",
    "| Loss functions | `nb.nn.functional.mse_loss`, `cross_entropy_loss` |\n",
    "| Gradients | `nb.value_and_grad(loss_fn, argnums=0)(model, ...)` |\n",
    "| Optimizer init | `opt_state = nb.nn.optim.adamw_init(model)` |\n",
    "| Optimizer step | `model, opt_state = nb.nn.optim.adamw_update(...)` |\n",
    "\n",
    "**Next:** [03b_mlp_training_jax.py](03b_mlp_training_jax.py) — the same\n",
    "MLP trained in a purely functional (JAX-like) style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Tutorial 03a completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Tutorial 03a completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
