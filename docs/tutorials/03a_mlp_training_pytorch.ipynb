{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fc0089",
   "metadata": {},
   "source": [
    "# Tutorial 3a: MLP Training (PyTorch-Style)\n",
    "\n",
    "Nabla provides a **PyTorch-style** `nn.Module` API for building and training\n",
    "neural networks. Models are defined as classes with `forward()` methods,\n",
    "parameters are automatically tracked, and training uses functional transforms\n",
    "(`value_and_grad`) combined with the AdamW optimizer.\n",
    "\n",
    "This tutorial trains a 2-layer MLP on a synthetic regression task."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ab06cd8",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla MLP Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56c81d",
   "metadata": {},
   "source": [
    "## 1. Define the Model\n",
    "\n",
    "Subclass `nb.nn.Module` and define layers in `__init__`. The `forward()`\n",
    "method specifies the computation. Parameters (from `nb.nn.Linear`, etc.)\n",
    "are automatically registered and tracked."
   ]
  },
  {
   "cell_type": "code",
   "id": "84c6c7a0",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nb.nn.Module):\n",
    "    \"\"\"Two-layer MLP with ReLU activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nb.nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nb.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nb.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MLP(4, 32, 1)\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  fc1: Linear({model.fc1.weight.shape})\")\n",
    "print(f\"  fc2: Linear({model.fc2.weight.shape})\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b9352",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Data\n",
    "\n",
    "We'll create a regression dataset: predict `y = sin(x0) + cos(x1) + 0.5*x2 - x3`."
   ]
  },
  {
   "cell_type": "code",
   "id": "12c8b101",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_np = np.random.randn(n_samples, 4).astype(np.float32)\n",
    "y_np = (\n",
    "    np.sin(X_np[:, 0])\n",
    "    + np.cos(X_np[:, 1])\n",
    "    + 0.5 * X_np[:, 2]\n",
    "    - X_np[:, 3]\n",
    ").reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "print(f\"Dataset: X {X.shape}, y {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970f395",
   "metadata": {},
   "source": [
    "## 3. Define the Loss Function\n",
    "\n",
    "The loss function takes the model as the first argument (so we can use\n",
    "`argnums=0` to differentiate w.r.t. model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c4a27d0",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X, y):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "    predictions = model(X)\n",
    "    return nb.nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "# Test it\n",
    "initial_loss = loss_fn(model, X, y)\n",
    "print(f\"Initial loss: {initial_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694e05f",
   "metadata": {},
   "source": [
    "## 4. Initialize the Optimizer\n",
    "\n",
    "Nabla provides a **functional optimizer API** that works seamlessly with\n",
    "`value_and_grad`. The optimizer state is a pytree (dict of tensors), and\n",
    "updates return new model + new state — no mutation."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f87b042",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = nb.nn.optim.adamw_init(model)\n",
    "print(f\"Optimizer state keys: {list(opt_state.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078298b5",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Each step:\n",
    "1. `value_and_grad` computes the loss and gradients w.r.t. the model\n",
    "2. `adamw_update` returns a new model and optimizer state with parameters updated"
   ]
  },
  {
   "cell_type": "code",
   "id": "efa0635e",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model, X, y)\n",
    "\n",
    "    # Update model parameters\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=learning_rate\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985347f",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Let's see how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "d8fdc7be",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = loss_fn(model, X, y)\n",
    "print(f\"\\nFinal loss: {final_loss}\")\n",
    "\n",
    "# Compare predictions vs targets on a few samples\n",
    "predictions = model(X)\n",
    "print(f\"\\nSample predictions vs targets:\")\n",
    "print(f\"{'Prediction':<14} {'Target':<14}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(5):\n",
    "    pred_i = nb.gather(predictions, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    true_i = nb.gather(y, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    print(f\"{pred_i.item():<14.4f} {true_i.item():<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d6306",
   "metadata": {},
   "source": [
    "## 7. Using the Stateful Optimizer (Alternative)\n",
    "\n",
    "Nabla also supports a stateful optimizer API closer to PyTorch's style."
   ]
  },
  {
   "cell_type": "code",
   "id": "491f1acf",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model2 = MLP(4, 32, 1)\n",
    "optimizer = nb.nn.optim.AdamW(model2, lr=1e-2)\n",
    "\n",
    "print(f\"\\nStateful AdamW training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(50):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(model2, X, y)\n",
    "\n",
    "    # Stateful update — mutates the optimizer's internal state\n",
    "    model2 = optimizer.step(grads)\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca96aa1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | API |\n",
    "|---------|-----|\n",
    "| Define models | `class MyModel(nb.nn.Module)` |\n",
    "| Linear layer | `nb.nn.Linear(in_dim, out_dim)` |\n",
    "| Loss functions | `nb.nn.functional.mse_loss`, `cross_entropy_loss` |\n",
    "| Gradients | `nb.value_and_grad(loss_fn, argnums=0)(model, ...)` |\n",
    "| Optimizer init | `opt_state = nb.nn.optim.adamw_init(model)` |\n",
    "| Optimizer step | `model, opt_state = nb.nn.optim.adamw_update(...)` |\n",
    "\n",
    "**Next:** [03b_mlp_training_jax](03b_mlp_training_jax) — the same\n",
    "MLP trained in a purely functional (JAX-like) style."
   ]
  },
  {
   "cell_type": "code",
   "id": "93045baa",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Tutorial 03a completed!\")"
   ]
  }
 ]
}
