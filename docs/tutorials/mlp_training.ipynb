{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Training (GPU)\n",
    "\n",
    "In this tutorial, we'll walk through how to use Nabla with GPU acceleration to train a neural network to learn a complex sin function. We'll cover installation, device setup, and the training loop with jitting for GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ All libraries loaded successfully! Python 3.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import nabla as nb\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    packages = [\"numpy\", \"nabla-ml\"]\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + packages, check=True)\n",
    "    import numpy as np\n",
    "    import nabla as nb\n",
    "\n",
    "print(\n",
    "    f\"ðŸŽ‰ All libraries loaded successfully! Python {sys.version_info.major}.{sys.version_info.minor}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Nabla and GPU Acceleration\n",
    "\n",
    "Nabla is a deep learning library that leverages the Modular MLIR compiler for high-performance computation. One of its key features is GPU acceleration, which is achieved through jitting (Just-In-Time compilation). This means that functions decorated with `@nb.jit` are compiled into optimized GPU code by the Modular compiler.\n",
    "\n",
    "### Why `to(device)`?\n",
    "\n",
    "In Nabla, tensors need to be explicitly moved to the desired device (CPU or GPU) using the `to(device)` method. This is because Nabla's GPU mode is only accessible within jitted functions. The device is determined at runtime, and tensors must be on the correct device for operations to execute efficiently.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Jitting**: Functions decorated with `@nb.jit` are compiled and optimized for GPU execution.\n",
    "2. **Device Placement**: Tensors must be moved to the appropriate device using `to(device)`.\n",
    "3. **Training Loop**: The training loop involves creating datasets, computing gradients, and updating parameters using an optimizer.\n",
    "\n",
    "Let's dive into the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device(type=gpu,id=0)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Play around with the hyperparameters! GPU acceleration is particularly useful for larger settings.\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = [1, 64, 128, 256, 128, 64, 1]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "PRINT_INTERVAL = 100\n",
    "SIN_PERIODS = 8\n",
    "\n",
    "# Play around with the devices!\n",
    "device = nb.cpu() if nb.accelerator_count() == 0 else nb.accelerator()\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(x: nb.Array, params: list[nb.Array]) -> nb.Array:\n",
    "    \"\"\"MLP forward pass through all layers.\"\"\"\n",
    "    output = x\n",
    "    for i in range(0, len(params) - 1, 2):\n",
    "        w, b = params[i], params[i + 1]\n",
    "        output = nb.matmul(output, w) + b\n",
    "        # Apply ReLU to all layers except the last\n",
    "        if i < len(params) - 2:\n",
    "            output = nb.relu(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def mean_squared_error(predictions: nb.Array, targets: nb.Array) -> nb.Array:\n",
    "    \"\"\"Compute mean squared error loss.\"\"\"\n",
    "    diff = predictions - targets\n",
    "    squared_errors = diff * diff\n",
    "    batch_size = nb.array(predictions.shape[0], dtype=nb.DType.float32).to(device)\n",
    "    loss = nb.sum(squared_errors) / batch_size\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mlp_forward_and_loss(inputs: list[nb.Array]) -> nb.Array:\n",
    "    \"\"\"Combined forward pass and loss computation for VJP with leaky ReLU.\"\"\"\n",
    "    x, targets, *params = inputs\n",
    "    predictions = mlp_forward(x, params)\n",
    "    loss = mean_squared_error(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sin_dataset(batch_size: int = 256) -> tuple[nb.Array, nb.Array]:\n",
    "    \"\"\"Create the 8-Period sin dataset.\"\"\"\n",
    "    x = nb.rand((batch_size, 1), lower=0.0, upper=1.0, dtype=nb.DType.float32).to(\n",
    "        device\n",
    "    )\n",
    "    targets = nb.sin(SIN_PERIODS * 2.0 * np.pi * x) / 2.0 + 0.5\n",
    "    return x, targets\n",
    "\n",
    "\n",
    "def initialize_for_complex_function(\n",
    "    layers: list[int], seed: int = 42\n",
    ") -> list[nb.Array]:\n",
    "    \"\"\"Initialize specifically for learning complex high-frequency functions.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    params = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        fan_in, fan_out = layers[i], layers[i + 1]\n",
    "        w = nb.he_normal((fan_in, fan_out), seed=seed).to(device)\n",
    "        b = nb.zeros((fan_out,)).to(device)\n",
    "        params.append(w)\n",
    "        params.append(b)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step(\n",
    "    params: list[nb.Array],\n",
    "    gradients: list[nb.Array],\n",
    "    m_states: list[nb.Array],\n",
    "    v_states: list[nb.Array],\n",
    "    step: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    beta1: float = 0.9,\n",
    "    beta2: float = 0.999,\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> tuple[list[nb.Array], list[nb.Array], list[nb.Array]]:\n",
    "    \"\"\"AdamW optimizer step with weight decay - OPTIMIZED to match JAX efficiency.\"\"\"\n",
    "    updated_params = []\n",
    "    updated_m = []\n",
    "    updated_v = []\n",
    "\n",
    "    for param, grad, m, v in zip(params, gradients, m_states, v_states, strict=False):\n",
    "        # Update moments\n",
    "        new_m = beta1 * m + (1.0 - beta1) * grad\n",
    "        new_v = beta2 * v + (1.0 - beta2) * (grad * grad)\n",
    "\n",
    "        # Bias correction\n",
    "        bias_correction1 = 1.0 - beta1**step\n",
    "        bias_correction2 = 1.0 - beta2**step\n",
    "\n",
    "        # Corrected moments\n",
    "        m_corrected = new_m / bias_correction1\n",
    "        v_corrected = new_v / bias_correction2\n",
    "\n",
    "        # Parameter update with weight decay\n",
    "        new_param = param - learning_rate * (\n",
    "            m_corrected / (v_corrected**0.5 + eps) + weight_decay * param\n",
    "        )\n",
    "\n",
    "        # Append updated values\n",
    "        updated_params.append(new_param)\n",
    "        updated_m.append(new_m)\n",
    "        updated_v.append(new_v)\n",
    "\n",
    "    return updated_params, updated_m, updated_v\n",
    "\n",
    "\n",
    "def init_adamw_state(params: list[nb.Array]) -> tuple[list[nb.Array], list[nb.Array]]:\n",
    "    \"\"\"Initialize AdamW state - optimized version.\"\"\"\n",
    "    m_states = []\n",
    "    v_states = []\n",
    "    for param in params:\n",
    "        # Use zeros_like for more efficient initialization\n",
    "        m_np = np.zeros_like(param.to_numpy())\n",
    "        v_np = np.zeros_like(param.to_numpy())\n",
    "        m_states.append(nb.Array.from_numpy(m_np).to(device))\n",
    "        v_states.append(nb.Array.from_numpy(v_np).to(device))\n",
    "    return m_states, v_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(\n",
    "    epoch: int,\n",
    "    initial_lr: float = 0.001,\n",
    "    decay_factor: float = 0.95,\n",
    "    decay_every: int = 1000,\n",
    ") -> float:\n",
    "    \"\"\"Learning rate schedule for complex function learning.\"\"\"\n",
    "    return initial_lr * (decay_factor ** (epoch // decay_every))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(show_graph=False)\n",
    "def train_step(\n",
    "    x: nb.Array,\n",
    "    targets: nb.Array,\n",
    "    params: list[nb.Array],\n",
    "    m_states: list[nb.Array],\n",
    "    v_states: list[nb.Array],\n",
    "    step: int,\n",
    "    learning_rate: float,\n",
    ") -> tuple[list[nb.Array], list[nb.Array], list[nb.Array], nb.Array]:\n",
    "    \"\"\"JIT-compiled training step combining gradient computation and optimizer update.\"\"\"\n",
    "\n",
    "    # Define loss function that takes separate arguments (JAX style)\n",
    "    def loss_fn(*inner_params):\n",
    "        predictions = mlp_forward(x, inner_params)\n",
    "        loss = mean_squared_error(predictions, targets)\n",
    "        return loss\n",
    "\n",
    "    loss_value, param_gradients = nb.value_and_grad(\n",
    "        loss_fn, argnums=list(range(len(params)))\n",
    "    )(*params)\n",
    "\n",
    "    # AdamW optimizer update\n",
    "    updated_params, updated_m, updated_v = adamw_step(\n",
    "        params, param_gradients, m_states, v_states, step, learning_rate\n",
    "    )\n",
    "\n",
    "    return updated_params, updated_m, updated_v, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit\n",
    "def compute_predictions_and_loss(\n",
    "    x_test: nb.Array, targets_test: nb.Array, params: list[nb.Array]\n",
    ") -> tuple[nb.Array, nb.Array]:\n",
    "    \"\"\"JIT-compiled function to compute predictions and loss.\"\"\"\n",
    "    predictions_test = mlp_forward(x_test, params)\n",
    "    test_loss = mean_squared_error(predictions_test, targets_test)\n",
    "    return predictions_test, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Learning COMPLEX 8-Period Sin Function with Nabla JIT ===\n",
      "Architecture: [1, 64, 128, 256, 128, 64, 1]\n",
      "Initial learning rate: 0.001\n",
      "Sin periods: 8\n",
      "Batch size: 128\n",
      "Initial loss: 1.839083\n",
      "Initial predictions range: [-1.542, -0.007]\n",
      "Targets range: [0.000, 1.000]\n",
      "\n",
      "Starting training...\n",
      "\n",
      "============================================================\n",
      "Epoch 100 | Loss: 0.163495 | Time: 0.0025s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (39.4%)\n",
      "  â””â”€ JIT Step:   0.0015s (59.5%)\n",
      "\n",
      "============================================================\n",
      "Epoch 200 | Loss: 0.107982 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.4%)\n",
      "  â””â”€ JIT Step:   0.0010s (50.4%)\n",
      "\n",
      "============================================================\n",
      "Epoch 300 | Loss: 0.105557 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.5%)\n",
      "  â””â”€ JIT Step:   0.0011s (50.3%)\n",
      "\n",
      "============================================================\n",
      "Epoch 400 | Loss: 0.104932 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.3%)\n",
      "  â””â”€ JIT Step:   0.0010s (50.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 500 | Loss: 0.104369 | Time: 0.0020s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.0%)\n",
      "  â””â”€ JIT Step:   0.0010s (50.8%)\n",
      "\n",
      "============================================================\n",
      "Epoch 600 | Loss: 0.102601 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.3%)\n",
      "  â””â”€ JIT Step:   0.0010s (50.5%)\n",
      "\n",
      "============================================================\n",
      "Epoch 700 | Loss: 0.098580 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (48.2%)\n",
      "  â””â”€ JIT Step:   0.0010s (50.6%)\n",
      "\n",
      "============================================================\n",
      "Epoch 800 | Loss: 0.093815 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (47.8%)\n",
      "  â””â”€ JIT Step:   0.0011s (51.0%)\n",
      "\n",
      "============================================================\n",
      "Epoch 900 | Loss: 0.088344 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (47.7%)\n",
      "  â””â”€ JIT Step:   0.0011s (51.1%)\n",
      "\n",
      "============================================================\n",
      "Epoch 1000 | Loss: 0.083684 | Time: 0.0021s\n",
      "============================================================\n",
      "  â”œâ”€ Data Gen:   0.0010s (47.9%)\n",
      "  â””â”€ JIT Step:   0.0011s (50.8%)\n",
      "\n",
      "Nabla JIT training completed!\n",
      "\n",
      "=== Final Evaluation ===\n",
      "Final test loss: 0.092112\n",
      "Final predictions range: [0.001, 1.010]\n",
      "Target range: [0.000, 1.000]\n",
      "Prediction-target correlation: 0.5211\n",
      "\n",
      "=== Nabla JIT Summary ===\n",
      "Final test loss: 0.092112\n",
      "Correlation with true function: 0.5211\n",
      "PARTIAL: Some learning but needs improvement ðŸ¤”\n"
     ]
    }
   ],
   "source": [
    "def test_nabla_complex_sin():\n",
    "    \"\"\"Test Nabla implementation with JIT for complex sin learning.\"\"\"\n",
    "    print(\"=== Learning 8-Period Sin Function with Nabla JIT ===\")\n",
    "    print(f\"Architecture: {LAYERS}\")\n",
    "    print(f\"Initial learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Sin periods: {SIN_PERIODS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "    # Initialize for complex function learning\n",
    "    params = initialize_for_complex_function(LAYERS)\n",
    "    m_states, v_states = init_adamw_state(params)\n",
    "\n",
    "    # Initial analysis\n",
    "    x_init, targets_init = create_sin_dataset(BATCH_SIZE)\n",
    "    predictions_init = mlp_forward(x_init, params)\n",
    "    initial_loss = mean_squared_error(predictions_init, targets_init)\n",
    "\n",
    "    pred_init_np = predictions_init.to_numpy()\n",
    "    target_init_np = targets_init.to_numpy()\n",
    "\n",
    "    print(f\"Initial loss: {initial_loss.to_numpy().item():.6f}\")\n",
    "    print(\n",
    "        f\"Initial predictions range: [{pred_init_np.min():.3f}, {pred_init_np.max():.3f}]\"\n",
    "    )\n",
    "    print(f\"Targets range: [{target_init_np.min():.3f}, {target_init_np.max():.3f}]\")\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    # Training loop\n",
    "    avg_loss = 0.0\n",
    "    avg_time = 0.0\n",
    "    avg_data_time = 0.0\n",
    "    avg_vjp_time = 0.0\n",
    "    avg_adamw_time = 0.0\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Learning rate schedule\n",
    "        current_lr = learning_rate_schedule(epoch, LEARNING_RATE)\n",
    "\n",
    "        # Create fresh batch\n",
    "        data_start = time.time()\n",
    "        x, targets = create_sin_dataset(BATCH_SIZE)\n",
    "        data_time = time.time() - data_start\n",
    "\n",
    "        # Training step using JIT-compiled function\n",
    "        vjp_start = time.time()\n",
    "\n",
    "        # Use JIT-compiled training step (combines gradient computation and optimizer update)\n",
    "        updated_params, updated_m, updated_v, loss_values = train_step(\n",
    "            x, targets, params, m_states, v_states, epoch, current_lr\n",
    "        )\n",
    "\n",
    "        vjp_time = time.time() - vjp_start\n",
    "\n",
    "        # Update return values (no separate AdamW step needed)\n",
    "        params, m_states, v_states = updated_params, updated_m, updated_v\n",
    "        adamw_time = 0.0  # Already included in the JIT step\n",
    "\n",
    "        # Loss extraction and conversion\n",
    "        loss_value = loss_values.to_numpy().item()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        avg_loss += loss_value\n",
    "        avg_time += epoch_time\n",
    "        avg_data_time += data_time\n",
    "        avg_vjp_time += vjp_time\n",
    "        avg_adamw_time += adamw_time\n",
    "\n",
    "        # check if the device of all update _params and loss_value are acutally equal to the device\n",
    "        if not all(p.device == device for p in updated_params):\n",
    "            raise ValueError(\n",
    "                \"Updated parameters are not on the expected device. \"\n",
    "                f\"Expected: {device}, Got: {[p.device for p in updated_params]}\"\n",
    "            )\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d} | Loss: {avg_loss / PRINT_INTERVAL:.6f} | Time: {avg_time / PRINT_INTERVAL:.4f}s\"\n",
    "            )\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(\n",
    "                f\"  â”œâ”€ Data Gen:   {avg_data_time / PRINT_INTERVAL:.4f}s ({avg_data_time / avg_time * 100:.1f}%)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  â””â”€ JIT Step:   {avg_vjp_time / PRINT_INTERVAL:.4f}s ({avg_vjp_time / avg_time * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "            avg_loss = 0.0\n",
    "            avg_time = 0.0\n",
    "            avg_data_time = 0.0\n",
    "            avg_vjp_time = 0.0\n",
    "            avg_adamw_time = 0.0\n",
    "\n",
    "    print(\"\\nNabla JIT training completed!\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    x_test_np = np.linspace(0, 1, 1000).reshape(-1, 1).astype(np.float32)\n",
    "    targets_test_np = (\n",
    "        np.sin(SIN_PERIODS * 2.0 * np.pi * x_test_np) / 2.0 + 0.5\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    x_test = nb.Array.from_numpy(x_test_np).to(device)\n",
    "    targets_test = nb.Array.from_numpy(targets_test_np).to(device)\n",
    "\n",
    "    # Use JIT-compiled function for evaluation\n",
    "    predictions_test, test_loss = compute_predictions_and_loss(\n",
    "        x_test, targets_test, params\n",
    "    )\n",
    "\n",
    "    pred_final_np = predictions_test.to_numpy()\n",
    "\n",
    "    final_test_loss = test_loss.to_numpy().item()\n",
    "\n",
    "    print(f\"Final test loss: {final_test_loss:.6f}\")\n",
    "    print(\n",
    "        f\"Final predictions range: [{pred_final_np.min():.3f}, {pred_final_np.max():.3f}]\"\n",
    "    )\n",
    "    print(f\"Target range: [{targets_test_np.min():.3f}, {targets_test_np.max():.3f}]\")\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(pred_final_np.flatten(), targets_test_np.flatten())[0, 1]\n",
    "    print(f\"Prediction-target correlation: {correlation:.4f}\")\n",
    "\n",
    "    return final_test_loss, correlation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_loss, correlation = test_nabla_complex_sin()\n",
    "    print(\"\\n=== Nabla JIT Summary ===\")\n",
    "    print(f\"Final test loss: {final_loss:.6f}\")\n",
    "    print(f\"Correlation with true function: {correlation:.4f}\")\n",
    "\n",
    "    if correlation > 0.95:\n",
    "        print(\"SUCCESS: Nabla JIT learned the complex function very well! ðŸŽ‰\")\n",
    "    elif correlation > 0.8:\n",
    "        print(\"GOOD: Nabla JIT learned the general shape well! ðŸ‘\")\n",
    "    elif correlation > 0.5:\n",
    "        print(\"PARTIAL: Some learning but needs improvement ðŸ¤”\")\n",
    "    else:\n",
    "        print(\"POOR: Nabla JIT failed to learn the complex function ðŸ˜ž\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
