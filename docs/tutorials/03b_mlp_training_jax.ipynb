{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a0c427",
   "metadata": {},
   "source": [
    "# Tutorial 3b: MLP Training (JAX-Style / Functional)\n",
    "\n",
    "In this style, the model is a **pure function** that takes parameters\n",
    "explicitly. Parameters are stored in nested dicts (pytrees). This is\n",
    "the same approach used by JAX and Flax.\n",
    "\n",
    "This tutorial trains the same 2-layer MLP from Tutorial 3a, but purely\n",
    "functionally."
   ]
  },
  {
   "cell_type": "code",
   "id": "20cf31b1",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "from nabla.nn.functional import xavier_normal\n",
    "\n",
    "print(\"Nabla MLP Training — JAX-style (functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9b935",
   "metadata": {},
   "source": [
    "## 1. Initialize Parameters\n",
    "\n",
    "Instead of a class, we create a nested dict of parameter tensors.\n",
    "Each tensor gets `requires_grad=True` so autodiff can track through it."
   ]
  },
  {
   "cell_type": "code",
   "id": "02514342",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_params(in_dim: int, hidden_dim: int, out_dim: int) -> dict:\n",
    "    \"\"\"Initialize MLP parameters as a pytree (nested dict).\"\"\"\n",
    "    params = {\n",
    "        \"fc1\": {\n",
    "            \"weight\": xavier_normal((in_dim, hidden_dim)),\n",
    "            \"bias\": nb.zeros((1, hidden_dim)),\n",
    "        },\n",
    "        \"fc2\": {\n",
    "            \"weight\": xavier_normal((hidden_dim, out_dim)),\n",
    "            \"bias\": nb.zeros((1, out_dim)),\n",
    "        },\n",
    "    }\n",
    "    # Mark all params as differentiable\n",
    "    for layer in params.values():\n",
    "        for p in layer.values():\n",
    "            p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "\n",
    "params = init_mlp_params(4, 32, 1)\n",
    "print(\"Parameter shapes:\")\n",
    "for name, layer in params.items():\n",
    "    for pname, p in layer.items():\n",
    "        print(f\"  {name}.{pname}: {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad23010",
   "metadata": {},
   "source": [
    "## 2. Define the Forward Pass\n",
    "\n",
    "The model is a pure function: it takes parameters and input, returns output.\n",
    "No side effects, no mutation."
   ]
  },
  {
   "cell_type": "code",
   "id": "573f78ac",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(params: dict, x):\n",
    "    \"\"\"Pure-function MLP forward pass.\"\"\"\n",
    "    x = x @ params[\"fc1\"][\"weight\"] + params[\"fc1\"][\"bias\"]\n",
    "    x = nb.relu(x)\n",
    "    x = x @ params[\"fc2\"][\"weight\"] + params[\"fc2\"][\"bias\"]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Quick test\n",
    "x_test = nb.uniform((3, 4))\n",
    "y_test = mlp_forward(params, x_test)\n",
    "print(f\"Forward pass test: input {x_test.shape} → output {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a4b74",
   "metadata": {},
   "source": [
    "## 3. Create Data & Define Loss\n",
    "\n",
    "Same synthetic dataset as Tutorial 3a: `y = sin(x0) + cos(x1) + 0.5*x2 - x3`."
   ]
  },
  {
   "cell_type": "code",
   "id": "0f17d314",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_np = np.random.randn(n_samples, 4).astype(np.float32)\n",
    "y_np = (\n",
    "    np.sin(X_np[:, 0])\n",
    "    + np.cos(X_np[:, 1])\n",
    "    + 0.5 * X_np[:, 2]\n",
    "    - X_np[:, 3]\n",
    ").reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "print(f\"Dataset: X {X.shape}, y {y.shape}\")\n",
    "\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    \"\"\"MSE loss as a pure function of params.\"\"\"\n",
    "    predictions = mlp_forward(params, X)\n",
    "    diff = predictions - y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "initial_loss = loss_fn(params, X, y)\n",
    "print(f\"Initial loss: {initial_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e9fd9",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "The key insight: `value_and_grad(loss_fn, argnums=0)` differentiates w.r.t.\n",
    "the first argument (`params`), which is a dict. It returns gradients with\n",
    "the **same pytree structure** as `params`."
   ]
  },
  {
   "cell_type": "code",
   "id": "30acae38",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = nb.nn.optim.adamw_init(params)\n",
    "lr = 1e-2\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, X, y)\n",
    "\n",
    "    # grads has the same structure as params:\n",
    "    # grads[\"fc1\"][\"weight\"], grads[\"fc1\"][\"bias\"], etc.\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=lr\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c78e3f",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "2512b898",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = loss_fn(params, X, y)\n",
    "print(f\"\\nFinal loss: {final_loss}\")\n",
    "\n",
    "predictions = mlp_forward(params, X)\n",
    "print(f\"\\nSample predictions vs targets:\")\n",
    "print(f\"{'Prediction':<14} {'Target':<14}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(5):\n",
    "    pred_i = nb.gather(predictions, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    true_i = nb.gather(y, nb.constant(np.array([i], dtype=np.int64)), axis=0)\n",
    "    print(f\"{pred_i.item():<14.4f} {true_i.item():<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2f4bd",
   "metadata": {},
   "source": [
    "## 6. Manual SGD (No Optimizer)\n",
    "\n",
    "The functional style makes it trivial to implement gradient descent manually\n",
    "using `tree_map`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d9441c1",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sgd = init_mlp_params(4, 32, 1)\n",
    "sgd_lr = 0.05\n",
    "\n",
    "print(f\"\\nManual SGD training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "for step in range(100):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params_sgd, X, y)\n",
    "\n",
    "    # Manual SGD: params = params - lr * grads\n",
    "    params_sgd = nb.tree_map(\n",
    "        lambda p, g: p - sgd_lr * g, params_sgd, grads\n",
    "    )\n",
    "\n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b3ac6",
   "metadata": {},
   "source": [
    "## PyTorch-Style vs JAX-Style: Comparison\n",
    "\n",
    "| Aspect | PyTorch-style (03a) | JAX-style (03b) |\n",
    "|--------|-------------------|-----------------|\n",
    "| Model | `class MLP(nn.Module)` | `def mlp_forward(params, x)` |\n",
    "| Params | Auto-tracked by Module | Explicit dict (pytree) |\n",
    "| State | Mutable attributes | Immutable, returned from functions |\n",
    "| Optimizer | Can be stateful or functional | Typically functional |\n",
    "| `@nb.compile` | Works with both | Works with both |\n",
    "\n",
    "Both styles are fully supported in Nabla. Choose the one that fits your\n",
    "mental model!\n",
    "\n",
    "**Next:** [04_transforms_and_compile](04_transforms_and_compile)\n",
    "— Advanced transforms (vmap, jacrev, jacfwd) and `@nb.compile`."
   ]
  },
  {
   "cell_type": "code",
   "id": "5ea0b534",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Tutorial 03b completed!\")"
   ]
  }
 ]
}
