# Activation Modules

## `ReLU`

```python
class ReLU() -> 'None':
```
Base class for imperative neural-network modules.

Modules are registered as pytree nodes, enabling direct use with transforms.


---
## `GELU`

```python
class GELU() -> 'None':
```
Base class for imperative neural-network modules.

Modules are registered as pytree nodes, enabling direct use with transforms.


---
## `Sigmoid`

```python
class Sigmoid() -> 'None':
```
Base class for imperative neural-network modules.

Modules are registered as pytree nodes, enabling direct use with transforms.


---
## `Tanh`

```python
class Tanh() -> 'None':
```
Base class for imperative neural-network modules.

Modules are registered as pytree nodes, enabling direct use with transforms.


---
## `SiLU`

```python
class SiLU() -> 'None':
```
Base class for imperative neural-network modules.

Modules are registered as pytree nodes, enabling direct use with transforms.


---
## `Softmax`

```python
class Softmax(axis: 'int' = -1) -> 'None':
```
Apply softmax along a given axis (default: last).


---
