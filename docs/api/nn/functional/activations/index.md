# Functional Activations

## `relu`

```python
def relu(x: nabla.core.tensor.Tensor) -> nabla.core.tensor.Tensor:
```

---
## `leaky_relu`

```python
def leaky_relu(x: nabla.core.tensor.Tensor, negative_slope: float = 0.01) -> nabla.core.tensor.Tensor:
```

---
## `sigmoid`

```python
def sigmoid(x: nabla.core.tensor.Tensor) -> nabla.core.tensor.Tensor:
```

---
## `tanh`

```python
def tanh(x: nabla.core.tensor.Tensor) -> nabla.core.tensor.Tensor:
```

---
## `softmax`

```python
def softmax(x: nabla.core.tensor.Tensor, axis: int = -1) -> nabla.core.tensor.Tensor:
```

---
## `log_softmax`

```python
def log_softmax(x: nabla.core.tensor.Tensor, axis: int = -1) -> nabla.core.tensor.Tensor:
```

---
## `gelu`

```python
def gelu(x: nabla.core.tensor.Tensor) -> nabla.core.tensor.Tensor:
```
Gaussian Error Linear Unit activation function.


---
