# relu

## Signature

```python
nabla.nn.relu(x: nabla.core.tensor.Tensor) -> nabla.core.tensor.Tensor
```

**Source**: `nabla.nn.functional.layers.activations`

Rectified Linear Unit activation function.

Args:
    x: Input tensor

Returns:
    Tensor with ReLU applied element-wise

