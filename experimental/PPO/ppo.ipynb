{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fa997f49",
            "metadata": {},
            "source": [
                "# Deep Dive: The Computational Graph of Pure JAX PPO\n",
                "\n",
                "This notebook combines high-level intuition, a concrete running example, and the exact mathematical derivation of the gradients with a modular JAX implementation.\n",
                "\n",
                "## 1. The Computational Context: \"End-to-End\" Compilation\n",
                "Before looking at the math, we must understand the hardware execution, which dictates the code structure. In high-performance Reinforcement Learning, data transfer is often the slowest part.\n",
                "\n",
                "### The Bottleneck Problem\n",
                "* **Standard RL (Python/Gym):** The CPU simulates physics (Environment) $\\leftrightarrow$ GPU computes gradients (Agent). This requires moving data back and forth over the PCIe bus thousands of times per second. This is the \"PCIe Bottleneck.\"\n",
                "* **Pure JAX RL (This Code):** The Environment is rewritten as stateless vector math. The entire training loop—simulation, data collection, and backpropagation—is compiled into a single XLA kernel.\n",
                "\n",
                "**Mechanism:** `jax.lax.scan` acts as a compiled `for` loop inside the GPU. The data never leaves VRAM until training is finished. This allows us to train millions of steps in seconds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b2d4399",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import argparse\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax import jit, value_and_grad, vmap\n",
                "import numpy as np\n",
                "import imageio\n",
                "from PIL import Image, ImageDraw\n",
                "import warnings\n",
                "import logging\n",
                "from typing import NamedTuple, Any\n",
                "\n",
                "# Suppress warnings\n",
                "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*os.fork().*\")\n",
                "logging.getLogger(\"imageio_ffmpeg\").setLevel(logging.ERROR)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df3e02a9",
            "metadata": {},
            "source": [
                "### The Environment: Stateless Vector Math\n",
                "In JAX, we cannot have side effects or internal state objects. Therefore, the environment must be a **pure function**.\n",
                "\n",
                "$$ (S_{t+1}, R_t, D_t) = f(S_t, A_t, \\text{PhysicsParams}) $$\n",
                "\n",
                "**Key Characteristics:**\n",
                "1.  **Stateless:** The `step` function does not modify `self`. It takes the current state as input and returns the next state.\n",
                "2.  **Vectorized:** We don't need a `VectorEnv` wrapper. `jax.vmap` automatically vectorizes this single-environment logic across a batch of agents.\n",
                "3.  **Differentiable (Optional):** Because it's written in JAX, we *could* differentiate through the physics, though PPO (as a model-free algorithm) treats the environment as a black box."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bec1773",
            "metadata": {},
            "outputs": [],
            "source": [
                "class PureJaxCartPole:\n",
                "    def __init__(self):\n",
                "        self.gravity = 9.8\n",
                "        self.masscart = 1.0\n",
                "        self.masspole = 0.1\n",
                "        self.total_mass = (self.masscart + self.masspole)\n",
                "        self.length = 0.5 \n",
                "        self.polemass_length = (self.masspole * self.length)\n",
                "        self.force_mag = 10.0\n",
                "        self.tau = 0.02\n",
                "        self.kinematics_integrator = 'euler'\n",
                "        self.theta_threshold_radians = 12 * 2 * jnp.pi / 360\n",
                "        self.x_threshold = 2.4\n",
                "\n",
                "    def reset(self, key):\n",
                "        state = jax.random.uniform(key, shape=(4,), minval=-0.05, maxval=0.05)\n",
                "        return state\n",
                "\n",
                "    def step(self, state, action):\n",
                "        x, x_dot, theta, theta_dot = state\n",
                "        force = jax.lax.select(action == 1, self.force_mag, -self.force_mag)\n",
                "        costheta = jnp.cos(theta)\n",
                "        sintheta = jnp.sin(theta)\n",
                "\n",
                "        temp = (force + self.polemass_length * theta_dot**2 * sintheta) / self.total_mass\n",
                "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass))\n",
                "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
                "\n",
                "        if self.kinematics_integrator == 'euler':\n",
                "            x = x + self.tau * x_dot\n",
                "            x_dot = x_dot + self.tau * xacc\n",
                "            theta = theta + self.tau * theta_dot\n",
                "            theta_dot = theta_dot + self.tau * thetaacc\n",
                "        else:\n",
                "            x_dot = x_dot + self.tau * xacc\n",
                "            x = x + self.tau * x_dot\n",
                "            theta_dot = theta_dot + self.tau * thetaacc\n",
                "            theta = theta + self.tau * theta_dot\n",
                "\n",
                "        next_state = jnp.array([x, x_dot, theta, theta_dot])\n",
                "        done = (x < -self.x_threshold) | (x > self.x_threshold) | (theta < -self.theta_threshold_radians) | (theta > self.theta_threshold_radians)\n",
                "        reward = 1.0\n",
                "        return next_state, reward, done"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a519f197",
            "metadata": {},
            "source": [
                "### Helper: Visualization\n",
                "We use a custom PIL-based renderer to verify the agent's behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d1a3cee3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def render_cartpole(state, width=600, height=400):\n",
                "    x, _, theta, _ = state\n",
                "    world_width = 2.4 * 2\n",
                "    scale = width / world_width\n",
                "    carty = 250\n",
                "    polewidth, polelen = 10.0, scale * 1.0\n",
                "    cartwidth, cartheight = 50.0, 30.0\n",
                "\n",
                "    img = Image.new('RGB', (width, height), (255, 255, 255))\n",
                "    draw = ImageDraw.Draw(img)\n",
                "    draw.line([(0, carty), (width, carty)], fill=(0, 0, 0), width=1)\n",
                "\n",
                "    cartx = x * scale + width / 2.0\n",
                "    draw.rectangle([cartx - cartwidth / 2, carty - cartheight / 2, cartx + cartwidth / 2, carty + cartheight / 2], fill=(0, 0, 0))\n",
                "\n",
                "    rotation_angle = -theta\n",
                "    l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
                "    coords = []\n",
                "    for px, py in [(l, b), (l, t), (r, t), (r, b)]:\n",
                "        px_rot = px * np.cos(rotation_angle) - py * np.sin(rotation_angle)\n",
                "        py_rot = px * np.sin(rotation_angle) + py * np.cos(rotation_angle)\n",
                "        coords.append((cartx + px_rot, carty - py_rot))\n",
                "        \n",
                "    draw.polygon(coords, fill=(204, 153, 102))\n",
                "    draw.ellipse([cartx - 2, carty - 2, cartx + 2, carty + 2], fill=(127, 127, 255))\n",
                "    return np.array(img)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3513347b",
            "metadata": {},
            "source": [
                "### Models and Optimizer\n",
                "We define the Actor (Policy) and Critic (Value Function) as simple MLPs. \n",
                "\n",
                "**1. Orthogonal Initialization:**\n",
                "Standard in PPO. We initialize weights such that the eigenvalues of the Jacobian are close to 1. This preserves the gradient magnitude during the early stages of training, which is crucial for deep RL stability.\n",
                "\n",
                "**2. Manual Adam Implementation:**\n",
                "Why not `optax`? To keep this notebook self-contained and to demonstrate that an optimizer is just a state update function:\n",
                "$$ \\theta_{t+1} = \\text{Update}(\\theta_t, \\nabla L, \\text{OptState}_t) $$\n",
                "This allows us to easily `scan` over the optimization steps without external dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6ae6d00",
            "metadata": {},
            "outputs": [],
            "source": [
                "def orthogonal_init(key, shape, scale=1.0):\n",
                "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
                "    a = jax.random.normal(key, flat_shape)\n",
                "    u, _, vt = jnp.linalg.svd(a, full_matrices=False)\n",
                "    q = u if u.shape == flat_shape else vt\n",
                "    return scale * q.reshape(shape)\n",
                "\n",
                "def init_actor_critic(key, obs_dim, action_dim, hidden_dim=64):\n",
                "    keys = jax.random.split(key, 6);\n",
                "    def init_layer(k, i, o, s=1.0): \n",
                "        return {'w': orthogonal_init(k, (i, o), s), 'b': jnp.zeros((o,))}\n",
                "    return {\n",
                "        'actor': {\n",
                "            'l1': init_layer(keys[0], obs_dim, hidden_dim, np.sqrt(2)),\n",
                "            'l2': init_layer(keys[1], hidden_dim, hidden_dim, np.sqrt(2)),\n",
                "            'head': init_layer(keys[2], hidden_dim, action_dim, 0.01)\n",
                "        },\n",
                "        'critic': {\n",
                "            'l1': init_layer(keys[3], obs_dim, hidden_dim, np.sqrt(2)),\n",
                "            'l2': init_layer(keys[4], hidden_dim, hidden_dim, np.sqrt(2)),\n",
                "            'head': init_layer(keys[5], hidden_dim, 1, 1.0)\n",
                "        }\n",
                "    }\n",
                "\n",
                "def forward_mlp(params, x):\n",
                "    x = jax.nn.tanh(x @ params['l1']['w'] + params['l1']['b'])\n",
                "    x = jax.nn.tanh(x @ params['l2']['w'] + params['l2']['b'])\n",
                "    return x @ params['head']['w'] + params['head']['b']\n",
                "\n",
                "def get_action_logits(params, obs): return forward_mlp(params['actor'], obs)\n",
                "def get_value(params, obs): return forward_mlp(params['critic'], obs).squeeze(-1)\n",
                "\n",
                "def adam_update(grads, opt_state, params, lr, max_grad_norm=0.5):\n",
                "    step = opt_state['step'] + 1\n",
                "    # Gradient Clipping\n",
                "    leaves, _ = jax.tree_util.tree_flatten(grads)\n",
                "    total_norm = jnp.sqrt(sum(jnp.sum(g ** 2) for g in leaves))\n",
                "    grads = jax.tree.map(lambda g: g * jnp.minimum(max_grad_norm / (total_norm + 1e-6), 1.0), grads)\n",
                "    # Adam Update\n",
                "    m = jax.tree.map(lambda m, g: 0.9 * m + 0.1 * g, opt_state['m'], grads)\n",
                "    v = jax.tree.map(lambda v, g: 0.999 * v + 0.001 * (g ** 2), opt_state['v'], grads)\n",
                "    m_hat = jax.tree.map(lambda m: m / (1 - 0.9 ** step), m)\n",
                "    v_hat = jax.tree.map(lambda v: v / (1 - 0.999 ** step), v)\n",
                "    params = jax.tree.map(lambda p, m, v: p - lr * m / (jnp.sqrt(v) + 1e-8), params, m_hat, v_hat)\n",
                "    return params, {'m': m, 'v': v, 'step': step}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3fe2049c",
            "metadata": {},
            "source": [
                "## 2. Phase 1: The Rollout (Data Collection)\n",
                "\n",
                "**Code Context:** `rollout_fn`\n",
                "\n",
                "We run $N$ environments in parallel. The policy $\\pi_{\\theta}(a|s)$ interacts with the environment dynamics $P(s_{t+1}|s_t, a_t)$.\n",
                "\n",
                "### The Trajectory Buffer\n",
                "We collect a batch of data $D = \\{ (s_t, a_t, r_t, s_{t+1}, \\log \\pi(a_t|s_t), V(s_t)) \\}_{t=0}^{T}$.\n",
                "This data is stored in static GPU arrays. Because JAX requires fixed shapes, we must decide the trajectory length $T$ (e.g., 128 steps) beforehand.\n",
                "\n",
                "**The Trace: A Concrete Example**\n",
                "Let us track one environment (Env #42) over 4 timesteps.\n",
                "\n",
                "| Step (t) | State ($s_t$) | Critic $V(s_t)$ | Actor Logits $z$ | Softmax $\\pi(s_t)$ | Action $a_t$ | Reward $r_t$ | Done $d_t$ |\n",
                "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
                "| 0 | 0.05 | $1.0$ | [-0.5, 0.5] | $\\approx [0.27, 0.73]$ | 1 (Right) | +1.0 | 0 |\n",
                "| 1 | 0.10 | $1.5$ | [-0.1, 0.1] | $\\approx [0.45, 0.55]$ | 0 (Left) | +1.0 | 0 |\n",
                "| 2 | 0.25 | $0.8$ | [-1.0, 1.0] | $\\approx [0.12, 0.88]$ | 1 (Right) | +1.0 | 1 |\n",
                "| 3 | 0.00 | $0.0$ | [0.0, 0.0] | $\\approx [0.50, 0.50]$ | 1 (Right) | 0.0 | 0 |\n",
                "\n",
                "**Crucial Math Note:**\n",
                "During this phase, **no gradients are computed**. We simply populate fixed-size buffers (Tensors) with integers and floats. This is purely inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d96d2588",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_rollout_fn(env, num_steps, num_envs):\n",
                "    \"\"\"Creates the rollout function (Closure).\"\"\"\n",
                "    \n",
                "    def rollout_step(carry, unused):\n",
                "        # Unpack the carry state\n",
                "        env_states, episode_returns, params, key = carry\n",
                "        key, subkey = jax.random.split(key)\n",
                "        \n",
                "        # 1. Get Action from Policy\n",
                "        logits = get_action_logits(params, env_states)\n",
                "        action = jax.random.categorical(subkey, logits)\n",
                "        value = get_value(params, env_states)\n",
                "        \n",
                "        # 2. Store Log Probs for later gradient calculation\n",
                "        log_prob_all = jax.nn.log_softmax(logits)\n",
                "        action_log_prob = jnp.take_along_axis(log_prob_all, action[:, None], axis=1).squeeze(-1)\n",
                "        \n",
                "        # 3. Step the Environment (Vectorized)\n",
                "        next_env_states, reward, done = vmap(env.step)(env_states, action)\n",
                "        \n",
                "        # 4. Handle Auto-Reset (Stateless)\n",
                "        key, *reset_keys = jax.random.split(key, num_envs + 1)\n",
                "        reset_states = vmap(env.reset)(jnp.array(reset_keys))\n",
                "        next_env_states = jnp.where(done[:, None], reset_states, next_env_states)\n",
                "        \n",
                "        # Track metrics\n",
                "        episode_returns = episode_returns + reward\n",
                "        final_return = jnp.where(done, episode_returns, 0.0)\n",
                "        episode_returns = jnp.where(done, 0.0, episode_returns)\n",
                "        \n",
                "        # Pack data for trajectory buffer\n",
                "        transition = (env_states, action, action_log_prob, reward, done, value, final_return)\n",
                "        return (next_env_states, episode_returns, params, key), transition\n",
                "\n",
                "    return rollout_step"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b283f6d",
            "metadata": {},
            "source": [
                "## 3. Phase 2: Signal Processing (GAE)\n",
                "\n",
                "**Code Context:** `calculate_gae`\n",
                "\n",
                "We now calculate the targets for learning. We need to answer: **How good was each action compared to the average?**\n",
                "\n",
                "### A. The Critic's Target: Discounted Return ($R_t$)\n",
                "The Critic aims to predict the sum of future rewards. We calculate this recursively backwards (Dynamic Programming):\n",
                "\n",
                "$$R_t = r_t + \\gamma \\cdot R_{t+1} \\cdot (1 - d_t)$$\n",
                "\n",
                "### B. The Actor's Weight: GAE Advantage ($\\hat{A}_t$)\n",
                "The Generalized Advantage Estimate balances **Bias** (using the Critic's imperfect prediction) and **Variance** (using the noisy Monte-Carlo rewards).\n",
                "\n",
                "1.  **TD Error ($\\delta_t$):** The one-step surprise.\n",
                "    $$\\delta_t = r_t + \\gamma V(s_{t+1})(1-d_t) - V(s_t)$$\n",
                "\n",
                "2.  **GAE ($\\hat{A}_t$):** The exponentially weighted sum of TD errors.\n",
                "    $$\\hat{A}_t = \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\delta_{t+k} = \\delta_t + \\gamma \\lambda \\hat{A}_{t+1}$$\n",
                "\n",
                "**Trace Example:**\n",
                "*   **Step 2:** $\\delta_2 = 1.0 + 0.99(0) - 0.8 = \\mathbf{+0.2}$. (End of episode, exact truth).\n",
                "*   **Step 1:** $\\delta_1 = 1.0 + 0.99(0.8) - 1.5 = \\mathbf{+0.292}$.\n",
                "*   **Step 0:** $\\delta_0 = 1.0 + 0.99(1.5) - 1.0 = \\mathbf{+1.485}$. (Large positive surprise).\n",
                "\n",
                "Because $\\hat{A}_0 > 0$, the action taken at $t=0$ was *better* than expected, so we should encourage it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eade543a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_gae(params, next_env_states, traj_batch, args):\n",
                "    obs, actions, logprobs, rewards, dones, values, final_returns = traj_batch\n",
                "    \n",
                "    # Bootstrap with the value of the LAST state in the sequence\n",
                "    next_value = get_value(params, next_env_states)\n",
                "    \n",
                "    def gae_scan_fn(carry, t):\n",
                "        last_gae_lam, next_val = carry\n",
                "        \n",
                "        # TD-Error: delta = r + gamma * V(next) - V(current)\n",
                "        delta = rewards[t] + args['gamma'] * next_val * (1.0 - dones[t]) - values[t]\n",
                "        \n",
                "        # GAE: advantage = delta + gamma * lambda * last_advantage\n",
                "        last_gae_lam = delta + args['gamma'] * args['gae_lambda'] * (1.0 - dones[t]) * last_gae_lam\n",
                "        return (last_gae_lam, values[t]), last_gae_lam\n",
                "\n",
                "    _, advantages = jax.lax.scan(\n",
                "        gae_scan_fn, \n",
                "        (jnp.zeros_like(next_value), next_value), \n",
                "        jnp.arange(args['num_steps']), \n",
                "        reverse=True # <--- Crucial: We calculate backwards!\n",
                "    )\n",
                "    \n",
                "    returns = advantages + values\n",
                "    return advantages, returns"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5759a52",
            "metadata": {},
            "source": [
                "## 4. Phase 3: The Gradient (Backpropagation)\n",
                "\n",
                "**Code Context:** `update_ppo` (containing `loss_fn`)\n",
                "\n",
                "We define a scalar loss function $J(\\theta)$ and compute $\\nabla_\\theta J$ using automatic differentiation.\n",
                "\n",
                "$$J_{total} = L_{actor} + c_{vf} L_{critic} + c_{ent} L_{entropy}$$\n",
                "\n",
                "### Part A: The Actor (PPO Clipped Objective)\n",
                "We want to increase the probability of good actions, but **not too much** at once to avoid policy collapse.\n",
                "\n",
                "1.  **Probability Ratio:** $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{old}(a_t|s_t)}$. Initially 1.0.\n",
                "2.  **Clipped Objective:**\n",
                "    $$L^{CLIP} = \\min(r_t \\hat{A}_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)$$\n",
                "    If $\\hat{A}_t > 0$ (good action), we increase prob up to $1+\\epsilon$. If $\\hat{A}_t < 0$ (bad action), we decrease prob down to $1-\\epsilon$.\n",
                "\n",
                "### Part B: The Critic (Value Loss)\n",
                "Simple regression. We want the critic to accurately predict the returns we actually saw.\n",
                "$$L^{VF} = \\text{MSE}(V_\\theta(s_t), R_t)$$\n",
                "\n",
                "### Part C: Entropy Bonus\n",
                "We add a bonus for randomness to prevent premature convergence (the agent deciding on one action too early).\n",
                "$$L^{S} = \\text{Entropy}(\\pi_\\theta)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "946d0e02",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_update_fn(args):\n",
                "    \"\"\"Creates the PPO Update function.\"\"\"\n",
                "    \n",
                "    def loss_fn(params, batch):\n",
                "        obs, act, logp_old, adv, ret, val_old = batch\n",
                "        \n",
                "        # 1. Re-run network on recorded observations\n",
                "        new_logits = get_action_logits(params, obs)\n",
                "        new_values = get_value(params, obs)\n",
                "        \n",
                "        # 2. Probability Ratios\n",
                "        logp_new = jnp.take_along_axis(jax.nn.log_softmax(new_logits), act[:, None], axis=1).squeeze(-1)\n",
                "        ratio = jnp.exp(logp_new - logp_old)\n",
                "        \n",
                "        # 3. Actor Loss (Clipped)\n",
                "        loss_actor_unclipped = adv * ratio\n",
                "        loss_actor_clipped = adv * jnp.clip(ratio, 1 - args['clip_coef'], 1 + args['clip_coef'])\n",
                "        loss_actor = -jnp.minimum(loss_actor_unclipped, loss_actor_clipped).mean()\n",
                "        \n",
                "        # 4. Critic Loss (Clipped option for stability)\n",
                "        if args['clip_vloss']:\n",
                "            v_loss_unclipped = (new_values - ret) ** 2\n",
                "            v_clipped = val_old + jnp.clip(new_values - val_old, -args['clip_coef'], args['clip_coef'])\n",
                "            v_loss_clipped = (v_clipped - ret) ** 2\n",
                "            loss_critic = 0.5 * jnp.maximum(v_loss_unclipped, v_loss_clipped).mean()\n",
                "        else:\n",
                "            loss_critic = 0.5 * ((new_values - ret) ** 2).mean()\n",
                "            \n",
                "        # 5. Entropy Loss (Exploration Bonus)\n",
                "        entropy = -jnp.sum(jax.nn.softmax(new_logits) * jax.nn.log_softmax(new_logits), axis=1).mean()\n",
                "        loss_entropy = -args['ent_coef'] * entropy\n",
                "        \n",
                "        total_loss = loss_actor + args['vf_coef'] * loss_critic + loss_entropy\n",
                "        return total_loss, (loss_actor, loss_critic, entropy)\n",
                "\n",
                "    def update_minibatch(carry, batch):\n",
                "        params, opt_state = carry\n",
                "        (loss, metrics), grads = value_and_grad(loss_fn, has_aux=True)(params, batch)\n",
                "        params, opt_state = adam_update(grads, opt_state, params, args['lr'], args['max_grad_norm'])\n",
                "        return (params, opt_state), metrics\n",
                "\n",
                "    return update_minibatch"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "854d59c7",
            "metadata": {},
            "source": [
                "## 5. Composition: The `make_train` Factory\n",
                "\n",
                "This is the heart of JAX compilation. We define the **entire** training lifecycle as a nested function structure. \n",
                "\n",
                "**The Hierarchy of Loops:**\n",
                "\n",
                "1.  **`train_step` (The Outer Loop):** Represents one \"Update\".\n",
                "    *   **Input:** Current Params, Optimizer State, Environment State.\n",
                "    *   **Output:** New Params, Metrics.\n",
                "\n",
                "    *   **Step 1: Rollout (`jax.lax.scan`):**\n",
                "        *   Runs `env.step` $T$ times.\n",
                "        *   Collects a batch of trajectories.\n",
                "\n",
                "    *   **Step 2: GAE Calculation:**\n",
                "        *   Processes the trajectories to find Advantages.\n",
                "\n",
                "    *   **Step 3: Optimization (`jax.lax.scan` - Epochs):**\n",
                "        *   Iterates $K$ epochs over the data.\n",
                "        *   **Step 3a: Minibatches (`jax.lax.scan`):**\n",
                "            *   Slices the data into small batches.\n",
                "            *   Computes gradients and updates params.\n",
                "\n",
                "All of this is passed to `jax.jit` at the end, creating one massive, optimized GPU kernel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8ac6eb2",
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_train(args):\n",
                "    \"\"\"Combines Env, Policy, and Update into a single JIT-able function.\"\"\"\n",
                "    \n",
                "    env = PureJaxCartPole()\n",
                "    rollout_fn = create_rollout_fn(env, args['num_steps'], args['num_envs'])\n",
                "    update_fn = create_update_fn(args)\n",
                "    \n",
                "    batch_size = args['num_envs'] * args['num_steps']\n",
                "    minibatch_size = batch_size // args['num_minibatches']\n",
                "\n",
                "    def train_step(carry, unused):\n",
                "        params, opt_state, env_states, episode_returns, key = carry\n",
                "        \n",
                "        # --- Phase 1: Data Collection ---\n",
                "        (next_env_states, episode_returns, params, key), traj_batch = jax.lax.scan(\n",
                "            rollout_fn, (env_states, episode_returns, params, key), None, length=args['num_steps']\n",
                "        )\n",
                "        \n",
                "        # --- Phase 2: GAE ---\n",
                "        advantages, returns = calculate_gae(params, next_env_states, traj_batch, args)\n",
                "        \n",
                "        # Flatten and Normalize\n",
                "        obs, actions, logprobs, _, _, values, final_returns = traj_batch\n",
                "        flat_inds = lambda x: x.reshape(batch_size, -1) if x.ndim > 2 else x.reshape(batch_size)\n",
                "        \n",
                "        b_obs, b_act, b_logp, b_adv, b_ret, b_val = map(flat_inds, \n",
                "            (obs, actions, logprobs, advantages, returns, values)\n",
                "        )\n",
                "        \n",
                "        if args['norm_adv']:\n",
                "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
                "\n",
                "        # --- Phase 3: Update (Multiple Epochs) ---\n",
                "        def update_epoch(carry, unused):\n",
                "            params, opt_state, key = carry\n",
                "            key, subkey = jax.random.split(key)\n",
                "            \n",
                "            # Shuffle data\n",
                "            perm = jax.random.permutation(subkey, batch_size)\n",
                "            b_obs_s, b_act_s, b_logp_s, b_adv_s, b_ret_s, b_val_s = \\\n",
                "                jax.tree_util.tree_map(lambda x: x[perm], (b_obs, b_act, b_logp, b_adv, b_ret, b_val))\n",
                "            \n",
                "            # Iterate over minibatches\n",
                "            def get_batch(i): \n",
                "                return jax.tree_util.tree_map(\n",
                "                    lambda x: jax.lax.dynamic_slice_in_dim(x, i, minibatch_size), \n",
                "                    (b_obs_s, b_act_s, b_logp_s, b_adv_s, b_ret_s, b_val_s)\n",
                "                )\n",
                "            \n",
                "            def run_minibatch(carry, i):\n",
                "                return update_fn(carry, get_batch(i))\n",
                "\n",
                "            (params, opt_state), metrics = jax.lax.scan(\n",
                "                run_minibatch, (params, opt_state), jnp.arange(0, batch_size, minibatch_size)\n",
                "            )\n",
                "            return (params, opt_state, key), metrics\n",
                "\n",
                "        (params, opt_state, key), metrics = jax.lax.scan(\n",
                "            update_epoch, (params, opt_state, key), None, length=args['update_epochs']\n",
                "        )\n",
                "        \n",
                "        # Return packing\n",
                "        avg_return = final_returns.sum() / (final_returns > 0).sum().clip(1.0)\n",
                "        metrics = jax.tree.map(lambda x: x.mean(), metrics)\n",
                "        \n",
                "        return (params, opt_state, next_env_states, episode_returns, key), (metrics, avg_return)\n",
                "\n",
                "    return jit(train_step)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8eb04c6",
            "metadata": {},
            "source": [
                "## 6. Execution Loop\n",
                "\n",
                "We are ready to launch.\n",
                "\n",
                "1.  **Compilation:** The first call to `train_step` will trigger XLA compilation. This might take 10-30 seconds. JAX is unrolling the graphs and optimizing memory layout.\n",
                "2.  **Execution:** Subsequent calls are near-instant. The Python loop merely dispatches the command to the GPU.\n",
                "\n",
                "Watch the **SPS (Steps Per Second)**. In a pure JAX implementation, this number can reach into the millions on modern hardware, orders of magnitude faster than standard Python loops."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "803114af",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image as IPyImage, display\n",
                "\n",
                "def record_video(params, env, run_name, step):\n",
                "    frames = []\n",
                "    # Use a fresh key for video\n",
                "    key = jax.random.PRNGKey(0)\n",
                "    state = env.reset(key)\n",
                "    \n",
                "    for _ in range(500):\n",
                "        # Render\n",
                "        frame = render_cartpole(np.array(state))\n",
                "        frames.append(frame)\n",
                "        \n",
                "        # Step\n",
                "        logits = get_action_logits(params, state)\n",
                "        action = jnp.argmax(logits)\n",
                "        state, _, done = env.step(state, action)\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "    video_dir = f\"videos/{run_name}\"\n",
                "    os.makedirs(video_dir, exist_ok=True)\n",
                "    # Save as GIF for reliable autoplay in notebooks\n",
                "    video_path = f\"{video_dir}/step_{step}.gif\"\n",
                "    imageio.mimsave(video_path, frames, fps=50, loop=0)\n",
                "    return video_path\n",
                "\n",
                "config = {\n",
                "    'seed': 42,\n",
                "    'total_timesteps': 500000,\n",
                "    'num_envs': 64,\n",
                "    'num_steps': 128,\n",
                "    'lr': 2.5e-4,\n",
                "    'num_minibatches': 4,\n",
                "    'update_epochs': 4,\n",
                "    'gamma': 0.99,\n",
                "    'gae_lambda': 0.95,\n",
                "    'clip_coef': 0.2,\n",
                "    'ent_coef': 0.01,\n",
                "    'vf_coef': 0.5,\n",
                "    'max_grad_norm': 0.5,\n",
                "    'norm_adv': True,\n",
                "    'clip_vloss': True,\n",
                "    'capture_video': True,\n",
                "    'video_freq': 20,\n",
                "}\n",
                "\n",
                "print(\"Compiling...\")\n",
                "train_step = make_train(config)\n",
                "\n",
                "# Init State\n",
                "rng = jax.random.PRNGKey(config['seed'])\n",
                "rng, init_key = jax.random.split(rng)\n",
                "params = init_actor_critic(init_key, obs_dim=4, action_dim=2)\n",
                "opt_state = {'m': jax.tree.map(jnp.zeros_like, params), 'v': jax.tree.map(jnp.zeros_like, params), 'step': 0}\n",
                "rng, *env_keys = jax.random.split(rng, config['num_envs'] + 1)\n",
                "env_states = vmap(PureJaxCartPole().reset)(jnp.array(env_keys))\n",
                "episode_returns = jnp.zeros(config['num_envs'])\n",
                "\n",
                "runner_state = (params, opt_state, env_states, episode_returns, rng)\n",
                "batch_size = config['num_envs'] * config['num_steps']\n",
                "num_updates = config['total_timesteps'] // batch_size\n",
                "\n",
                "print(f\"Starting training for {num_updates} updates...\")\n",
                "start_time = time.perf_counter()\n",
                "run_name = f\"ppo_notebook_{int(time.time())}\"\n",
                "\n",
                "for i in range(num_updates):\n",
                "    runner_state, (metrics, avg_return) = train_step(runner_state, None)\n",
                "    \n",
                "    if i % 10 == 0:\n",
                "        sps = (i + 1) * batch_size / (time.perf_counter() - start_time)\n",
                "        print(f\"Update {i}: Return {avg_return:.2f} | SPS: {int(sps)} | Loss: {metrics[0].mean():.3f}\")\n",
                "        \n",
                "    if config['capture_video'] and i % config['video_freq'] == 0:\n",
                "        # Extract params from runner_state (it's the first element)\n",
                "        current_params = runner_state[0]\n",
                "        video_path = record_video(current_params, PureJaxCartPole(), run_name, i * batch_size)\n",
                "        print(f\"Displaying video for step {i * batch_size}:\")\n",
                "        display(IPyImage(filename=video_path))\n",
                "\n",
                "print(\"Done!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
