{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive: The Computational Graph of Pure JAX PPO\n",
    "\n",
    "**Objective:** Build a high-performance, single-file implementation of Proximal Policy Optimization (PPO) that runs entirely on the GPU.\n",
    "\n",
    "## 1. The Philosophy: \"End-to-End\" Compilation\n",
    "\n",
    "To understand this code, we must understand the hardware bottleneck in standard Deep RL.\n",
    "\n",
    "### The Standard approach (Python/Gym)\n",
    "In libraries like PyTorch + Gym, the training loop looks like a ping-pong match:\n",
    "1. **CPU:** Simulates the environment (physics, rules).\n",
    "2. **PCIe Bus:** Transfers observation data to GPU.\n",
    "3. **GPU:** Neural Network predicts action.\n",
    "4. **PCIe Bus:** Transfers action back to CPU.\n",
    "5. **CPU:** Steps the environment.\n",
    "\n",
    "This communication overhead often costs more time than the actual computation.\n",
    "\n",
    "### The Pure JAX approach\n",
    "We rewrite the **Environment itself** in JAX (vector math). This allows us to compile the Environment, the Agent, and the Optimizer into a **single XLA kernel**.\n",
    "\n",
    "* **Input:** A random seed.\n",
    "* **Output:** Trained parameters.\n",
    "* **Mechanism:** `jax.lax.scan` creates a compiled `for` loop that runs entirely inside VRAM. The data never leaves the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad, vmap\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw\n",
    "import warnings\n",
    "import logging\n",
    "from typing import NamedTuple, Any\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*os.fork().*\")\n",
    "logging.getLogger(\"imageio_ffmpeg\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Environment: Functional & Stateless\n",
    "\n",
    "In standard OOP (Object Oriented Programming), an environment has internal state (`self.state`). In JAX, we need **Functional Programming**.\n",
    "\n",
    "The environment is defined by two pure functions:\n",
    "1. `reset(key) -> state`: Deterministically creates a starting state.\n",
    "2. `step(state, action) -> (next_state, reward, done)`: Advances physics by one tick.\n",
    "\n",
    "**Why reimplement CartPole?**\n",
    "Standard `gym` is written in Python/C++. We cannot `jit` or `vmap` it. By rewriting the dynamics equation (Euler integration) in JAX, we can simulate **thousands** of CartPoles in parallel on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureJaxCartPole:\n",
    "    def __init__(self):\n",
    "        # Physical constants (static, not state)\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masscart + self.masspole)\n",
    "        self.length = 0.5 \n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02 # Seconds per step\n",
    "        self.theta_threshold_radians = 12 * 2 * jnp.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "    def reset(self, key):\n",
    "        # Pure function: Input Key -> Output State\n",
    "        state = jax.random.uniform(key, shape=(4,), minval=-0.05, maxval=0.05)\n",
    "        return state\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # Unpack state: [Position, Velocity, Angle, Angular Velocity]\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        \n",
    "        # Physics Math (Euler Integration)\n",
    "        force = jax.lax.select(action == 1, self.force_mag, -self.force_mag)\n",
    "        costheta = jnp.cos(theta)\n",
    "        sintheta = jnp.sin(theta)\n",
    "\n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        # State Update\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "\n",
    "        next_state = jnp.array([x, x_dot, theta, theta_dot])\n",
    "        \n",
    "        # Termination Logic\n",
    "        done = (x < -self.x_threshold) | (x > self.x_threshold) | (theta < -self.theta_threshold_radians) | (theta > self.theta_threshold_radians)\n",
    "        reward = 1.0 # Keep pole up = +1 reward\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Visualization\n",
    "Since our environment is just math numbers on a GPU, we don't have a built-in window. We use this CPU-based helper to render the state arrays into pixels for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_cartpole(state, width=600, height=400):\n",
    "    x, _, theta, _ = state\n",
    "    world_width = 2.4 * 2\n",
    "    scale = width / world_width\n",
    "    carty = 250\n",
    "    polewidth, polelen = 10.0, scale * 1.0\n",
    "    cartwidth, cartheight = 50.0, 30.0\n",
    "\n",
    "    img = Image.new('RGB', (width, height), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.line([(0, carty), (width, carty)], fill=(0, 0, 0), width=1)\n",
    "\n",
    "    cartx = x * scale + width / 2.0\n",
    "    draw.rectangle([cartx - cartwidth / 2, carty - cartheight / 2, cartx + cartwidth / 2, carty + cartheight / 2], fill=(0, 0, 0))\n",
    "\n",
    "    rotation_angle = -theta\n",
    "    l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "    coords = []\n",
    "    for px, py in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "        px_rot = px * np.cos(rotation_angle) - py * np.sin(rotation_angle)\n",
    "        py_rot = px * np.sin(rotation_angle) + py * np.cos(rotation_angle)\n",
    "        coords.append((cartx + px_rot, carty - py_rot))\n",
    "        \n",
    "    draw.polygon(coords, fill=(204, 153, 102))\n",
    "    draw.ellipse([cartx - 2, carty - 2, cartx + 2, carty + 2], fill=(127, 127, 255))\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Networks and Optimizer\n",
    "\n",
    "In JAX, a Neural Network is not a class holding data. It is a **pure function** that transforms input data using a **parameter tree (dictionary)**.\n",
    "\n",
    "### Design Choices:\n",
    "1.  **Orthogonal Initialization:** PPO is sensitive to initial weights. Orthogonal init ensures features are decorrelated at the start, leading to stable initial gradients.\n",
    "2.  **Separate Actor/Critic:** We use two separate parameter sets (Actor and Critic). This avoids the \"competition\" for features that can happen in shared backbones.\n",
    "3.  **Manual Adam:** We implement Adam manually to keep the entire optimizer state stateless and JIT-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_init(key, shape, scale=1.0):\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = jax.random.normal(key, flat_shape)\n",
    "    u, _, vt = jnp.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else vt\n",
    "    return scale * q.reshape(shape)\n",
    "\n",
    "def init_actor_critic(key, obs_dim, action_dim, hidden_dim=64):\n",
    "    keys = jax.random.split(key, 6)\n",
    "    def init_layer(k, i, o, s=1.0): \n",
    "        return {'w': orthogonal_init(k, (i, o), s), 'b': jnp.zeros((o,))}\n",
    "    # Returning a dictionary of arrays (The Parameter Tree)\n",
    "    return {\n",
    "        'actor': {\n",
    "            'l1': init_layer(keys[0], obs_dim, hidden_dim, np.sqrt(2)),\n",
    "            'l2': init_layer(keys[1], hidden_dim, hidden_dim, np.sqrt(2)),\n",
    "            'head': init_layer(keys[2], hidden_dim, action_dim, 0.01)\n",
    "        },\n",
    "        'critic': {\n",
    "            'l1': init_layer(keys[3], obs_dim, hidden_dim, np.sqrt(2)),\n",
    "            'l2': init_layer(keys[4], hidden_dim, hidden_dim, np.sqrt(2)),\n",
    "            'head': init_layer(keys[5], hidden_dim, 1, 1.0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def forward_mlp(params, x):\n",
    "    # Functional forward pass\n",
    "    x = jax.nn.tanh(x @ params['l1']['w'] + params['l1']['b'])\n",
    "    x = jax.nn.tanh(x @ params['l2']['w'] + params['l2']['b'])\n",
    "    return x @ params['head']['w'] + params['head']['b']\n",
    "\n",
    "def get_action_logits(params, obs): return forward_mlp(params['actor'], obs)\n",
    "def get_value(params, obs): return forward_mlp(params['critic'], obs).squeeze(-1)\n",
    "\n",
    "def adam_update(grads, opt_state, params, lr, max_grad_norm=0.5):\n",
    "    # A stateless implementation of Adam optimization\n",
    "    step = opt_state['step'] + 1\n",
    "    # 1. Global Gradient Clipping\n",
    "    leaves, _ = jax.tree_util.tree_flatten(grads)\n",
    "    total_norm = jnp.sqrt(sum(jnp.sum(g ** 2) for g in leaves))\n",
    "    grads = jax.tree.map(lambda g: g * jnp.minimum(max_grad_norm / (total_norm + 1e-6), 1.0), grads)\n",
    "    \n",
    "    # 2. Adam Momentum Updates\n",
    "    m = jax.tree.map(lambda m, g: 0.9 * m + 0.1 * g, opt_state['m'], grads)\n",
    "    v = jax.tree.map(lambda v, g: 0.999 * v + 0.001 * (g ** 2), opt_state['v'], grads)\n",
    "    \n",
    "    # 3. Bias Correction\n",
    "    m_hat = jax.tree.map(lambda m: m / (1 - 0.9 ** step), m)\n",
    "    v_hat = jax.tree.map(lambda v: v / (1 - 0.999 ** step), v)\n",
    "    \n",
    "    # 4. Weight Update\n",
    "    params = jax.tree.map(lambda p, m, v: p - lr * m / (jnp.sqrt(v) + 1e-8), params, m_hat, v_hat)\n",
    "    return params, {'m': m, 'v': v, 'step': step}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 1: The Rollout (Data Collection)\n",
    "\n",
    "PPO is an **on-policy** algorithm. This means we must collect data using the *current* policy, train on it, and then discard it.\n",
    "\n",
    "**The Hardware Trick:**\n",
    "We use `jax.lax.scan` to run the environment loop. In Python, a loop over 128 steps would trigger 128 GPU kernel launches. In JAX, `scan` fuses this entire loop into **one single kernel**.\n",
    "\n",
    "**Dimensions:**\n",
    "We run $N$ environments in parallel (via `vmap`).\n",
    "* Input State: $(N_{envs}, 4)$\n",
    "* Output Trajectory: $(N_{steps}, N_{envs}, 4)$\n",
    "\n",
    "**Crucial Concept:** \n",
    "During this phase, `jax.stop_gradient` is effectively on. We are not backpropagating through time. We are just recording integer IDs (actions) and floats (rewards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rollout_fn(env, num_steps, num_envs):\n",
    "    \"\"\"Creates the rollout function (Closure).\"\"\"\n",
    "    \n",
    "    def rollout_step(carry, unused):\n",
    "        # Unpack the carry state\n",
    "        env_states, episode_returns, params, key = carry\n",
    "        key, subkey = jax.random.split(key)\n",
    "        \n",
    "        # 1. Get Action from Policy\n",
    "        logits = get_action_logits(params, env_states)\n",
    "        action = jax.random.categorical(subkey, logits)\n",
    "        value = get_value(params, env_states)\n",
    "        \n",
    "        # 2. Store Log Probs for later gradient calculation\n",
    "        log_prob_all = jax.nn.log_softmax(logits)\n",
    "        action_log_prob = jnp.take_along_axis(log_prob_all, action[:, None], axis=1).squeeze(-1)\n",
    "        \n",
    "        # 3. Step the Environment (Vectorized over num_envs)\n",
    "        next_env_states, reward, done = vmap(env.step)(env_states, action)\n",
    "        \n",
    "        # 4. Handle Auto-Reset (Stateless)\n",
    "        # If done, we replace the state with a fresh reset state immediately.\n",
    "        key, *reset_keys = jax.random.split(key, num_envs + 1)\n",
    "        reset_states = vmap(env.reset)(jnp.array(reset_keys))\n",
    "        next_env_states = jnp.where(done[:, None], reset_states, next_env_states)\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_returns = episode_returns + reward\n",
    "        final_return = jnp.where(done, episode_returns, 0.0)\n",
    "        episode_returns = jnp.where(done, 0.0, episode_returns)\n",
    "        \n",
    "        # Pack data for trajectory buffer\n",
    "        transition = (env_states, action, action_log_prob, reward, done, value, final_return)\n",
    "        return (next_env_states, episode_returns, params, key), transition\n",
    "\n",
    "    return rollout_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 2: GAE (Generalized Advantage Estimation)\n",
    "\n",
    "After collecting data, we need to grade the agent's performance. \n",
    "We need to calculate the **Advantage** ($A_t$): *\"How much better was this action than the average action the Critic expected?\"*\n",
    "\n",
    "### The Bias-Variance Trade-off ($\\\\lambda$)\n",
    "We estimate the return using **GAE**:\n",
    "$$ A_t^{GAE} = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} + ... $$\n",
    "\n",
    "Where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the **TD Error**.\n",
    "\n",
    "* **If $\\lambda = 0$:** The target is mostly the Critic's prediction ($r_t + V_{next}$). Low Variance, High Bias (Stable but biased).\n",
    "* **If $\\lambda = 1$:** The target is the full Monte Carlo sum of rewards. High Variance, Low Bias (Accurate but noisy).\n",
    "* **We use $\\lambda=0.95$:** A sweet spot that balances stability and accuracy.\n",
    "\n",
    "**Implementation Note:** Because $A_t$ depends on $A_{t+1}$, we must calculate this **backwards** (from $t=T$ down to $0$). We use `jax.lax.scan` with `reverse=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gae(params, next_env_states, traj_batch, args):\n",
    "    obs, actions, logprobs, rewards, dones, values, final_returns = traj_batch\n",
    "    \n",
    "    # Bootstrap with the value of the LAST state in the sequence\n",
    "    next_value = get_value(params, next_env_states)\n",
    "    \n",
    "    def gae_scan_fn(carry, t):\n",
    "        last_gae_lam, next_val = carry\n",
    "        \n",
    "        # TD-Error: delta = r + gamma * V(next) - V(current)\n",
    "        delta = rewards[t] + args['gamma'] * next_val * (1.0 - dones[t]) - values[t]\n",
    "        \n",
    "        # GAE recursive formula\n",
    "        last_gae_lam = delta + args['gamma'] * args['gae_lambda'] * (1.0 - dones[t]) * last_gae_lam\n",
    "        return (last_gae_lam, values[t]), last_gae_lam\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        gae_scan_fn, \n",
    "        (jnp.zeros_like(next_value), next_value), \n",
    "        jnp.arange(args['num_steps']), \n",
    "        reverse=True # <--- Crucial: We calculate backwards!\n",
    "    )\n",
    "    \n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 3: The PPO Loss Function\n",
    "\n",
    "This is the heart of the algorithm. We use gradient descent to update our Policy $\\pi$ and Value Function $V$.\n",
    "\n",
    "$$ J_{total} = J_{actor} + c_{vf} J_{critic} - c_{ent} J_{entropy} $$\n",
    "\n",
    "### 1. The Actor Loss (Clipping)\n",
    "We want to increase the probability of good actions (high Advantage). However, if we change the policy *too much*, training becomes unstable. \n",
    "\n",
    "**The PPO Solution:**\n",
    "We define the ratio $r_t = \\frac{\\pi_{new}}{\\pi_{old}}$.\n",
    "$$ L^{CLIP} = \\min(r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t) $$\n",
    "This \"Pessimistic Bound\" ignores the gradient if the policy has changed too much (outside $1 \\pm \\epsilon$), preventing catastrophic updates.\n",
    "\n",
    "### 2. The Critic Loss (Value Clipping)\n",
    "The Critic learns to predict returns (Regression). We use a similar clipping trick here: if the Value estimate changes too drastically from the previous iteration, we clip the loss. This prevents the Critic from chasing outliers.\n",
    "\n",
    "### 3. Entropy Bonus\n",
    "We subtract entropy from the loss (or add it to the objective). This encourages the distribution to remain \"flat\" (random). If entropy drops too low, the agent becomes deterministic and stops exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_update_fn(args):\n",
    "    \"\"\"Creates the PPO Update function.\"\"\"\n",
    "    \n",
    "    def loss_fn(params, batch):\n",
    "        obs, act, logp_old, adv, ret, val_old = batch\n",
    "        \n",
    "        # 1. Re-run network on recorded observations\n",
    "        new_logits = get_action_logits(params, obs)\n",
    "        new_values = get_value(params, obs)\n",
    "        \n",
    "        # 2. Probability Ratios\n",
    "        logp_new = jnp.take_along_axis(jax.nn.log_softmax(new_logits), act[:, None], axis=1).squeeze(-1)\n",
    "        ratio = jnp.exp(logp_new - logp_old)\n",
    "        \n",
    "        # 3. Actor Loss (Clipped)\n",
    "        loss_actor_unclipped = adv * ratio\n",
    "        loss_actor_clipped = adv * jnp.clip(ratio, 1 - args['clip_coef'], 1 + args['clip_coef'])\n",
    "        loss_actor = -jnp.minimum(loss_actor_unclipped, loss_actor_clipped).mean()\n",
    "        \n",
    "        # 4. Critic Loss (Clipped option for stability)\n",
    "        if args['clip_vloss']:\n",
    "            v_loss_unclipped = (new_values - ret) ** 2\n",
    "            v_clipped = val_old + jnp.clip(new_values - val_old, -args['clip_coef'], args['clip_coef'])\n",
    "            v_loss_clipped = (v_clipped - ret) ** 2\n",
    "            loss_critic = 0.5 * jnp.maximum(v_loss_unclipped, v_loss_clipped).mean()\n",
    "        else:\n",
    "            loss_critic = 0.5 * ((new_values - ret) ** 2).mean()\n",
    "            \n",
    "        # 5. Entropy Loss (Exploration Bonus)\n",
    "        entropy = -jnp.sum(jax.nn.softmax(new_logits) * jax.nn.log_softmax(new_logits), axis=1).mean()\n",
    "        loss_entropy = -args['ent_coef'] * entropy\n",
    "        \n",
    "        total_loss = loss_actor + args['vf_coef'] * loss_critic + loss_entropy\n",
    "        return total_loss, (loss_actor, loss_critic, entropy)\n",
    "\n",
    "    def update_minibatch(carry, batch):\n",
    "        params, opt_state = carry\n",
    "        (loss, metrics), grads = value_and_grad(loss_fn, has_aux=True)(params, batch)\n",
    "        params, opt_state = adam_update(grads, opt_state, params, args['lr'], args['max_grad_norm'])\n",
    "        return (params, opt_state), metrics\n",
    "\n",
    "    return update_minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Composition: The `make_train` Factory\n",
    "\n",
    "This function is the \"Orchestrator.\" It connects the components into a single Computational Graph that JAX can compile.\n",
    "\n",
    "### Data Flow Architecture\n",
    "1.  **Collection (Scan):**\n",
    "    * Input: `(NumEnvs, ObsDim)`\n",
    "    * Output: `(NumSteps, NumEnvs, ObsDim)` \n",
    "2.  **Processing (GAE):**\n",
    "    * Calculates Advantages using the full time-batch.\n",
    "3.  **Flattening:**\n",
    "    * The `NumSteps` and `NumEnvs` dimensions are merged. We don't care about time anymore, only independent data points.\n",
    "    * Shape: `(NumSteps * NumEnvs, ObsDim)` = `(BatchSize, ObsDim)`\n",
    "4.  **SGD Epochs (Scan):**\n",
    "    * We shuffle the large batch.\n",
    "    * We iterate over minibatches to update the network.\n",
    "\n",
    "**The JIT Magic:**\n",
    "The final line `return jit(train_step)` compiles **all** of this logic (Simulation + GAE + Gradient Descent) into a single C++/CUDA binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(args):\n",
    "    \"\"\"Combines Env, Policy, and Update into a single JIT-able function.\"\"\"\n",
    "    \n",
    "    env = PureJaxCartPole()\n",
    "    rollout_fn = create_rollout_fn(env, args['num_steps'], args['num_envs'])\n",
    "    update_fn = create_update_fn(args)\n",
    "    \n",
    "    batch_size = args['num_envs'] * args['num_steps']\n",
    "    minibatch_size = batch_size // args['num_minibatches']\n",
    "\n",
    "    def train_step(carry, unused):\n",
    "        params, opt_state, env_states, episode_returns, key = carry\n",
    "        \n",
    "        # --- Phase 1: Data Collection ---\n",
    "        (next_env_states, episode_returns, params, key), traj_batch = jax.lax.scan(\n",
    "            rollout_fn, (env_states, episode_returns, params, key), None, length=args['num_steps']\n",
    "        )\n",
    "        \n",
    "        # --- Phase 2: GAE ---\n",
    "        advantages, returns = calculate_gae(params, next_env_states, traj_batch, args)\n",
    "        \n",
    "        # Flatten and Normalize\n",
    "        obs, actions, logprobs, _, _, values, final_returns = traj_batch\n",
    "        flat_inds = lambda x: x.reshape(batch_size, -1) if x.ndim > 2 else x.reshape(batch_size)\n",
    "        \n",
    "        b_obs, b_act, b_logp, b_adv, b_ret, b_val = map(flat_inds, \n",
    "            (obs, actions, logprobs, advantages, returns, values)\n",
    "        )\n",
    "        \n",
    "        if args['norm_adv']:\n",
    "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
    "\n",
    "        # --- Phase 3: Update (Multiple Epochs) ---\n",
    "        def update_epoch(carry, unused):\n",
    "            params, opt_state, key = carry\n",
    "            key, subkey = jax.random.split(key)\n",
    "            \n",
    "            # Shuffle data\n",
    "            perm = jax.random.permutation(subkey, batch_size)\n",
    "            b_obs_s, b_act_s, b_logp_s, b_adv_s, b_ret_s, b_val_s = \\\n",
    "                jax.tree_util.tree_map(lambda x: x[perm], (b_obs, b_act, b_logp, b_adv, b_ret, b_val))\n",
    "            \n",
    "            # Iterate over minibatches\n",
    "            def get_batch(i): \n",
    "                return jax.tree_util.tree_map(\n",
    "                    lambda x: jax.lax.dynamic_slice_in_dim(x, i, minibatch_size), \n",
    "                    (b_obs_s, b_act_s, b_logp_s, b_adv_s, b_ret_s, b_val_s)\n",
    "                )\n",
    "            \n",
    "            def run_minibatch(carry, i):\n",
    "                return update_fn(carry, get_batch(i))\n",
    "\n",
    "            (params, opt_state), metrics = jax.lax.scan(\n",
    "                run_minibatch, (params, opt_state), jnp.arange(0, batch_size, minibatch_size)\n",
    "            )\n",
    "            return (params, opt_state, key), metrics\n",
    "\n",
    "        (params, opt_state, key), metrics = jax.lax.scan(\n",
    "            update_epoch, (params, opt_state, key), None, length=args['update_epochs']\n",
    "        )\n",
    "        \n",
    "        # Return packing\n",
    "        avg_return = final_returns.sum() / (final_returns > 0).sum().clip(1.0)\n",
    "        metrics = jax.tree.map(lambda x: x.mean(), metrics)\n",
    "        \n",
    "        return (params, opt_state, next_env_states, episode_returns, key), (metrics, avg_return)\n",
    "\n",
    "    return jit(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execution\n",
    "\n",
    "**The \"First Call\" Penalty:**\n",
    "When you run `train_step` for the first time, JAX/XLA must compile the computational graph. This might take 5-15 seconds. \n",
    "\n",
    "**Subsequent Calls:**\n",
    "Once compiled, the function runs at full GPU speed. On a modern GPU, this CartPole implementation can exceed **2 million Steps Per Second (SPS)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 42,\n",
    "    'total_timesteps': 500000,\n",
    "    'num_envs': 64,\n",
    "    'num_steps': 128,\n",
    "    'lr': 2.5e-4,\n",
    "    'num_minibatches': 4,\n",
    "    'update_epochs': 4,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_coef': 0.2,\n",
    "    'ent_coef': 0.01,\n",
    "    'vf_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'norm_adv': True,\n",
    "    'clip_vloss': True,\n",
    "}\n",
    "\n",
    "print(\"Compiling...\")\n",
    "train_step = make_train(config)\n",
    "\n",
    "# Init State\n",
    "rng = jax.random.PRNGKey(config['seed'])\n",
    "rng, init_key = jax.random.split(rng)\n",
    "params = init_actor_critic(init_key, obs_dim=4, action_dim=2)\n",
    "opt_state = {'m': jax.tree.map(jnp.zeros_like, params), 'v': jax.tree.map(jnp.zeros_like, params), 'step': 0}\n",
    "rng, *env_keys = jax.random.split(rng, config['num_envs'] + 1)\n",
    "env_states = vmap(PureJaxCartPole().reset)(jnp.array(env_keys))\n",
    "episode_returns = jnp.zeros(config['num_envs'])\n",
    "\n",
    "runner_state = (params, opt_state, env_states, episode_returns, rng)\n",
    "batch_size = config['num_envs'] * config['num_steps']\n",
    "num_updates = config['total_timesteps'] // batch_size\n",
    "\n",
    "print(f\"Starting training for {num_updates} updates...\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for i in range(num_updates):\n",
    "    runner_state, (metrics, avg_return) = train_step(runner_state, None)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        sps = (i + 1) * batch_size / (time.perf_counter() - start_time)\n",
    "        print(f\"Update {i}: Return {avg_return:.2f} | SPS: {int(sps)} | Loss: {metrics[0].mean():.3f}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}