# ===----------------------------------------------------------------------=== #
# Nabla 2026
# SPDX-License-Identifier: Apache-2.0
# ===----------------------------------------------------------------------=== #

from .core import (
    GRAPH,
    PyTreeDef,
    Tensor,
    TensorImpl,
    default_device,
    default_dtype,
    defaults,
    defaults_like,
    driver_tensor_type,
    realize_all,
    tensor_leaves,
    traced,
    tree_flatten,
    tree_leaves,
    tree_map,
    tree_structure,
    tree_unflatten,
    untraced,
    with_batch_dims,
)
from .core.autograd.api import grad, value_and_grad
from .core.autograd.utils import backward
from .core.sharding.spec import (
    DeviceMesh,
    DimSpec,
    P,
    PartitionSpec,
    ShardingSpec,
    compute_local_shape,
    get_num_shards,
)
from .ops.base import (
    BinaryOperation,
    CreationOperation,
    Operation,
    ReduceOperation,
    UnaryOperation,
)
from .ops.binary import (
    add,
    div,
    matmul,
    mul,
    sub,
    mod,
    pow,
    outer,
)
from .ops.communication import (
    all_gather,
    all_reduce,
    distributed_broadcast,
    all_to_all,
    ppermute,
    reduce_scatter,
    reshard,
    shard,
    to_device,
    transfer_to,
    cpu,
    gpu,
    accelerator,
)
from .ops.comparison import (
    equal,
    greater,
    greater_equal,
    less,
    less_equal,
    not_equal,
    logical_xor,
    logical_and,
    logical_or,
    logical_not,
)
from .ops.control_flow import cond, scan, where, while_loop
from .ops.creation import (
    arange,
    constant,
    full,
    full_like,
    gaussian,
    hann_window,
    normal,
    ones,
    ones_like,
    uniform,
    zeros,
    zeros_like,
    triu,
    tril,
)
from .ops.multi_output import (
    chunk,
    minmax,
    split,
    unbind,
)
from .ops.reduction import (
    mean,
    mean_physical,
    reduce_max,
    reduce_min,
    reduce_sum,
    reduce_sum_physical,
    argmax,
    argmin,
    cumsum,
)
from .ops.unary import (
    abs,
    exp,
    log,
    neg,
    relu,
    sigmoid,
    softmax,
    logsoftmax,
    sqrt,
    tanh,
    acos,
    atanh,
    cos,
    erf,
    floor,
    is_inf,
    is_nan,
    log1p,
    rsqrt,
    silu,
    sin,
    trunc,
    gelu,
    round,
    cast,
)
from .ops.view import (
    broadcast_to,
    broadcast_to_physical,
    concatenate,
    decr_batch_dims,
    gather,
    incr_batch_dims,
    move_axis_from_batch_dims,
    move_axis_to_batch_dims,
    moveaxis,
    reshape,
    scatter,
    slice_tensor,
    slice_update,
    squeeze,
    squeeze_physical,
    stack,
    swap_axes,
    transpose,
    unsqueeze,
    unsqueeze_physical,
    flip,
    permute,
    flatten,
    rebind,
    pad,
    as_interleaved_complex,
    view_as_real_interleaved,
)

transpose = swap_axes
from .ops.utils import call_custom_kernel
from .transforms.compile import CompilationStats, CompiledFunction, compile
from .transforms.shard_map import shard_map
from .transforms.vmap import vmap

from max.driver import Accelerator, CPU
from max.dtype import DType

__all__ = [
    "defaults",
    "default_device",
    "default_dtype",
    "defaults_like",
    "Tensor",
    "TensorImpl",
    "GRAPH",
    "driver_tensor_type",
    "Operation",
    "BinaryOperation",
    "ReduceOperation",
    "UnaryOperation",
    "CreationOperation",
    "add",
    "mul",
    "sub",
    "div",
    "matmul",
    "mod",
    "pow",
    "outer",
    "reduce_sum",
    "reduce_max",
    "reduce_min",
    "mean",
    "argmax",
    "argmin",
    "cumsum",
    "sum",
    "max",
    "min",
    "shard",
    "all_gather",
    "all_reduce",
    "all_to_all",
    "distributed_broadcast",
    "reduce_scatter",
    "reshard",
    "ppermute",
    "to_device",
    "transfer_to",
    "cpu",
    "gpu",
    "accelerator",
    "where",
    "cond",
    "while_loop",
    "scan",
    "split",
    "chunk",
    "unbind",
    "minmax",
    "equal",
    "not_equal",
    "greater",
    "greater_equal",
    "less",
    "less_equal",
    "logical_xor",
    "logical_and",
    "logical_or",
    "logical_not",
    "relu",
    "sigmoid",
    "tanh",
    "exp",
    "neg",
    "abs",
    "log",
    "sqrt",
    "softmax",
    "logsoftmax",
    "acos",
    "atanh",
    "cos",
    "erf",
    "floor",
    "is_inf",
    "is_nan",
    "log1p",
    "rsqrt",
    "silu",
    "sin",
    "trunc",
    "gelu",
    "round",
    "cast",
    "unsqueeze",
    "squeeze",
    "swap_axes",
    "transpose",
    "flip",
    "moveaxis",
    "broadcast_to",
    "reshape",
    "flatten",
    "rebind",
    "pad",
    "as_interleaved_complex",
    "view_as_real_interleaved",
    "gather",
    "scatter",
    "slice_tensor",
    "slice_update",
    "concatenate",
    "stack",
    "flip",
    "permute",
    "incr_batch_dims",
    "decr_batch_dims",
    "move_axis_to_batch_dims",
    "move_axis_from_batch_dims",
    "unsqueeze_physical",
    "squeeze_physical",
    "broadcast_to_physical",
    "reduce_sum_physical",
    "mean_physical",
    "grad",
    "value_and_grad",
    "backward",
    "vmap",
    "shard_map",
    "compile",
    "CompiledFunction",
    "CompilationStats",
    "PyTreeDef",
    "tensor_leaves",
    "traced",
    "tree_flatten",
    "tree_leaves",
    "tree_map",
    "tree_structure",
    "tree_unflatten",
    "untraced",
    "with_batch_dims",
    "constant",
    "full",
    "zeros",
    "ones",
    "arange",
    "uniform",
    "gaussian",
    "hann_window",
    "normal",
    "triu",
    "tril",
    "zeros_like",
    "ones_like",
    "full_like",
    "DeviceMesh",
    "DimSpec",
    "ShardingSpec",
    "P",
    "PartitionSpec",
    "compute_local_shape",
    "get_num_shards",
    "realize_all",
    "Accelerator",
    "CPU",
    "DType",
    "call_custom_kernel",
    "_clear_caches",
]

sum = reduce_sum
max = reduce_max
min = reduce_min


def _clear_caches() -> None:
    """Clear all caches and state for fresh start (useful for testing)."""
    GRAPH.clear_all()
