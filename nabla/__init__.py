# ===----------------------------------------------------------------------=== #
# Nabla 2026
# SPDX-License-Identifier: Apache-2.0
# ===----------------------------------------------------------------------=== #

from .core import (
    GRAPH,
    PyTreeDef,
    Tensor,
    TensorImpl,
    default_device,
    default_dtype,
    defaults,
    defaults_like,
    driver_tensor_type,
    realize_all,
    tensor_leaves,
    traced,
    tree_flatten,
    tree_leaves,
    tree_map,
    tree_structure,
    tree_unflatten,
    untraced,
    untraced,
    with_batch_dims,
)
from .core.autograd.api import grad
from .core.sharding.spec import (
    DeviceMesh,
    DimSpec,
    P,
    PartitionSpec,
    ShardingSpec,
    compute_local_shape,
    get_num_shards,
)
from .ops.base import BinaryOperation, Operation, ReduceOperation, UnaryOperation
from .ops.binary import (
    AddOp,
    DivOp,
    MatmulOp,
    MulOp,
    SubOp,
    add,
    div,
    matmul,
    mul,
    sub,
)
from .ops.communication import (
    all_gather,
    all_reduce,
    shard,
    to_device,
    cpu,
    gpu,
    accelerator,
)
from .ops.comparison import (
    EqualOp,
    GreaterEqualOp,
    GreaterOp,
    LessEqualOp,
    LessOp,
    LogicalAndOp,
    LogicalOrOp,
    LogicalNotOp,
    NotEqualOp,
    equal,
    greater,
    greater_equal,
    less,
    less_equal,
    logical_and,
    logical_or,
    logical_not,
    not_equal,
)
from .ops.control_flow import (
    CondOp,
    ScanOp,
    WhereOp,
    WhileLoopOp,
    cond,
    scan,
    where,
    while_loop,
)
from .ops.creation import (
    arange,
    constant,
    full,
    full_like,
    gaussian,
    normal,
    ones,
    ones_like,
    uniform,
    zeros,
    zeros_like,
)
from .ops.multi_output import (
    ChunkOp,
    MinMaxOp,
    SplitOp,
    UnbindOp,
    chunk,
    minmax,
    split,
    unbind,
)
from .ops.reduction import (
    MeanOp,
    ReduceMaxOp,
    ReduceMinOp,
    ReduceSumOp,
    mean,
    mean_physical,
    reduce_max,
    reduce_min,
    reduce_sum,
    reduce_sum_physical,
)
from .ops.unary import (
    AbsOp,
    ExpOp,
    LogOp,
    NegOp,
    ReluOp,
    SigmoidOp,
    SqrtOp,
    TanhOp,
    abs,
    exp,
    log,
    neg,
    relu,
    sigmoid,
    softmax,
    sqrt,
    tanh,
)
from .ops.view import (
    broadcast_to,
    broadcast_to_physical,
    concatenate,
    decr_batch_dims,
    gather,
    incr_batch_dims,
    move_axis_from_batch_dims,
    move_axis_to_batch_dims,
    moveaxis,
    reshape,
    scatter,
    squeeze,
    squeeze_physical,
    stack,
    swap_axes,
    unsqueeze,
    unsqueeze_physical,
)
from .transforms.compile import CompilationStats, CompiledFunction, compile
from .transforms.shard_map import shard_map
from .transforms.vmap import vmap

from max.driver import Accelerator, CPU
from max.dtype import DType

__all__ = [
    "defaults",
    "default_device",
    "default_dtype",
    "defaults_like",
    "Tensor",
    "TensorImpl",
    "GRAPH",
    "driver_tensor_type",
    "Operation",
    "BinaryOperation",
    "ReduceOperation",
    "UnaryOperation",
    "add",
    "mul",
    "sub",
    "div",
    "matmul",
    "AddOp",
    "MulOp",
    "SubOp",
    "DivOp",
    "MatmulOp",
    "ReduceSumOp",
    "MeanOp",
    "ReduceMaxOp",
    "ReduceMaxOp",
    "ReduceMinOp",
    "reduce_sum",
    "reduce_max",
    "reduce_min",
    "mean",
    "shard",
    "all_gather",
    "all_reduce",
    "to_device",
    "cpu",
    "gpu",
    "accelerator",
    "where",
    "cond",
    "while_loop",
    "scan",
    "WhereOp",
    "CondOp",
    "WhileLoopOp",
    "ScanOp",
    "split",
    "chunk",
    "unbind",
    "minmax",
    "SplitOp",
    "ChunkOp",
    "UnbindOp",
    "MinMaxOp",
    "equal",
    "not_equal",
    "greater",
    "greater_equal",
    "less",
    "less_equal",
    "EqualOp",
    "NotEqualOp",
    "GreaterOp",
    "GreaterEqualOp",
    "LessOp",
    "LessEqualOp",
    "logical_and",
    "logical_or",
    "logical_not",
    "LogicalAndOp",
    "LogicalOrOp",
    "LogicalNotOp",
    "relu",
    "sigmoid",
    "tanh",
    "exp",
    "neg",
    "abs",
    "softmax",
    "ReluOp",
    "SigmoidOp",
    "TanhOp",
    "ExpOp",
    "NegOp",
    "AbsOp",
    "LogOp",
    "SqrtOp",
    "log",
    "sqrt",
    "unsqueeze",
    "squeeze",
    "swap_axes",
    "moveaxis",
    "broadcast_to",
    "reshape",
    "gather",
    "scatter",
    "concatenate",
    "stack",
    "incr_batch_dims",
    "decr_batch_dims",
    "move_axis_to_batch_dims",
    "move_axis_from_batch_dims",
    "unsqueeze_physical",
    "squeeze_physical",
    "broadcast_to_physical",
    "reduce_sum_physical",
    "mean_physical",
    "vmap",
    "shard_map",
    "compile",
    "CompiledFunction",
    "CompilationStats",
    "PyTreeDef",
    "tensor_leaves",
    "traced",
    "tree_flatten",
    "tree_leaves",
    "tree_map",
    "tree_structure",
    "tree_unflatten",
    "untraced",
    "with_batch_dims",
    "constant",
    "full",
    "zeros",
    "ones",
    "arange",
    "uniform",
    "gaussian",
    "normal",
    "zeros_like",
    "ones_like",
    "full_like",
    "DeviceMesh",
    "DimSpec",
    "ShardingSpec",
    "P",
    "PartitionSpec",
    "compute_local_shape",
    "get_num_shards",
    "xpr",
    "capture_trace",
    "_clear_caches",
    "realize_all",
    "CPU",
    "GPU",
    "DType",
]


def _clear_caches() -> None:
    """Clear all caches and state for fresh start (useful for testing)."""
    GRAPH.clear_all()
