{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af0fdb",
   "metadata": {},
   "source": [
    "# Example 6a: Transformer Training — PyTorch-Style (Imperative)\n",
    "\n",
    "Nabla supports two training paradigms; this notebook demonstrates the **PyTorch-style** imperative API:\n",
    "\n",
    "| Paradigm | Gradient API | Optimizer API |\n",
    "|----------|--------------|---------------|\n",
    "| **PyTorch-style** (this notebook) | `loss.backward()` + `.grad` | `AdamW(model)` → `optimizer.step()` |\n",
    "| **JAX-style** ([6b](06b_transformer_jax)) | `nb.value_and_grad(fn)(args)` | `adamw_init` + `adamw_update` |\n",
    "\n",
    "We build a small Transformer encoder for a synthetic **sequence classification** task\n",
    "using `nb.nn.TransformerEncoderLayer`, `Embedding`, and `MultiHeadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d597535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Transformer Training — PyTorch-style\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Nabla Transformer Training — PyTorch-style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120857f",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding\n",
    "\n",
    "We'll use sinusoidal positional encoding, computed as a fixed buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4043dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
    "    div_term = np.exp(\n",
    "        np.arange(0, d_model, 2, dtype=np.float32) * -(np.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe  # (max_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f5f71",
   "metadata": {},
   "source": [
    "## 2. Define the Model\n",
    "\n",
    "Our `TransformerClassifier` is an `nb.nn.Module` subclass with these components:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `nb.nn.Embedding` | Maps token IDs → dense vectors |\n",
    "| Sinusoidal PE | Encodes position information (fixed, not learned) |\n",
    "| `nb.nn.TransformerEncoderLayer` × N | Self-attention + feed-forward blocks |\n",
    "| `nb.nn.Linear` | Classification head |\n",
    "\n",
    "The `__init__` method creates these components; `forward` chains them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab2ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nb.nn.Module):\n",
    "    \"\"\"Transformer encoder for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers,\n",
    "                 num_classes, max_len=128, dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        self.embedding = nb.nn.Embedding(vocab_size, d_model)\n",
    "        pe_np = make_positional_encoding(max_len, d_model)\n",
    "        self.pe = nb.Tensor.from_dlpack(pe_np)  # fixed, not learned\n",
    "\n",
    "        # --- Encoder stack ---\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nb.nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads,\n",
    "                dim_feedforward=dim_feedforward, dropout=0.0,\n",
    "            )\n",
    "            setattr(self, f\"encoder_{i}\", layer)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nb.nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Embed + positional encoding\n",
    "        x = self.embedding(token_ids)\n",
    "        seq_len = token_ids.shape[-1]\n",
    "        pe = nb.slice_tensor(self.pe, start=(0, 0), size=(seq_len, self.d_model))\n",
    "        x = x + pe\n",
    "\n",
    "        # Encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Mean pool + classify\n",
    "        return self.classifier(nb.mean(x, axis=-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5612fd5",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Data\n",
    "\n",
    "Generate a simple classification task:\n",
    "- Sequences of random token IDs\n",
    "- Labels based on a rule (e.g., majority token determines class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f792c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 150 sequences of length 8\n",
      "Vocab size: 20, Classes: 3\n",
      "Sample tokens: [ 6 19 14 10  7  6 18 10]\n",
      "Sample label:  0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "num_classes = 3\n",
    "n_samples = 150\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Generate random token sequences\n",
    "token_ids_np = np.random.randint(0, vocab_size, (n_samples, seq_len)).astype(np.int64)\n",
    "\n",
    "# Labels: class = (sum of tokens) mod num_classes\n",
    "labels_np = (token_ids_np.sum(axis=1) % num_classes).astype(np.int64)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_onehot_np = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "labels_onehot_np[np.arange(n_samples), labels_np] = 1.0\n",
    "\n",
    "token_ids = nb.Tensor.from_dlpack(token_ids_np)\n",
    "labels = nb.Tensor.from_dlpack(labels_onehot_np)\n",
    "\n",
    "print(f\"Dataset: {n_samples} sequences of length {seq_len}\")\n",
    "print(f\"Vocab size: {vocab_size}, Classes: {num_classes}\")\n",
    "print(f\"Sample tokens: {token_ids_np[0]}\")\n",
    "print(f\"Sample label:  {labels_np[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebce09c",
   "metadata": {},
   "source": [
    "## 4. Build Model and Optimizer\n",
    "\n",
    "**Important initialization order for the stateful optimizer:**\n",
    "Create `AdamW` while the model is in **train mode** (`_training=True`, the default).\n",
    "Nabla's Module pytree includes `_training` in its metadata, so the optimizer's internal\n",
    "moment tensors (`m`, `v`) are snapshot-initialized with that training mode.\n",
    "Calling `model.eval()` *before* creating the optimizer would bake `_training=False` into\n",
    "those snapshots, causing a pytree metadata mismatch the first time `model.train()` is\n",
    "called inside the training loop.\n",
    "\n",
    "Rule of thumb:\n",
    "- **Stateful optimizer** (`AdamW(model)`) → create in **train mode**, call `model.eval()` only for eval passes.\n",
    "- **Functional optimizer** (`adamw_init(model)`) → `model.eval()` *before* `adamw_init` so every pass shares the same `_training=False` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 2 encoder layers, d_model=32, heads=4\n",
      "Total trainable parameters: 17827\n",
      "Optimizer: AdamW (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    max_len=seq_len,\n",
    "    dim_feedforward=64,\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {num_layers} encoder layers, d_model={d_model}, heads={num_heads}\")\n",
    "print(f\"Total trainable parameters: {n_params}\")\n",
    "\n",
    "# Create optimizer while model is in train mode (default _training=True)\n",
    "model.train()\n",
    "optimizer = nb.nn.optim.AdamW(model, lr=1e-3)\n",
    "print(f\"Optimizer: AdamW (lr={optimizer.lr})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fb1e2",
   "metadata": {},
   "source": [
    "## 5. PyTorch-Style Training Loop\n",
    "\n",
    "Imperative four-step loop: `zero_grad → forward → backward → step`\n",
    "\n",
    "- `model.train()` at the top of each iteration ensures the model is in train mode before `optimizer.step()`.\n",
    "- `loss.backward()` populates `.grad` on every `requires_grad=True` parameter and **batch-realizes** all gradients before returning.\n",
    "- `optimizer.step()` (no arguments) reads `.grad`, applies the AdamW update, and returns the updated model.\n",
    "- Assigning `model = optimizer.step()` is necessary because Nabla's lazy execution cannot mutate tensor data truly in-place.\n",
    "\n",
    "For comparability, we use **60 training steps** and record:\n",
    "- total eager loop time\n",
    "- average milliseconds per eager step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ea97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch    Loss         Accuracy  \n",
      "--------------------------------\n",
      "10       3.0606       30.67%    \n",
      "20       2.6096       30.67%    \n",
      "30       2.2841       30.00%    \n",
      "40       2.0109       32.00%    \n",
      "50       1.7453       32.00%    \n",
      "60       1.5089       32.67%    \n",
      "\n",
      "Eager PyTorch-style training time: 6.3128 s\n",
      "Eager PyTorch-style avg step:     105.213 ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "print(f\"\\n{'Epoch':<8} {'Loss':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "eager_train_start = time.perf_counter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()        # ensures train mode before optimizer step\n",
    "    model.zero_grad()    # clear .grad from previous iteration\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(token_ids)\n",
    "    loss = nb.nn.functional.cross_entropy_loss(logits, labels)\n",
    "\n",
    "    # Backward pass — fills .grad on all trainable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step — reads .grad, applies AdamW, returns updated model\n",
    "    model = optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        logits_eval = model(token_ids)\n",
    "        pred_classes = nb.argmax(logits_eval, axis=-1)\n",
    "        target_classes = nb.Tensor.from_dlpack(labels_np.astype(np.int64))\n",
    "        correct = nb.equal(pred_classes, target_classes)\n",
    "        accuracy = nb.mean(nb.cast(correct, nb.DType.float32)).item()\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.4f} {accuracy:<10.2%}\")\n",
    "\n",
    "eager_train_elapsed = time.perf_counter() - eager_train_start\n",
    "eager_train_step_ms = (eager_train_elapsed / max(1, num_epochs)) * 1000.0\n",
    "print(f\"\\nEager PyTorch-style training time: {eager_train_elapsed:.4f} s\")\n",
    "print(f\"Eager PyTorch-style avg step:     {eager_train_step_ms:.3f} ms/step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555f365",
   "metadata": {},
   "source": [
    "## 6. Compiled Training (Bonus)\n",
    "\n",
    "`@nb.compile` runs the same training-step function with cached compiled execution when input metadata matches (shape, dtype, sharding, structure).\n",
    "\n",
    "> **API note:** Inside a compiled function, `value_and_grad` (functional transform) must be used — not `loss.backward()`. The imperative `.backward()` / `.grad` path is for eager execution only.\n",
    "\n",
    "Speedup interpretation in this notebook is simple:\n",
    "- compiled cached runs remove most Python overhead\n",
    "- eager runs keep Python in the step loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc6650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Fresh model + functional optimizer state for compiled run\n",
    "model_c = TransformerClassifier(\n",
    "    vocab_size=vocab_size, d_model=d_model, num_heads=num_heads,\n",
    "    num_layers=num_layers, num_classes=num_classes,\n",
    "    max_len=seq_len, dim_feedforward=64,\n",
    ")\n",
    "# eval() BEFORE adamw_init so both share _training=False pytree structure\n",
    "model_c.eval()\n",
    "opt_state_c = nb.nn.optim.adamw_init(model_c)\n",
    "\n",
    "\n",
    "def loss_fn_for_compile(model, tokens, targets):\n",
    "    logits = model(tokens)\n",
    "    return nb.nn.functional.cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def compiled_step(model, opt_state, tokens, targets):\n",
    "    loss, grads = nb.value_and_grad(loss_fn_for_compile, argnums=0)(\n",
    "        model, tokens, targets\n",
    "    )\n",
    "    model, opt_state = nb.nn.optim.adamw_update(model, grads, opt_state, lr=1e-3)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "def eager_step(model, opt_state, tokens, targets):\n",
    "    loss, grads = nb.value_and_grad(loss_fn_for_compile, argnums=0)(\n",
    "        model, tokens, targets\n",
    "    )\n",
    "    model, opt_state = nb.nn.optim.adamw_update(model, grads, opt_state, lr=1e-3)\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebdbbe",
   "metadata": {},
   "source": [
    "Run the compiled timing loop.\n",
    "\n",
    "For fair comparison, this uses the **same number of steps** as eager training (`num_epochs = 60`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9306dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled training (functional API inside @nb.compile):\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       1.1420      \n",
      "20       1.0677      \n",
      "30       1.0248      \n",
      "40       0.9630      \n",
      "50       0.8735      \n",
      "60       0.7395      \n",
      "\n",
      "Compiled cache stats: CompilationStats(hits=59, misses=1, fallbacks=0, hit_rate=98.3%)\n",
      "First compiled call (trace+compile): 1047.90 ms\n",
      "Cached compiled step avg:          10.83 ms\n",
      "Eager functional step avg:         157.98 ms\n",
      "Compiled cached speedup vs eager:  14.59x\n"
     ]
    }
   ],
   "source": [
    "# Use the same step count as the eager loop for comparability\n",
    "n_timed_steps = num_epochs\n",
    "\n",
    "print(f\"\\nCompiled training (functional API inside @nb.compile):\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# 1) First compiled call includes trace+compile overhead\n",
    "compile_start = time.perf_counter()\n",
    "model_c, opt_state_c, loss_c = compiled_step(model_c, opt_state_c, token_ids, labels)\n",
    "first_compiled_ms = (time.perf_counter() - compile_start) * 1000.0\n",
    "\n",
    "# 2) Cached compiled execution timing (same loop length as eager)\n",
    "cached_start = time.perf_counter()\n",
    "for step in range(1, n_timed_steps):\n",
    "    model_c, opt_state_c, loss_c = compiled_step(model_c, opt_state_c, token_ids, labels)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss_c.item():<12.4f}\")\n",
    "cached_elapsed = time.perf_counter() - cached_start\n",
    "cached_step_ms = (cached_elapsed / max(1, n_timed_steps - 1)) * 1000.0\n",
    "\n",
    "print(\"\\nCompiled cache stats:\", compiled_step.stats)\n",
    "print(f\"First compiled call (trace+compile): {first_compiled_ms:.2f} ms\")\n",
    "print(f\"Cached compiled step avg:          {cached_step_ms:.2f} ms\")\n",
    "\n",
    "# 3) Eager functional baseline timing (same math, no @nb.compile)\n",
    "model_e = TransformerClassifier(\n",
    "    vocab_size=vocab_size, d_model=d_model, num_heads=num_heads,\n",
    "    num_layers=num_layers, num_classes=num_classes,\n",
    "    max_len=seq_len, dim_feedforward=64,\n",
    ")\n",
    "model_e.eval()\n",
    "opt_state_e = nb.nn.optim.adamw_init(model_e)\n",
    "\n",
    "# one warmup\n",
    "model_e, opt_state_e, _ = eager_step(model_e, opt_state_e, token_ids, labels)\n",
    "\n",
    "eager_start = time.perf_counter()\n",
    "for _ in range(n_timed_steps - 1):\n",
    "    model_e, opt_state_e, loss_e = eager_step(model_e, opt_state_e, token_ids, labels)\n",
    "eager_elapsed = time.perf_counter() - eager_start\n",
    "eager_step_ms = (eager_elapsed / max(1, n_timed_steps - 1)) * 1000.0\n",
    "\n",
    "speedup = eager_step_ms / max(cached_step_ms, 1e-9)\n",
    "print(f\"Eager functional step avg:         {eager_step_ms:.2f} ms\")\n",
    "print(f\"Compiled cached speedup vs eager:  {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e66ab2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### PyTorch-Style (Eager) — This Notebook\n",
    "\n",
    "| Component | API |\n",
    "|-----------|-----|\n",
    "| Token embedding | `nb.nn.Embedding(vocab_size, d_model)` |\n",
    "| Transformer layer | `nb.nn.TransformerEncoderLayer(d_model, heads, ff_dim)` |\n",
    "| Fixed buffer | `tensor.requires_grad = False` |\n",
    "| Training mode | `model.train()` / `model.eval()` |\n",
    "| Clear gradients | `model.zero_grad()` |\n",
    "| Compute gradients | `loss.backward()` |\n",
    "| Parameter update | `model = optimizer.step()` |\n",
    "\n",
    "### JAX-Style (Functional) — See [6b](06b_transformer_jax)\n",
    "\n",
    "| Concept | API |\n",
    "|---------|-----|\n",
    "| Model state | Nested dict pytree |\n",
    "| Compute loss + grads | `nb.value_and_grad(fn, argnums=0)(params, ...)` |\n",
    "| Optimizer | `adamw_init` + `adamw_update` |\n",
    "| Compiled training | `@nb.compile` on functional `value_and_grad` step |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
