{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f30266",
   "metadata": {},
   "source": [
    "# Example 11: Custom Mojo Kernels\n",
    "\n",
    "Nabla lets you write custom operations in **Mojo** — a high-performance\n",
    "language that compiles to MAX graphs. This means you can:\n",
    "\n",
    "1. Write a Mojo kernel (elementwise, reduction, etc.)\n",
    "2. Wrap it as a Nabla `Operation` in Python\n",
    "3. Use it like any built-in op — including with `nb.grad`, `nb.vmap`, etc.\n",
    "\n",
    "**Requirements:** The `modular` package must be installed (`pip install modular`).\n",
    "Mojo kernels are compiled automatically by the MAX engine at graph execution time.\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────────┐     ┌────────────────┐\n",
    "│  Mojo kernel │ ──▶ │ Python Operation │ ──▶ │  Nabla Tensor  │\n",
    "│  (.mojo file)│     │ (UnaryOperation) │     │  computation   │\n",
    "└──────────────┘     └──────────────────┘     └────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7a7190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX SDK available — custom kernels enabled\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "# Check if MAX/Mojo is available\n",
    "try:\n",
    "    from max.graph import TensorValue\n",
    "    from nabla.ops import UnaryOperation, call_custom_kernel\n",
    "    HAS_MOJO = True\n",
    "    print(\"MAX SDK available — custom kernels enabled\")\n",
    "except ImportError:\n",
    "    HAS_MOJO = False\n",
    "    print(\"MAX SDK not installed — showing code patterns only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6f97d",
   "metadata": {},
   "source": [
    "## 1. Writing a Mojo Kernel\n",
    "\n",
    "A Mojo kernel is a struct registered with `@compiler.register(\"name\")`.\n",
    "The `execute` method receives input/output tensors and a device context.\n",
    "\n",
    "Here's a simple kernel that adds 1 to every element:\n",
    "\n",
    "```mojo\n",
    "# kernels/custom_kernel.mojo\n",
    "import compiler\n",
    "from runtime.asyncrt import DeviceContextPtr\n",
    "from tensor import InputTensor, OutputTensor, foreach\n",
    "from utils.index import IndexList\n",
    "\n",
    "\n",
    "@compiler.register(\"add_one\")\n",
    "struct AddOneKernel:\n",
    "    @staticmethod\n",
    "    fn execute[\n",
    "        target: StaticString\n",
    "    ](\n",
    "        output: OutputTensor,\n",
    "        x: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        ctx: DeviceContextPtr,\n",
    "    ):\n",
    "        @parameter\n",
    "        fn add_one[width: Int](idx: IndexList[x.rank]) -> SIMD[x.dtype, width]:\n",
    "            return x.load[width](idx) + 1\n",
    "\n",
    "        foreach[add_one, target=target](output, ctx)\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- `@compiler.register(\"add_one\")` — the name you'll reference from Python\n",
    "- `foreach` auto-vectorizes the elementwise function across the tensor\n",
    "- `InputTensor` / `OutputTensor` handle memory layout automatically\n",
    "- The kernel directory also needs an `__init__.mojo` file (can be empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc1149",
   "metadata": {},
   "source": [
    "## 2. Python Operation Wrapper\n",
    "\n",
    "To use the Mojo kernel in Nabla, wrap it as a `UnaryOperation` subclass.\n",
    "The `kernel` method bridges Python tensors to the Mojo function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963b8502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddOneOp registered\n"
     ]
    }
   ],
   "source": [
    "if HAS_MOJO:\n",
    "    class AddOneOp(UnaryOperation):\n",
    "        \"\"\"Custom op: adds 1 to every element using a Mojo kernel.\"\"\"\n",
    "\n",
    "        @property\n",
    "        def name(self) -> str:\n",
    "            return \"add_one\"\n",
    "\n",
    "        def kernel(self, args, kwargs):\n",
    "            \"\"\"Invoke the Mojo kernel via call_custom_kernel.\"\"\"\n",
    "            x = args[0]\n",
    "            # Point to the directory containing the .mojo kernel files\n",
    "            kernel_dir = Path(\"../../tests/mojo/kernels\")\n",
    "            result = call_custom_kernel(\"my_kernel\", kernel_dir, x, x.type)\n",
    "            return [result]  # Must return a list of TensorValues\n",
    "\n",
    "        def _derivative(self, primals, output):\n",
    "            \"\"\"d(x+1)/dx = 1 — gradient passes through unchanged.\"\"\"\n",
    "            return nb.ones_like(primals)\n",
    "\n",
    "    # Instantiate the op (ops are stateless singletons)\n",
    "    _add_one_op = AddOneOp()\n",
    "\n",
    "    def add_one(x):\n",
    "        \"\"\"Add 1 to every element using our custom Mojo kernel.\"\"\"\n",
    "        return _add_one_op([x], {})[0]\n",
    "\n",
    "    print(\"AddOneOp registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba09be",
   "metadata": {},
   "source": [
    "**Important details:**\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `name` | Must match the `@compiler.register(\"...\")` name |\n",
    "| `kernel(args, kwargs)` | `args` is a `list[TensorValue]`, `kwargs` is a `dict` |\n",
    "| `_derivative(primals, output)` | Enables `nb.grad` — return $\\frac{\\partial \\text{out}}{\\partial \\text{in}}$ |\n",
    "\n",
    "For non-elementwise ops, override `vjp_rule` and `jvp_rule` directly\n",
    "instead of `_derivative`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892e6be",
   "metadata": {},
   "source": [
    "## 3. Using the Custom Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c8ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [1. 2. 3.]\n",
      "Output: [2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "if HAS_MOJO:\n",
    "    x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "    y = add_one(x)\n",
    "    print(f\"Input:  {x.to_numpy()}\")\n",
    "    print(f\"Output: {y.to_numpy()}\")  # [2.0, 3.0, 4.0]\n",
    "else:\n",
    "    print(\"Skipped — MAX SDK not available\")\n",
    "    print(\"Expected: add_one([1.0, 2.0, 3.0]) → [2.0, 3.0, 4.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c37b59",
   "metadata": {},
   "source": [
    "## 4. Differentiating Through Custom Ops\n",
    "\n",
    "Because we implemented `_derivative`, Nabla can differentiate through\n",
    "our custom kernel just like any built-in op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedc59e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = sum((x+1) * x)\n",
      "f'(x) = 2x + 1\n",
      "Input:    [1. 2. 3.]\n",
      "Gradient: [3. 5. 7.]\n"
     ]
    }
   ],
   "source": [
    "if HAS_MOJO:\n",
    "    def f(x):\n",
    "        \"\"\"A function using our custom kernel.\"\"\"\n",
    "        return nb.sum(add_one(x) * x)  # sum((x+1) * x) = sum(x² + x)\n",
    "\n",
    "    x = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "    grad_f = nb.grad(f)\n",
    "    g = grad_f(x)\n",
    "\n",
    "    print(f\"f(x) = sum((x+1) * x)\")\n",
    "    print(f\"f'(x) = 2x + 1\")\n",
    "    print(f\"Input:    {x.to_numpy()}\")\n",
    "    print(f\"Gradient: {g.to_numpy()}\")  # [3.0, 5.0, 7.0]\n",
    "else:\n",
    "    print(\"Skipped — expected gradient of sum((x+1)*x) = 2x+1 = [3.0, 5.0, 7.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290b17f",
   "metadata": {},
   "source": [
    "## 5. Writing More Complex Kernels\n",
    "\n",
    "The `foreach` pattern handles elementwise ops, but you can write any\n",
    "computation in Mojo. Here's a sketch of a fused multiply-add kernel:\n",
    "\n",
    "```mojo\n",
    "@compiler.register(\"fused_mul_add\")\n",
    "struct FusedMulAdd:\n",
    "    @staticmethod\n",
    "    fn execute[\n",
    "        target: StaticString\n",
    "    ](\n",
    "        output: OutputTensor,\n",
    "        a: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        b: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        c: InputTensor[dtype = output.dtype, rank = output.rank],\n",
    "        ctx: DeviceContextPtr,\n",
    "    ):\n",
    "        # output = a * b + c\n",
    "        @parameter\n",
    "        fn fma[width: Int](idx: IndexList[a.rank]) -> SIMD[a.dtype, width]:\n",
    "            return a.load[width](idx) * b.load[width](idx) + c.load[width](idx)\n",
    "\n",
    "        foreach[fma, target=target](output, ctx)\n",
    "```\n",
    "\n",
    "For multi-input ops, extend `Operation` instead of `UnaryOperation`\n",
    "and implement `vjp_rule` / `jvp_rule` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b126f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- Custom Mojo kernels compile automatically via the MAX engine\n",
    "- Wrap kernels as `UnaryOperation` (or `Operation`) subclasses\n",
    "- `call_custom_kernel` handles kernel loading and invocation\n",
    "- Implement `_derivative` (elementwise) or `vjp_rule`/`jvp_rule` for autodiff\n",
    "- Custom ops compose with all Nabla transforms (`grad`, `vmap`, `compile`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
