{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0517f994",
   "metadata": {},
   "source": [
    "# Example 6b: Transformer Training (JAX-Style / Functional)\n",
    "\n",
    "This example builds the same sequence classification Transformer as 6a,\n",
    "but without nn.Module — everything is **pure functions** operating on\n",
    "**parameter dicts** (pytrees).\n",
    "\n",
    "This style is closer to JAX/Flax and shows Nabla's functional flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee5ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Transformer Training — JAX-style (functional)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "import nabla.nn.functional as F\n",
    "\n",
    "print(\"Nabla Transformer Training — JAX-style (functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cf0ec",
   "metadata": {},
   "source": [
    "## 1. Parameter Initialization\n",
    "\n",
    "In the functional style, each layer is a **dict of tensors** (a pytree).\n",
    "We start with simple building blocks and compose them into a full model.\n",
    "\n",
    "### Primitive Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83f6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(in_dim: int, out_dim: int) -> dict:\n",
    "    \"\"\"Initialize a linear layer: {weight, bias}.\"\"\"\n",
    "    return {\n",
    "        \"weight\": F.xavier_normal((in_dim, out_dim)),\n",
    "        \"bias\": nb.zeros((1, out_dim)),\n",
    "    }\n",
    "\n",
    "\n",
    "def init_layer_norm(dim: int) -> dict:\n",
    "    \"\"\"Initialize layer norm: {weight, bias}.\"\"\"\n",
    "    return {\"weight\": nb.ones((dim,)), \"bias\": nb.zeros((dim,))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c9990",
   "metadata": {},
   "source": [
    "### Composite Layers\n",
    "\n",
    "Multi-head attention and encoder layers are just nested dicts of the primitives above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b507d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mha(d_model: int) -> dict:\n",
    "    \"\"\"Initialize multi-head attention: Q, K, V, and output projections.\"\"\"\n",
    "    return {\n",
    "        \"q_proj\": init_linear(d_model, d_model),\n",
    "        \"k_proj\": init_linear(d_model, d_model),\n",
    "        \"v_proj\": init_linear(d_model, d_model),\n",
    "        \"out_proj\": init_linear(d_model, d_model),\n",
    "    }\n",
    "\n",
    "\n",
    "def init_encoder_layer(d_model: int, dim_ff: int) -> dict:\n",
    "    \"\"\"Initialize one Transformer encoder layer.\"\"\"\n",
    "    return {\n",
    "        \"attn\": init_mha(d_model),\n",
    "        \"norm1\": init_layer_norm(d_model),\n",
    "        \"norm2\": init_layer_norm(d_model),\n",
    "        \"ff1\": init_linear(d_model, dim_ff),\n",
    "        \"ff2\": init_linear(dim_ff, d_model),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56782a",
   "metadata": {},
   "source": [
    "### Full Model Initialization\n",
    "\n",
    "The top-level `init_transformer` returns the complete parameter tree. Notice how\n",
    "`num_heads` is passed as a function argument to `transformer_forward` (below),\n",
    "**not** stored in the params dict — keeping the pytree purely numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b70639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer(vocab_size, d_model, num_layers, num_heads,\n",
    "                     num_classes, max_len, dim_ff):\n",
    "    \"\"\"Initialize all transformer parameters as a nested dict.\"\"\"\n",
    "    # Embedding\n",
    "    emb_weight = F.xavier_normal((vocab_size, d_model))\n",
    "\n",
    "    # Sinusoidal positional encoding (fixed, not learned)\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    pos = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
    "    div = np.exp(np.arange(0, d_model, 2, dtype=np.float32) * -(np.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(pos * div)\n",
    "    pe[:, 1::2] = np.cos(pos * div)\n",
    "\n",
    "    return {\n",
    "        \"embedding\": emb_weight,\n",
    "        \"pe\": nb.Tensor.from_dlpack(pe),  # non-differentiable constant\n",
    "        \"layers\": [init_encoder_layer(d_model, dim_ff) for _ in range(num_layers)],\n",
    "        \"classifier\": init_linear(d_model, num_classes),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40830076",
   "metadata": {},
   "source": [
    "## 2. Pure Function Layers\n",
    "\n",
    "Each layer is a **pure function**: `output = layer(params, input)`.\n",
    "No hidden state, no mutation — just inputs in, outputs out.\n",
    "\n",
    "### Basic Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1769eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(params: dict, x):\n",
    "    \"\"\"Functional linear layer: y = xW + b.\"\"\"\n",
    "    return x @ params[\"weight\"] + params[\"bias\"]\n",
    "\n",
    "\n",
    "def layer_norm(params: dict, x, eps: float = 1e-5):\n",
    "    \"\"\"Functional layer normalization.\"\"\"\n",
    "    return F.layer_norm(x, weight=params[\"weight\"], bias=params[\"bias\"], eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e48255",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention\n",
    "\n",
    "The attention function projects input to Q, K, V, splits into heads,\n",
    "computes scaled dot-product attention, and concatenates the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20a3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(params: dict, x, num_heads: int):\n",
    "    \"\"\"Functional multi-head self-attention.\"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape[0], x.shape[1], x.shape[2]\n",
    "    head_dim = d_model // num_heads\n",
    "\n",
    "    # Project to Q, K, V  →  (batch, seq, d_model)\n",
    "    q = linear(params[\"q_proj\"], x)\n",
    "    k = linear(params[\"k_proj\"], x)\n",
    "    v = linear(params[\"v_proj\"], x)\n",
    "\n",
    "    # Reshape to (batch, heads, seq, head_dim)\n",
    "    def reshape_heads(t):\n",
    "        t = nb.reshape(t, (batch_size, seq_len, num_heads, head_dim))\n",
    "        return nb.permute(t, (0, 2, 1, 3))\n",
    "\n",
    "    q, k, v = reshape_heads(q), reshape_heads(k), reshape_heads(v)\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    attn_out = F.scaled_dot_product_attention(q, k, v, training=False)\n",
    "\n",
    "    # Concatenate heads → (batch, seq, d_model)\n",
    "    attn_out = nb.permute(attn_out, (0, 2, 1, 3))\n",
    "    attn_out = nb.reshape(attn_out, (batch_size, seq_len, d_model))\n",
    "\n",
    "    return linear(params[\"out_proj\"], attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8fe78",
   "metadata": {},
   "source": [
    "### Encoder Layer and Full Forward Pass\n",
    "\n",
    "An encoder layer combines attention + feed-forward with residual connections\n",
    "and layer normalization (pre-norm variant). The full forward pass chains\n",
    "embedding → positional encoding → encoder stack → mean pooling → classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d6b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(params: dict, x, num_heads: int):\n",
    "    \"\"\"Functional Transformer encoder layer (pre-norm).\"\"\"\n",
    "    # Self-attention + residual\n",
    "    normed = layer_norm(params[\"norm1\"], x)\n",
    "    x = x + multi_head_attention(params[\"attn\"], normed, num_heads)\n",
    "\n",
    "    # Feed-forward + residual\n",
    "    normed = layer_norm(params[\"norm2\"], x)\n",
    "    x = x + linear(params[\"ff2\"], nb.gelu(linear(params[\"ff1\"], normed)))\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_forward(params: dict, token_ids, num_heads: int):\n",
    "    \"\"\"Full transformer forward pass: tokens → logits.\"\"\"\n",
    "    # Embed + positional encoding\n",
    "    x = F.embedding(token_ids, params[\"embedding\"])\n",
    "    seq_len = token_ids.shape[-1]\n",
    "    d_model = int(x.shape[-1])\n",
    "    pe = nb.slice_tensor(params[\"pe\"], start=(0, 0), size=(seq_len, d_model))\n",
    "    x = x + pe\n",
    "\n",
    "    # Encoder layers\n",
    "    for layer_params in params[\"layers\"]:\n",
    "        x = encoder_layer(layer_params, x, num_heads)\n",
    "\n",
    "    # Mean pooling + classify\n",
    "    return linear(params[\"classifier\"], nb.mean(x, axis=-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c709b2a",
   "metadata": {},
   "source": [
    "## 3. Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9062bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 150 sequences of length 8\n",
      "Vocab: 20, Classes: 3\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "num_classes = 3\n",
    "n_samples = 150\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dim_ff = 64\n",
    "\n",
    "# Random token sequences, labels = (sum of tokens) mod num_classes\n",
    "token_ids_np = np.random.randint(0, vocab_size, (n_samples, seq_len)).astype(np.int64)\n",
    "labels_np = (token_ids_np.sum(axis=1) % num_classes).astype(np.int64)\n",
    "labels_onehot_np = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "labels_onehot_np[np.arange(n_samples), labels_np] = 1.0\n",
    "\n",
    "token_ids = nb.Tensor.from_dlpack(token_ids_np)\n",
    "labels = nb.Tensor.from_dlpack(labels_onehot_np)\n",
    "\n",
    "print(f\"Dataset: {n_samples} sequences of length {seq_len}\")\n",
    "print(f\"Vocab: {vocab_size}, Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41c9e6",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0bcf3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 2 layers, d_model=32, heads=4\n",
      "Total trainable parameters: 18083\n"
     ]
    }
   ],
   "source": [
    "params = init_transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    num_classes=num_classes,\n",
    "    max_len=seq_len,\n",
    "    dim_ff=dim_ff,\n",
    ")\n",
    "\n",
    "opt_state = nb.nn.optim.adamw_init(params)\n",
    "\n",
    "# Count parameters\n",
    "from nabla import tree_leaves, Tensor\n",
    "n_params = sum(p.numel() for p in tree_leaves(params) if isinstance(p, Tensor))\n",
    "print(f\"Model: {num_layers} layers, d_model={d_model}, heads={num_heads}\")\n",
    "print(f\"Total trainable parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9cbfc",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2ee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, tokens, targets):\n",
    "    logits = transformer_forward(params, tokens, num_heads=num_heads)\n",
    "    return nb.nn.functional.cross_entropy_loss(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e957db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    Loss         Accuracy  \n",
      "--------------------------------\n",
      "10       1.0926       44.67%    \n",
      "20       1.0410       48.67%    \n",
      "30       1.0012       49.33%    \n",
      "40       0.9542       53.33%    \n",
      "50       0.8927       62.00%    \n",
      "60       0.8154       64.67%    \n",
      "\n",
      "Functional training time: 11.0480 s\n",
      "Functional avg step:      184.133 ms/step\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "num_epochs = 60\n",
    "\n",
    "print(f\"{'Epoch':<8} {'Loss':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "train_start = time.perf_counter()\n",
    "for epoch in range(num_epochs):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, token_ids, labels)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(params, grads, opt_state, lr=lr)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        logits = transformer_forward(params, token_ids, num_heads=num_heads)\n",
    "        pred_classes = nb.argmax(logits, axis=-1)\n",
    "        target_classes = nb.Tensor.from_dlpack(labels_np.astype(np.int64))\n",
    "        correct = nb.equal(pred_classes, target_classes)\n",
    "        accuracy = nb.mean(nb.cast(correct, nb.DType.float32)).item()\n",
    "        print(f\"{epoch + 1:<8} {loss.item():<12.4f} {accuracy:<10.2%}\")\n",
    "train_elapsed = time.perf_counter() - train_start\n",
    "print(f\"\\nFunctional training time: {train_elapsed:.4f} s\")\n",
    "print(f\"Functional avg step:      {(train_elapsed / max(1, num_epochs)) * 1000.0:.3f} ms/step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06cfb8",
   "metadata": {},
   "source": [
    "## 6. Compiled Training (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44eeadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.compile\n",
    "def compiled_step(params, opt_state, tokens, targets):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, tokens, targets)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=1e-3\n",
    "    )\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "def eager_step(params, opt_state, tokens, targets):\n",
    "    \"\"\"Same logic without @nb.compile — serves as eager baseline.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(loss_fn, argnums=0)(params, tokens, targets)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=1e-3\n",
    "    )\n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc2d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled training:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       1.1312      \n",
      "20       1.0764      \n",
      "30       1.0369      \n",
      "40       1.0024      \n",
      "50       0.9391      \n",
      "60       0.8277      \n",
      "\n",
      "Compiled cache stats: CompilationStats(hits=60, misses=1, fallbacks=0, hit_rate=98.4%)\n",
      "Compiled avg step:   13.10 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager avg step:      209.53 ms\n",
      "Compiled speedup:    16.0x\n"
     ]
    }
   ],
   "source": [
    "# Fresh parameters for compiled training\n",
    "params2 = init_transformer(\n",
    "    vocab_size=vocab_size, d_model=d_model, num_layers=num_layers,\n",
    "    num_heads=num_heads, num_classes=num_classes, max_len=seq_len, dim_ff=dim_ff,\n",
    ")\n",
    "opt_state2 = nb.nn.optim.adamw_init(params2)\n",
    "\n",
    "# First call is warmup (trace + compile)\n",
    "params2, opt_state2, _ = compiled_step(params2, opt_state2, token_ids, labels)\n",
    "\n",
    "print(f\"Compiled training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "compiled_steps = num_epochs\n",
    "compiled_start = time.perf_counter()\n",
    "for step in range(compiled_steps):\n",
    "    params2, opt_state2, loss = compiled_step(params2, opt_state2, token_ids, labels)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.4f}\")\n",
    "compiled_elapsed = time.perf_counter() - compiled_start\n",
    "compiled_step_ms = (compiled_elapsed / max(1, compiled_steps)) * 1000.0\n",
    "\n",
    "print(f\"\\nCompiled cache stats: {compiled_step.stats}\")\n",
    "print(f\"Compiled avg step:   {compiled_step_ms:.2f} ms\")\n",
    "\n",
    "# --- Eager functional baseline (same math, no @nb.compile) ---\n",
    "params_e = init_transformer(\n",
    "    vocab_size=vocab_size, d_model=d_model, num_layers=num_layers,\n",
    "    num_heads=num_heads, num_classes=num_classes, max_len=seq_len, dim_ff=dim_ff,\n",
    ")\n",
    "opt_state_e = nb.nn.optim.adamw_init(params_e)\n",
    "params_e, opt_state_e, _ = eager_step(params_e, opt_state_e, token_ids, labels)  # warmup\n",
    "\n",
    "eager_start = time.perf_counter()\n",
    "for step in range(compiled_steps):\n",
    "    params_e, opt_state_e, loss_e = eager_step(params_e, opt_state_e, token_ids, labels)\n",
    "eager_elapsed = time.perf_counter() - eager_start\n",
    "eager_step_ms = (eager_elapsed / max(1, compiled_steps)) * 1000.0\n",
    "\n",
    "speedup = eager_step_ms / max(compiled_step_ms, 1e-9)\n",
    "print(f\"Eager avg step:      {eager_step_ms:.2f} ms\")\n",
    "print(f\"Compiled speedup:    {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dbdf7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The functional style decomposes the Transformer into pure functions:\n",
    "\n",
    "| Function | Role |\n",
    "|----------|------|\n",
    "| `init_transformer(...)` | Creates the parameter pytree |\n",
    "| `transformer_forward(params, tokens, num_heads)` | Pure forward pass |\n",
    "| `loss_fn(params, tokens, targets)` | Computes scalar loss |\n",
    "| `value_and_grad(loss_fn, argnums=0)` | Returns (loss, gradient pytree) |\n",
    "| `adamw_update(params, grads, ...)` | Returns (new_params, new_opt_state) |\n",
    "\n",
    "No mutation, no hidden state — everything flows through function arguments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
