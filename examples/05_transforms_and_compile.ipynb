{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4589739",
   "metadata": {},
   "source": [
    "# Example 5: Transforms and `@nb.compile`\n",
    "\n",
    "Nabla's transforms are **higher-order functions** that take a function and\n",
    "return a new function with modified behavior. They are fully composable\n",
    "and work with any Nabla operation, including nn.Modules.\n",
    "\n",
    "| Transform | What it does |\n",
    "|-----------|-------------|\n",
    "| `vmap` | Auto-vectorize over a batch dimension |\n",
    "| `grad` | Compute gradients (reverse-mode) |\n",
    "| `jacrev` | Full Jacobian via reverse-mode |\n",
    "| `jacfwd` | Full Jacobian via forward-mode |\n",
    "| `compile` | Compile computation graph to MAX graph |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87de2c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabla Transforms & Compile Example\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "print(\"Nabla Transforms & Compile Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da678734",
   "metadata": {},
   "source": [
    "## 1. `vmap` — Automatic Vectorization\n",
    "\n",
    "`vmap` transforms a function that operates on a single example into one\n",
    "that operates on a batch — without writing any batching logic yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f723de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched dot products (5 pairs of 3D vectors):\n",
      "Tensor([0.9671 1.0155 1.0364 0.5181 0.8411] : f32[5])\n",
      "Shape: [Dim(5)]\n"
     ]
    }
   ],
   "source": [
    "def single_dot(x, y):\n",
    "    \"\"\"Dot product of two vectors (no batch dimension).\"\"\"\n",
    "    return nb.reduce_sum(x * y)\n",
    "\n",
    "# Without vmap: manual loop\n",
    "x_batch = nb.uniform((5, 3))\n",
    "y_batch = nb.uniform((5, 3))\n",
    "\n",
    "# With vmap: automatic vectorization!\n",
    "batched_dot = nb.vmap(single_dot, in_axes=(0, 0))\n",
    "result = batched_dot(x_batch, y_batch)\n",
    "print(f\"Batched dot products (5 pairs of 3D vectors):\")\n",
    "print(result)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57f200",
   "metadata": {},
   "source": [
    "### `in_axes` and `out_axes`\n",
    "\n",
    "`in_axes` controls which axis of each argument is the batch axis.\n",
    "`out_axes` controls where to place the batch axis in the output.\n",
    "Use `None` for arguments that should be broadcast (not batched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2f9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched weighted sum (shared weights):\n",
      "Tensor([2.8038 5.0148 2.5579 2.3182] : f32[4])\n",
      "Shape: [Dim(4)]\n"
     ]
    }
   ],
   "source": [
    "def weighted_sum(x, w):\n",
    "    \"\"\"Weighted sum: w * x, summed.\"\"\"\n",
    "    return nb.reduce_sum(w * x)\n",
    "\n",
    "# x is batched (axis 0), w is shared across the batch\n",
    "batch_fn = nb.vmap(weighted_sum, in_axes=(0, None))\n",
    "\n",
    "x_batch = nb.uniform((4, 3))\n",
    "w = nb.Tensor.from_dlpack(np.array([1.0, 2.0, 3.0], dtype=np.float32))\n",
    "\n",
    "result = batch_fn(x_batch, w)\n",
    "print(f\"Batched weighted sum (shared weights):\")\n",
    "print(result)\n",
    "print(f\"Shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9545cd3",
   "metadata": {},
   "source": [
    "## 2. `vmap` of `grad` — Per-Example Gradients\n",
    "\n",
    "Composing `vmap` with `grad` gives per-example gradients — something that's\n",
    "difficult to do efficiently in most frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbd58fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-example gradients (3 samples, 2 weights):\n",
      "Tensor(\n",
      "  [[ 4.  0.]\n",
      "   [ 0.  6.]\n",
      "   [10. 10.]] : f32[3,2]\n",
      ")\n",
      "Shape: [Dim(3), Dim(2)]\n"
     ]
    }
   ],
   "source": [
    "def per_sample_loss(x, w):\n",
    "    \"\"\"Loss for a single sample: (w @ x)^2.\"\"\"\n",
    "    return nb.reduce_sum(w * x) ** 2\n",
    "\n",
    "# grad of the loss w.r.t. w for a single sample\n",
    "grad_single = nb.grad(per_sample_loss, argnums=1)\n",
    "\n",
    "# vmap over samples — per-example gradients!\n",
    "per_example_grad = nb.vmap(grad_single, in_axes=(0, None))\n",
    "\n",
    "x_batch = nb.Tensor.from_dlpack(\n",
    "    np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=np.float32)\n",
    ")\n",
    "w = nb.Tensor.from_dlpack(np.array([2.0, 3.0], dtype=np.float32))\n",
    "\n",
    "grads = per_example_grad(x_batch, w)\n",
    "print(\"Per-example gradients (3 samples, 2 weights):\")\n",
    "print(grads)\n",
    "print(f\"Shape: {grads.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3afcd8",
   "metadata": {},
   "source": [
    "## 3. `jacrev` and `jacfwd` — Full Jacobians\n",
    "\n",
    "From Example 2: `jacrev` and `jacfwd` compute full Jacobian matrices.\n",
    "Here we show them applied to a more interesting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a5fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  [Dim(2)]\n",
      "Output shape: [Dim(3)]\n",
      "\n",
      "Jacobian via jacrev (shape [Dim(3), Dim(2)]):\n",
      "Tensor(\n",
      "  [[ 0.5224 -0.2612]\n",
      "   [ 0.2135  0.5693]\n",
      "   [-0.183   0.5491]] : f32[3,2]\n",
      ")\n",
      "\n",
      "Jacobian via jacfwd (shape [Dim(3), Dim(2)]):\n",
      "Tensor(\n",
      "  [[ 0.5224 -0.2612]\n",
      "   [ 0.2135  0.5693]\n",
      "   [-0.183   0.5491]] : f32[3,2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def neural_layer(x):\n",
    "    \"\"\"A simple neural network layer: tanh(xW + b).\"\"\"\n",
    "    W = nb.Tensor.from_dlpack(\n",
    "        np.array([[1.0, 0.3, -0.2], [-0.5, 0.8, 0.6]], dtype=np.float32)\n",
    "    )\n",
    "    b = nb.Tensor.from_dlpack(np.array([0.1, -0.1, 0.2], dtype=np.float32))\n",
    "    return nb.tanh(x @ W + b)\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 0.5], dtype=np.float32))\n",
    "\n",
    "J_rev = nb.jacrev(neural_layer)(x)\n",
    "J_fwd = nb.jacfwd(neural_layer)(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {neural_layer(x).shape}\")\n",
    "print(f\"\\nJacobian via jacrev (shape {J_rev.shape}):\")\n",
    "print(J_rev)\n",
    "print(f\"\\nJacobian via jacfwd (shape {J_fwd.shape}):\")\n",
    "print(J_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223f538",
   "metadata": {},
   "source": [
    "## 4. Composing Jacobians — Hessians\n",
    "\n",
    "Since transforms compose, we can compute Hessians by nesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a126b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E(x) = 0.5 * x^T @ A @ x, where A = [[2,1],[1,3]]\n",
      "E([1,2]) = Tensor(9. : f32[])\n",
      "Gradient: Tensor([4. 7.] : f32[2])\n",
      "  (should be Ax = [4, 7])\n",
      "\n",
      "Hessian (should be A = [[2,1],[1,3]]):\n",
      "Tensor(\n",
      "  [[2. 1.]\n",
      "   [1. 3.]] : f32[2,2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def energy(x):\n",
    "    \"\"\"Energy function: E(x) = 0.5 * x^T A x where A = [[2, 1], [1, 3]].\"\"\"\n",
    "    A = nb.Tensor.from_dlpack(\n",
    "        np.array([[2.0, 1.0], [1.0, 3.0]], dtype=np.float32)\n",
    "    )\n",
    "    return 0.5 * nb.reduce_sum(x * (A @ x))\n",
    "\n",
    "x = nb.Tensor.from_dlpack(np.array([1.0, 2.0], dtype=np.float32))\n",
    "print(f\"E(x) = 0.5 * x^T @ A @ x, where A = [[2,1],[1,3]]\")\n",
    "print(f\"E([1,2]) = {energy(x)}\")\n",
    "print(f\"Gradient: {nb.grad(energy)(x)}\")\n",
    "print(f\"  (should be Ax = [4, 7])\")\n",
    "\n",
    "H = nb.jacfwd(nb.grad(energy))(x)\n",
    "print(f\"\\nHessian (should be A = [[2,1],[1,3]]):\")\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd99e2",
   "metadata": {},
   "source": [
    "## 5. `@nb.compile` — Graph Compilation\n",
    "\n",
    "`@nb.compile` traces a function, captures its computation graph, and\n",
    "compiles it into a MAX graph. Subsequent calls with the same\n",
    "tensor shapes/dtypes hit a cache, reducing Python overhead in repeated calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d684de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_fn(x, y):\n",
    "    \"\"\"A function with many operations.\"\"\"\n",
    "    for _ in range(5):\n",
    "        x = nb.relu(x @ y + x)\n",
    "    return nb.reduce_sum(x)\n",
    "\n",
    "@nb.compile\n",
    "def fast_fn(x, y):\n",
    "    \"\"\"Same function, but compiled.\"\"\"\n",
    "    for _ in range(5):\n",
    "        x = nb.relu(x @ y + x)\n",
    "    return nb.reduce_sum(x)\n",
    "\n",
    "x = nb.uniform((32, 32))\n",
    "y = nb.uniform((32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b4b58",
   "metadata": {},
   "source": [
    "### Benchmarking Eager vs Compiled\n",
    "\n",
    "The first compiled call is warmup. Subsequent calls reuse cached compiled execution for matching input metadata.\n",
    "\n",
    "For this notebook, treat speedup as reduced Python overhead in the step loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4bfee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager:    0.0154s\n",
      "Compiled: 0.0042s\n",
      "Speedup:  3.7x\n"
     ]
    }
   ],
   "source": [
    "# Warmup compiled version (first call traces and compiles)\n",
    "_ = fast_fn(x, y)\n",
    "\n",
    "# Benchmark eager\n",
    "start = time.perf_counter()\n",
    "for _ in range(20):\n",
    "    _ = slow_fn(x, y)\n",
    "eager_time = time.perf_counter() - start\n",
    "\n",
    "# Benchmark compiled\n",
    "start = time.perf_counter()\n",
    "for _ in range(20):\n",
    "    _ = fast_fn(x, y)\n",
    "compiled_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Eager:    {eager_time:.4f}s\")\n",
    "print(f\"Compiled: {compiled_time:.4f}s\")\n",
    "print(f\"Speedup:  {eager_time / max(compiled_time, 1e-9):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477278ad",
   "metadata": {},
   "source": [
    "## 6. Compiled Training Loop\n",
    "\n",
    "`@nb.compile` also applies to full training steps.\n",
    "\n",
    "In this notebook, any compiled-vs-eager speed difference should be read as Python-overhead reduction during repeated training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d38c5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyMLP(nb.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nb.nn.Linear(4, 16)\n",
    "        self.fc2 = nb.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(nb.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabf249",
   "metadata": {},
   "source": [
    "### Compiling the Full Training Step\n",
    "\n",
    "Here we compile one full step (`forward + backward + optimizer update`) and compare it to eager execution with the same math.\n",
    "\n",
    "Speedup in this comparison is interpreted as reduced Python overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c81dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(model, x, y):\n",
    "    return nb.nn.functional.mse_loss(model(x), y)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def train_step(model, opt_state, x, y):\n",
    "    \"\"\"Compiled training step: forward + backward + optimizer update.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(my_loss_fn, argnums=0)(model, x, y)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "def eager_train_step(model, opt_state, x, y):\n",
    "    \"\"\"Same logic without @nb.compile — serves as the eager baseline.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(my_loss_fn, argnums=0)(model, x, y)\n",
    "    model, opt_state = nb.nn.optim.adamw_update(\n",
    "        model, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45e8d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled training loop:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       0.937205    \n",
      "20       0.886352    \n",
      "30       0.833265    \n",
      "40       0.794533    \n",
      "50       0.760996    \n",
      "60       0.724098    \n",
      "\n",
      "Compiled cache stats: CompilationStats(hits=60, misses=1, fallbacks=0, hit_rate=98.4%)\n",
      "Compiled avg step:   0.919 ms/step\n",
      "Eager avg step:      21.697 ms/step\n",
      "Compiled speedup:    23.6x\n"
     ]
    }
   ],
   "source": [
    "# Setup data and model\n",
    "np.random.seed(0)\n",
    "X = nb.Tensor.from_dlpack(np.random.randn(100, 4).astype(np.float32))\n",
    "y = nb.Tensor.from_dlpack(np.random.randn(100, 1).astype(np.float32))\n",
    "\n",
    "num_steps = 60\n",
    "\n",
    "# --- Compiled ---\n",
    "model = TinyMLP()\n",
    "opt_state = nb.nn.optim.adamw_init(model)\n",
    "\n",
    "# First call is warmup (trace + compile)\n",
    "model, opt_state, _ = train_step(model, opt_state, X, y)\n",
    "\n",
    "print(f\"Compiled training loop:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "train_start = time.perf_counter()\n",
    "for step in range(num_steps):\n",
    "    model, opt_state, loss = train_step(model, opt_state, X, y)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")\n",
    "train_elapsed = time.perf_counter() - train_start\n",
    "compiled_step_ms = (train_elapsed / num_steps) * 1000.0\n",
    "\n",
    "print(f\"\\nCompiled cache stats: {train_step.stats}\")\n",
    "print(f\"Compiled avg step:   {compiled_step_ms:.3f} ms/step\")\n",
    "\n",
    "# --- Eager baseline (same math, no @nb.compile) ---\n",
    "model_e = TinyMLP()\n",
    "opt_state_e = nb.nn.optim.adamw_init(model_e)\n",
    "model_e, opt_state_e, _ = eager_train_step(model_e, opt_state_e, X, y)  # warmup\n",
    "\n",
    "eager_start = time.perf_counter()\n",
    "for step in range(num_steps):\n",
    "    model_e, opt_state_e, loss_e = eager_train_step(model_e, opt_state_e, X, y)\n",
    "eager_elapsed = time.perf_counter() - eager_start\n",
    "eager_step_ms = (eager_elapsed / num_steps) * 1000.0\n",
    "\n",
    "speedup = eager_step_ms / max(compiled_step_ms, 1e-9)\n",
    "print(f\"Eager avg step:      {eager_step_ms:.3f} ms/step\")\n",
    "print(f\"Compiled speedup:    {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae5120",
   "metadata": {},
   "source": [
    "## 7. Compiled Training with JAX-Style Params\n",
    "\n",
    "`@nb.compile` works equally well with dict-based parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f91357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nabla.nn.functional import xavier_normal\n",
    "\n",
    "\n",
    "def init_params():\n",
    "    return {\n",
    "        \"w1\": xavier_normal((4, 16)),\n",
    "        \"b1\": nb.zeros((1, 16)),\n",
    "        \"w2\": xavier_normal((16, 1)),\n",
    "        \"b2\": nb.zeros((1, 1)),\n",
    "    }\n",
    "\n",
    "\n",
    "def forward(params, x):\n",
    "    h = nb.relu(x @ params[\"w1\"] + params[\"b1\"])\n",
    "    return h @ params[\"w2\"] + params[\"b2\"]\n",
    "\n",
    "\n",
    "def jax_loss_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    diff = pred - y\n",
    "    return nb.mean(diff * diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cebe89",
   "metadata": {},
   "source": [
    "### Compiled Training with Parameter Dicts\n",
    "\n",
    "Compilation works the same way with JAX-style parameter dicts — the entire\n",
    "`value_and_grad → optimizer update` flow gets fused into one graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b11338dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled JAX-style training:\n",
      "Step     Loss        \n",
      "----------------------\n",
      "10       0.937205    \n",
      "20       0.886352    \n",
      "30       0.833265    \n",
      "40       0.794533    \n",
      "50       0.760996    \n",
      "60       0.724098    \n",
      "\n",
      "Compiled cache stats: CompilationStats(hits=60, misses=1, fallbacks=0, hit_rate=98.4%)\n",
      "Compiled JAX avg step: 0.580 ms/step\n",
      "Eager JAX avg step:    17.940 ms/step\n",
      "Compiled speedup:      30.9x\n"
     ]
    }
   ],
   "source": [
    "@nb.compile\n",
    "def jax_train_step(params, opt_state, x, y):\n",
    "    loss, grads = nb.value_and_grad(jax_loss_fn, argnums=0)(params, x, y)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "def eager_jax_train_step(params, opt_state, x, y):\n",
    "    \"\"\"Same logic without @nb.compile — eager baseline.\"\"\"\n",
    "    loss, grads = nb.value_and_grad(jax_loss_fn, argnums=0)(params, x, y)\n",
    "    params, opt_state = nb.nn.optim.adamw_update(\n",
    "        params, grads, opt_state, lr=1e-2\n",
    "    )\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "# --- Compiled ---\n",
    "params = init_params()\n",
    "opt_state = nb.nn.optim.adamw_init(params)\n",
    "params, opt_state, _ = jax_train_step(params, opt_state, X, y)  # warmup\n",
    "\n",
    "print(f\"Compiled JAX-style training:\")\n",
    "print(f\"{'Step':<8} {'Loss':<12}\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "jax_steps = 60\n",
    "jax_start = time.perf_counter()\n",
    "for step in range(jax_steps):\n",
    "    params, opt_state, loss = jax_train_step(params, opt_state, X, y)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"{step + 1:<8} {loss.item():<12.6f}\")\n",
    "jax_elapsed = time.perf_counter() - jax_start\n",
    "compiled_jax_ms = (jax_elapsed / jax_steps) * 1000.0\n",
    "\n",
    "print(f\"\\nCompiled cache stats: {jax_train_step.stats}\")\n",
    "print(f\"Compiled JAX avg step: {compiled_jax_ms:.3f} ms/step\")\n",
    "\n",
    "# --- Eager baseline ---\n",
    "params_e = init_params()\n",
    "opt_state_e = nb.nn.optim.adamw_init(params_e)\n",
    "params_e, opt_state_e, _ = eager_jax_train_step(params_e, opt_state_e, X, y)\n",
    "\n",
    "eager_jax_start = time.perf_counter()\n",
    "for step in range(jax_steps):\n",
    "    params_e, opt_state_e, loss_e = eager_jax_train_step(params_e, opt_state_e, X, y)\n",
    "eager_jax_elapsed = time.perf_counter() - eager_jax_start\n",
    "eager_jax_ms = (eager_jax_elapsed / jax_steps) * 1000.0\n",
    "\n",
    "speedup = eager_jax_ms / max(compiled_jax_ms, 1e-9)\n",
    "print(f\"Eager JAX avg step:    {eager_jax_ms:.3f} ms/step\")\n",
    "print(f\"Compiled speedup:      {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b8256",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Transform | Usage | Key benefit |\n",
    "|-----------|-------|------------|\n",
    "| `vmap(f)` | Auto-batch any function | No manual batching |\n",
    "| `vmap(grad(f))` | Per-example gradients | Efficient |\n",
    "| `jacrev(f)` / `jacfwd(f)` | Full Jacobians | Compose for Hessians |\n",
    "| `@nb.compile` | Compile train step | Lower Python overhead in repeated loops |\n",
    "\n",
    "All transforms compose freely with each other:\n",
    "`compile(vmap(grad(f)))`, `jacfwd(jacrev(f))`, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
