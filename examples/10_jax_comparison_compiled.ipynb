{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98764fd7",
   "metadata": {},
   "source": [
    "# Example 10: Compiled vs Eager vs JAX\n",
    "\n",
    "This benchmark compares three training modes on the same MLP:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `@nb.compile` | Fused graph execution (fastest) |\n",
    "| Eager (deferred) | Lazy evaluation with `realize_all` |\n",
    "| Eager (MAX graph) | Builds MAX graph each step |\n",
    "\n",
    "The task: fit $f(x) = \\frac{\\sin(8\\pi x) + 1}{2}$ with a 9-layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1331d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nabla as nb\n",
    "\n",
    "# Try to import JAX\n",
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax import grad, jit\n",
    "\n",
    "    HAS_JAX = True\n",
    "except ImportError:\n",
    "    HAS_JAX = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921252b6",
   "metadata": {},
   "source": [
    "## 1. Dataset and Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6a984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 500 samples, fitting (sin(8Ï€Â·x) + 1)/2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "n_steps = 60\n",
    "\n",
    "X_np = np.linspace(0, 1, n_samples).reshape(-1, 1).astype(np.float32)\n",
    "y_np = (np.sin(8 * np.pi * X_np) + 1) / 2.0\n",
    "\n",
    "X = nb.Tensor.from_dlpack(X_np)\n",
    "y = nb.Tensor.from_dlpack(y_np)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, fitting (sin(8Ï€Â·x) + 1)/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab803",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "A 9-layer MLP with Xavier initialization.\n",
    "All three backends will train the same architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c953de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 1 â†’ 16 â†’ 32 â†’ 64 â†’ 64 â†’ 64 â†’ 64 â†’ 32 â†’ 16 â†’ 1 (17793 params)\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 16, 32, 64, 64, 64, 64, 32, 16, 1]\n",
    "\n",
    "# Xavier initialization\n",
    "params = {}\n",
    "for i in range(len(layers) - 1):\n",
    "    in_dim, out_dim = layers[i], layers[i + 1]\n",
    "    limit = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    params[f\"layer{i + 1}\"] = {\n",
    "        \"w\": nb.Tensor.from_dlpack(np.random.uniform(-limit, limit, (in_dim, out_dim)).astype(np.float32)),\n",
    "        \"b\": nb.Tensor.from_dlpack(np.zeros((out_dim,), dtype=np.float32)),\n",
    "    }\n",
    "\n",
    "total_params = sum(layers[i] * layers[i + 1] + layers[i + 1] for i in range(len(layers) - 1))\n",
    "print(f\"Architecture: {' â†’ '.join(map(str, layers))} ({total_params} params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e45918",
   "metadata": {},
   "source": [
    "### Forward, Loss, and Train Steps\n",
    "\n",
    "We define the forward pass, MSE loss, and two training modes:\n",
    "- **Compiled** (`@nb.compile`): cached compiled execution for repeated calls\n",
    "- **Eager**: deferred evaluation with manual `realize_all`\n",
    "\n",
    "Interpretation used in this notebook:\n",
    "- compiled speedups are treated as lower Python overhead in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef40d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(params, x):\n",
    "    h = x\n",
    "    for i in range(1, len(layers)):\n",
    "        h = h @ params[f\"layer{i}\"][\"w\"] + params[f\"layer{i}\"][\"b\"]\n",
    "        if i < len(layers) - 1:\n",
    "            h = nb.relu(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    pred = mlp_forward(params, x)\n",
    "    diff = pred - y\n",
    "    return nb.mean(diff * diff)\n",
    "\n",
    "\n",
    "@nb.compile\n",
    "def train_step_compiled(params, x, y):\n",
    "    loss, grads = nb.value_and_grad(loss_fn)(params, x, y)\n",
    "    lr = 0.01\n",
    "    new_params = {}\n",
    "    for layer_name in params:\n",
    "        new_params[layer_name] = {\n",
    "            \"w\": params[layer_name][\"w\"] - grads[layer_name][\"w\"] * lr,\n",
    "            \"b\": params[layer_name][\"b\"] - grads[layer_name][\"b\"] * lr,\n",
    "        }\n",
    "    return loss, new_params\n",
    "\n",
    "\n",
    "def train_step_eager(params, x, y):\n",
    "    loss, grads = nb.value_and_grad(loss_fn, realize=False)(params, x, y)\n",
    "    lr = 0.01\n",
    "    new_params = {}\n",
    "    for layer_name in params:\n",
    "        new_params[layer_name] = {\n",
    "            \"w\": params[layer_name][\"w\"] - grads[layer_name][\"w\"] * lr,\n",
    "            \"b\": params[layer_name][\"b\"] - grads[layer_name][\"b\"] * lr,\n",
    "        }\n",
    "    # Batch-realize all outputs\n",
    "    all_outputs = [loss]\n",
    "    for lp in new_params.values():\n",
    "        all_outputs.extend(lp.values())\n",
    "    nb.realize_all(*all_outputs)\n",
    "    return loss, new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d6e88",
   "metadata": {},
   "source": [
    "## 2. Nabla Benchmarks (Compiled vs Eager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210a55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 0.364887\n",
      "  Step  50: loss = 0.127728\n",
      "\n",
      "Compiled: 0.0610s (984.3 steps/sec)\n",
      "Loss: 0.337955 â†’ 0.126996\n",
      "Compile stats: CompilationStats(hits=60, misses=1, fallbacks=0, hit_rate=98.4%)\n"
     ]
    }
   ],
   "source": [
    "params_compiled = params\n",
    "\n",
    "# Warmup (triggers compilation)\n",
    "loss, params_compiled = train_step_compiled(params_compiled, X, y)\n",
    "print(f\"Warmup loss: {loss.to_numpy():.6f}\")\n",
    "\n",
    "# Timed run\n",
    "start = time.perf_counter()\n",
    "losses_compiled = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_compiled = train_step_compiled(params_compiled, X, y)\n",
    "    losses_compiled.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "elapsed_compiled = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nCompiled: {elapsed_compiled:.4f}s ({n_steps / elapsed_compiled:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_compiled[0]:.6f} â†’ {losses_compiled[-1]:.6f}\")\n",
    "print(f\"Compile stats: {train_step_compiled.stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71bd5",
   "metadata": {},
   "source": [
    "### Eager (Deferred Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36f70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 0.364887\n",
      "  Step  50: loss = 0.127728\n",
      "\n",
      "Eager: 1.3166s (45.6 steps/sec)\n",
      "Loss: 0.337955 â†’ 0.126996\n"
     ]
    }
   ],
   "source": [
    "params_eager = params\n",
    "\n",
    "loss, params_eager = train_step_eager(params_eager, X, y)\n",
    "print(f\"Warmup loss: {loss.to_numpy():.6f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "losses_eager = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_eager = train_step_eager(params_eager, X, y)\n",
    "    losses_eager.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "elapsed_eager = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nEager: {elapsed_eager:.4f}s ({n_steps / elapsed_eager:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_eager[0]:.6f} â†’ {losses_eager[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f48ad",
   "metadata": {},
   "source": [
    "### Eager (MAX Graph Mode)\n",
    "\n",
    "`EAGER_MAX_GRAPH=True` builds a MAX execution graph for every step.\n",
    "This is typically slower than deferred but avoids Python-level overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e29106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 0.364887\n",
      "  Step  50: loss = 0.127728\n",
      "\n",
      "Eager MAX: 4.7352s (12.7 steps/sec)\n",
      "Loss: 0.337955 â†’ 0.126996\n"
     ]
    }
   ],
   "source": [
    "import nabla.config as nabla_config\n",
    "orig_eager_max = nabla_config.EAGER_MAX_GRAPH\n",
    "nabla_config.EAGER_MAX_GRAPH = True\n",
    "\n",
    "params_eager_max = params\n",
    "\n",
    "loss, params_eager_max = train_step_eager(params_eager_max, X, y)\n",
    "print(f\"Warmup loss: {loss.to_numpy():.6f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "losses_eager_max = []\n",
    "for i in range(n_steps):\n",
    "    loss, params_eager_max = train_step_eager(params_eager_max, X, y)\n",
    "    losses_eager_max.append(float(loss.to_numpy()))\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Step {i + 1:3d}: loss = {loss.to_numpy():.6f}\")\n",
    "elapsed_eager_max = time.perf_counter() - start\n",
    "\n",
    "nabla_config.EAGER_MAX_GRAPH = orig_eager_max  # restore\n",
    "print(f\"\\nEager MAX: {elapsed_eager_max:.4f}s ({n_steps / elapsed_eager_max:.1f} steps/sec)\")\n",
    "print(f\"Loss: {losses_eager_max[0]:.6f} â†’ {losses_eager_max[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419e38f",
   "metadata": {},
   "source": [
    "## 3. JAX `@jit` Comparison (Optional)\n",
    "\n",
    "If JAX is installed, we run the same MLP training with `@jax.jit` for a\n",
    "direct performance comparison. The architecture and hyperparameters are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e3cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX warmup loss: 0.364887\n",
      "  Step  50: loss = 0.127728\n",
      "\n",
      "JAX JIT: 0.0463s (1294.7 steps/sec)\n",
      "Loss: 0.337955 â†’ 0.126996\n"
     ]
    }
   ],
   "source": [
    "if HAS_JAX:\n",
    "    # Convert params to flat list for JAX\n",
    "    jax_params = []\n",
    "    for layer_name in sorted(params.keys()):\n",
    "        jax_params.append(jnp.array(params[layer_name][\"w\"].to_numpy()))\n",
    "        jax_params.append(jnp.array(params[layer_name][\"b\"].to_numpy()))\n",
    "    X_jax, y_jax = jnp.array(X_np), jnp.array(y_np)\n",
    "\n",
    "    def jax_mlp(params_flat, x):\n",
    "        h = x\n",
    "        for i in range(0, len(params_flat) - 2, 2):\n",
    "            h = h @ params_flat[i] + params_flat[i + 1]\n",
    "            h = jax.nn.relu(h)\n",
    "        return h @ params_flat[-2] + params_flat[-1]\n",
    "\n",
    "    def jax_loss(params_flat, x, y):\n",
    "        return jnp.mean((jax_mlp(params_flat, x) - y) ** 2)\n",
    "\n",
    "    @jit\n",
    "    def jax_train_step(params_flat, x, y):\n",
    "        loss = jax_loss(params_flat, x, y)\n",
    "        grads = grad(jax_loss)(params_flat, x, y)\n",
    "        return loss, [p - g * 0.01 for p, g in zip(params_flat, grads, strict=False)]\n",
    "\n",
    "    # Warmup\n",
    "    loss_jax, jax_params = jax_train_step(jax_params, X_jax, y_jax)\n",
    "    jax.block_until_ready(loss_jax)\n",
    "    print(f\"JAX warmup loss: {float(loss_jax):.6f}\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    losses_jax = []\n",
    "    for i in range(n_steps):\n",
    "        loss_jax, jax_params = jax_train_step(jax_params, X_jax, y_jax)\n",
    "        jax.block_until_ready(loss_jax)\n",
    "        losses_jax.append(float(loss_jax))\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Step {i + 1:3d}: loss = {float(loss_jax):.6f}\")\n",
    "    elapsed_jax = time.perf_counter() - start\n",
    "\n",
    "    print(f\"\\nJAX JIT: {elapsed_jax:.4f}s ({n_steps / elapsed_jax:.1f} steps/sec)\")\n",
    "    print(f\"Loss: {losses_jax[0]:.6f} â†’ {losses_jax[-1]:.6f}\")\n",
    "else:\n",
    "    print(\"JAX not installed â€” skipping JAX benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db800c2",
   "metadata": {},
   "source": [
    "## 4. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f09fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Nabla @nb.compile:  0.0610s  (984.3 steps/sec)\n",
      "Nabla Eager:        1.3166s  (45.6 steps/sec)\n",
      "Nabla Eager (MAX):  4.7352s  (12.7 steps/sec)\n",
      "JAX @jit:           0.0463s  (1294.7 steps/sec)\n",
      "\n",
      "JAX JIT is 1.32x faster than Nabla compiled\n",
      "\n",
      "Compile speedup over eager: 21.60x\n",
      "Loss match (compiled vs eager): âœ… diff=0.00000000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nabla @nb.compile:  {elapsed_compiled:.4f}s  ({n_steps / elapsed_compiled:.1f} steps/sec)\")\n",
    "print(f\"Nabla Eager:        {elapsed_eager:.4f}s  ({n_steps / elapsed_eager:.1f} steps/sec)\")\n",
    "print(f\"Nabla Eager (MAX):  {elapsed_eager_max:.4f}s  ({n_steps / elapsed_eager_max:.1f} steps/sec)\")\n",
    "\n",
    "if HAS_JAX:\n",
    "    print(f\"JAX @jit:           {elapsed_jax:.4f}s  ({n_steps / elapsed_jax:.1f} steps/sec)\")\n",
    "    speedup_vs_jax = elapsed_jax / elapsed_compiled\n",
    "    if speedup_vs_jax > 1:\n",
    "        print(f\"\\nðŸš€ Nabla compiled is {speedup_vs_jax:.2f}x faster than JAX JIT\")\n",
    "    else:\n",
    "        print(f\"\\nJAX JIT is {1 / speedup_vs_jax:.2f}x faster than Nabla compiled\")\n",
    "\n",
    "speedup = elapsed_eager / elapsed_compiled\n",
    "print(f\"\\nCompile speedup over eager: {speedup:.2f}x\")\n",
    "\n",
    "# Verify correctness across modes\n",
    "loss_diff = abs(losses_compiled[-1] - losses_eager[-1])\n",
    "print(f\"Loss match (compiled vs eager): {'âœ…' if loss_diff < 1e-4 else 'âš ï¸'} diff={loss_diff:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee19e0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key takeaways:**\n",
    "- `@nb.compile` reuses cached compiled execution when input metadata matches\n",
    "- Eager mode is slower here because of Python overhead in the per-step loop\n",
    "- `EAGER_MAX_GRAPH` mode builds a MAX graph each step â€” useful for debugging\n",
    "- All three modes produce numerically identical results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eed7d",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13816fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "âœ“ MLP training works with compile!\n",
      "âœ“ Full pytree parameters (weights + biases) work correctly\n",
      "âœ“ Loss decreases properly: 0.337955 -> 0.126996\n",
      "âœ“ 21.60x speedup from compilation\n",
      "âœ“ Cache hit rate: 98.4%\n",
      "âœ“ Compared against JAX JIT successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ MLP training works with compile!\")\n",
    "print(\"âœ“ Full pytree parameters (weights + biases) work correctly\")\n",
    "print(\n",
    "    f\"âœ“ Loss decreases properly: {losses_compiled[0]:.6f} -> {losses_compiled[-1]:.6f}\"\n",
    ")\n",
    "print(f\"âœ“ {speedup:.2f}x speedup from compilation\")\n",
    "print(f\"âœ“ Cache hit rate: {train_step_compiled.stats.hit_rate:.1f}%\")\n",
    "if HAS_JAX:\n",
    "    print(\"âœ“ Compared against JAX JIT successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
