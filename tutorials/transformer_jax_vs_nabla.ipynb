{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX vs. Nabla: Training a Transformer\n",
    "\n",
    "This notebook provides a detailed, from-scratch implementation of a Transformer model for a sequence-reversal task. The goal is to compare the APIs and programming models of two deep learning frameworks: **JAX** and **Nabla**.\n",
    "\n",
    "Each core component of the Transformer is implemented for both frameworks in the same code cell, allowing for a direct comparison of their syntax and approach. The implementations are kept as architecturally similar as possible to highlight the differences in the libraries themselves.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental building blocks of the Transformer architecture.\n",
    "- Compare the functional, JIT-centric approaches of JAX and Nabla.\n",
    "- See how a complete sequence-to-sequence model is built and trained in both frameworks.\n",
    "\n",
    "At the end of the notebook, you will find two separate code cells that run the complete training loops, one for JAX and one for Nabla, followed by visualization of the training loss curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "First, we import the necessary libraries and define the configuration parameters for our model and training task. These parameters are identical for both the JAX and Nabla implementations to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.3' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Installation and imports\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "try:\n",
    "    import time\n",
    "\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    import nabla as nb\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "\n",
    "    # Install required packages\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"jax\",\n",
    "            \"jaxlib\",\n",
    "            \"matplotlib\",\n",
    "            \"numpy\",\n",
    "            \"nabla-ml\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    # Re-import after installation\n",
    "    import time\n",
    "\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    import nabla as nb\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION (Shared for both JAX and Nabla)\n",
    "# ============================================================================\n",
    "\n",
    "# Task Configuration\n",
    "VOCAB_SIZE = 20  # Total vocabulary size (0=PAD, 1=START, 2=END, 3-19=content)\n",
    "SOURCE_SEQ_LEN = 9  # Length of input sequences to reverse\n",
    "TARGET_SEQ_LEN = SOURCE_SEQ_LEN + 2  # +1 for END token, +1 for START token in decoder\n",
    "MAX_SEQ_LEN = TARGET_SEQ_LEN\n",
    "\n",
    "# Model Architecture\n",
    "NUM_LAYERS = 2  # Number of encoder and decoder layers\n",
    "D_MODEL = 64  # Model dimension (embedding size)\n",
    "NUM_HEADS = 4  # Number of attention heads (must divide D_MODEL)\n",
    "D_FF = 128  # Feed-forward network hidden dimension\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 128  # Number of sequences per training batch\n",
    "LEARNING_RATE = 0.0005  # AdamW learning rate\n",
    "NUM_EPOCHS = 100  # Total training epochs\n",
    "PRINT_INTERVAL = 10  # Print progress every N epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding\n",
    "\n",
    "Since Transformers do not have inherent knowledge of sequence order (unlike RNNs), we inject positional information using sinusoidal positional encodings. These are fixed (non-learned) vectors added to the input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_jax(max_seq_len: int, d_model: int) -> jnp.ndarray:\n",
    "    \"\"\"Create sinusoidal positional encodings for JAX.\"\"\"\n",
    "    position = jnp.arange(max_seq_len).reshape((max_seq_len, 1))\n",
    "    half_d_model = d_model // 2\n",
    "    dim_indices = jnp.arange(half_d_model).reshape((1, half_d_model))\n",
    "    scaling_factors = 10000.0 ** (2.0 * dim_indices / d_model)\n",
    "    angles = position / scaling_factors\n",
    "    sin_vals = jnp.sin(angles)\n",
    "    cos_vals = jnp.cos(angles)\n",
    "    stacked = jnp.stack([sin_vals, cos_vals], axis=2)\n",
    "    pe = stacked.reshape((max_seq_len, d_model))\n",
    "    return pe.reshape((1, max_seq_len, d_model))\n",
    "\n",
    "\n",
    "def positional_encoding_nabla(max_seq_len: int, d_model: int) -> nb.Array:\n",
    "    \"\"\"Create sinusoidal positional encodings for Nabla.\"\"\"\n",
    "    position = nb.ndarange((max_seq_len,)).reshape((max_seq_len, 1))\n",
    "    half_d_model = d_model // 2\n",
    "    dim_indices = nb.ndarange((half_d_model,)).reshape((1, half_d_model))\n",
    "    scaling_factors = 10000.0 ** (2.0 * dim_indices / d_model)\n",
    "    angles = position / scaling_factors\n",
    "    sin_vals = nb.sin(angles)\n",
    "    cos_vals = nb.cos(angles)\n",
    "    stacked = nb.stack([sin_vals, cos_vals], axis=2)\n",
    "    pe = stacked.reshape((max_seq_len, d_model))\n",
    "    return pe.reshape((1, max_seq_len, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "This is the core mechanism of the Transformer. It computes attention scores by taking the dot product of a query vector with all key vectors, scaling the result, applying a softmax, and then using these weights to create a weighted sum of the value vectors.\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_jax(q, k, v, mask=None):\n",
    "    \"\"\"Scaled dot-product attention for JAX.\"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(\n",
    "        jnp.array([d_k], dtype=jnp.float32)\n",
    "    )\n",
    "    if mask is not None:\n",
    "        scores = jnp.where(mask == 0, -1e9, scores)\n",
    "    attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "    output = jnp.matmul(attention_weights, v)\n",
    "    return output\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention_nabla(q, k, v, mask=None):\n",
    "    \"\"\"Scaled dot-product attention for Nabla.\"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = nb.matmul(q, k.permute((0, 1, 3, 2))) / nb.sqrt(\n",
    "        nb.array([d_k], dtype=nb.DType.float32)\n",
    "    )\n",
    "    if mask is not None:\n",
    "        scores = nb.where(mask, scores, nb.full_like(scores, -1e9))\n",
    "    attention_weights = nb.softmax(scores, axis=-1)\n",
    "    output = nb.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, we project the queries, keys, and values into multiple lower-dimensional spaces (\"heads\"). Attention is computed in parallel for each head, and the results are concatenated and projected back to the original dimension. This allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_jax(x, xa, params, mask=None):\n",
    "    \"\"\"Multi-head attention for JAX.\"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    d_head = d_model // NUM_HEADS\n",
    "\n",
    "    q_linear = jnp.matmul(x, params[\"w_q\"])\n",
    "    k_linear = jnp.matmul(xa, params[\"w_k\"])\n",
    "    v_linear = jnp.matmul(xa, params[\"w_v\"])\n",
    "\n",
    "    q = q_linear.reshape(batch_size, seq_len, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "    k = k_linear.reshape(batch_size, -1, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "    v = v_linear.reshape(batch_size, -1, NUM_HEADS, d_head).transpose((0, 2, 1, 3))\n",
    "\n",
    "    attention_output = scaled_dot_product_attention_jax(q, k, v, mask)\n",
    "\n",
    "    attention_output = attention_output.transpose((0, 2, 1, 3)).reshape(\n",
    "        batch_size, seq_len, d_model\n",
    "    )\n",
    "    return jnp.matmul(attention_output, params[\"w_o\"])\n",
    "\n",
    "\n",
    "def multi_head_attention_nabla(x, xa, params, mask=None):\n",
    "    \"\"\"Multi-head attention for Nabla.\"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    d_head = d_model // NUM_HEADS\n",
    "\n",
    "    q_linear = nb.matmul(x, params[\"w_q\"])\n",
    "    k_linear = nb.matmul(xa, params[\"w_k\"])\n",
    "    v_linear = nb.matmul(xa, params[\"w_v\"])\n",
    "\n",
    "    q = q_linear.reshape((batch_size, seq_len, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "    k = k_linear.reshape((batch_size, -1, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "    v = v_linear.reshape((batch_size, -1, NUM_HEADS, d_head)).permute((0, 2, 1, 3))\n",
    "\n",
    "    attention_output = scaled_dot_product_attention_nabla(q, k, v, mask)\n",
    "\n",
    "    attention_output = attention_output.permute((0, 2, 1, 3)).reshape(\n",
    "        (batch_size, seq_len, d_model)\n",
    "    )\n",
    "    return nb.matmul(attention_output, params[\"w_o\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Position-wise Feed-Forward Network\n",
    "\n",
    "Each encoder and decoder layer contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_jax(x, params):\n",
    "    \"\"\"Position-wise feed-forward network for JAX.\"\"\"\n",
    "    hidden = jax.nn.relu(jnp.matmul(x, params[\"w1\"]) + params[\"b1\"])\n",
    "    output = jnp.matmul(hidden, params[\"w2\"]) + params[\"b2\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "def feed_forward_nabla(x, params):\n",
    "    \"\"\"Position-wise feed-forward network for Nabla.\"\"\"\n",
    "    hidden = nb.relu(nb.matmul(x, params[\"w1\"]) + params[\"b1\"])\n",
    "    output = nb.matmul(hidden, params[\"w2\"]) + params[\"b2\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer Normalization\n",
    "\n",
    "Layer Normalization is used to stabilize the network and speed up training. We use a \"Pre-Norm\" architecture, where normalization is applied *before* each sub-layer (attention and FFN), followed by a residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_jax(x, params, eps=1e-6):\n",
    "    \"\"\"Layer normalization for JAX.\"\"\"\n",
    "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "    variance = jnp.mean((x - mean) * (x - mean), axis=-1, keepdims=True)\n",
    "    normalized = (x - mean) / jnp.sqrt(variance + eps)\n",
    "    return params[\"gamma\"] * normalized + params[\"beta\"]\n",
    "\n",
    "\n",
    "def layer_norm_nabla(x, params, eps=1e-6):\n",
    "    \"\"\"Layer normalization for Nabla.\"\"\"\n",
    "    mean = nb.mean(x, axes=[-1], keep_dims=True)\n",
    "    variance = nb.mean((x - mean) * (x - mean), axes=[-1], keep_dims=True)\n",
    "    normalized = (x - mean) / nb.sqrt(variance + eps)\n",
    "    return params[\"gamma\"] * normalized + params[\"beta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encoder and Decoder Layers\n",
    "\n",
    "We now assemble the building blocks into complete Encoder and Decoder layers.\n",
    "\n",
    "- **Encoder Layer**: Contains a self-attention mechanism and a feed-forward network.\n",
    "- **Decoder Layer**: Contains a masked self-attention mechanism, a cross-attention mechanism (attending to the encoder's output), and a feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Layer Implementations ---\n",
    "def encoder_layer_jax(x, params, mask):\n",
    "    norm_x = layer_norm_jax(x, params[\"norm1\"])\n",
    "    attention_output = multi_head_attention_jax(norm_x, norm_x, params[\"mha\"], mask)\n",
    "    x = x + attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm2\"])\n",
    "    ffn_output = feed_forward_jax(norm_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder_layer_jax(x, encoder_output, params, look_ahead_mask, padding_mask):\n",
    "    norm_x = layer_norm_jax(x, params[\"norm1\"])\n",
    "    masked_attention_output = multi_head_attention_jax(\n",
    "        norm_x, norm_x, params[\"masked_mha\"], look_ahead_mask\n",
    "    )\n",
    "    x = x + masked_attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm2\"])\n",
    "    cross_attention_output = multi_head_attention_jax(\n",
    "        norm_x, encoder_output, params[\"cross_mha\"], padding_mask\n",
    "    )\n",
    "    x = x + cross_attention_output\n",
    "\n",
    "    norm_x = layer_norm_jax(x, params[\"norm3\"])\n",
    "    ffn_output = feed_forward_jax(norm_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "# --- Nabla Layer Implementations ---\n",
    "def encoder_layer_nabla(x, params, mask):\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm1\"])\n",
    "    attention_output = multi_head_attention_nabla(\n",
    "        normed_x, normed_x, params[\"mha\"], mask\n",
    "    )\n",
    "    x = x + attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm2\"])\n",
    "    ffn_output = feed_forward_nabla(normed_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder_layer_nabla(x, encoder_output, params, look_ahead_mask, padding_mask):\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm1\"])\n",
    "    masked_attention_output = multi_head_attention_nabla(\n",
    "        normed_x, normed_x, params[\"masked_mha\"], look_ahead_mask\n",
    "    )\n",
    "    x = x + masked_attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm2\"])\n",
    "    cross_attention_output = multi_head_attention_nabla(\n",
    "        normed_x, encoder_output, params[\"cross_mha\"], padding_mask\n",
    "    )\n",
    "    x = x + cross_attention_output\n",
    "\n",
    "    normed_x = layer_norm_nabla(x, params[\"norm3\"])\n",
    "    ffn_output = feed_forward_nabla(normed_x, params[\"ffn\"])\n",
    "    x = x + ffn_output\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Embedding Lookup\n",
    "\n",
    "Standard libraries provide a simple `embedding[ids]` lookup. To maintain a \"from-scratch\" feel and ensure a direct comparison, we implement this lookup manually using `where` operations. This converts integer token IDs into their corresponding dense vector representations from an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup_jax(token_ids, embedding_matrix):\n",
    "    \"\"\"Manual embedding lookup for JAX.\"\"\"\n",
    "    return embedding_matrix[token_ids]\n",
    "\n",
    "\n",
    "def embedding_lookup_nabla(token_ids, embedding_matrix):\n",
    "    \"\"\"Manual embedding lookup for Nabla.\"\"\"\n",
    "    batch_size, seq_len = token_ids.shape\n",
    "    vocab_size, d_model = embedding_matrix.shape\n",
    "    output = nb.zeros((batch_size, seq_len, d_model))\n",
    "    for token_idx in range(vocab_size):\n",
    "        token_idx_array = nb.array([token_idx], dtype=nb.DType.int32)\n",
    "        condition = nb.equal(\n",
    "            token_ids, nb.broadcast_to(token_idx_array, token_ids.shape)\n",
    "        )\n",
    "        condition_expanded = nb.broadcast_to(\n",
    "            condition.reshape((batch_size, seq_len, 1)), (batch_size, seq_len, d_model)\n",
    "        )\n",
    "        token_embedding = embedding_matrix[token_idx : token_idx + 1, :].reshape(\n",
    "            (1, 1, d_model)\n",
    "        )\n",
    "        token_embedding_expanded = nb.broadcast_to(\n",
    "            token_embedding, (batch_size, seq_len, d_model)\n",
    "        )\n",
    "        output = nb.where(condition_expanded, token_embedding_expanded, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full Transformer Forward Pass\n",
    "\n",
    "Here, we combine all the preceding components into the complete encoder-decoder forward pass. This function takes the source and target token sequences, processes them through the respective stacks of layers, and produces the final output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Full Forward Pass ---\n",
    "def transformer_forward_jax(encoder_inputs, decoder_inputs, params):\n",
    "    target_seq_len = decoder_inputs.shape[1]\n",
    "    positions = jnp.arange(target_seq_len)\n",
    "    causal_mask = positions.reshape((target_seq_len, 1)) >= positions.reshape(\n",
    "        (1, target_seq_len)\n",
    "    )\n",
    "    look_ahead_mask = causal_mask.reshape((1, 1, target_seq_len, target_seq_len))\n",
    "\n",
    "    encoder_seq_len = encoder_inputs.shape[1]\n",
    "    decoder_seq_len = decoder_inputs.shape[1]\n",
    "\n",
    "    encoder_embeddings = embedding_lookup_jax(\n",
    "        encoder_inputs, params[\"encoder\"][\"embedding\"]\n",
    "    )\n",
    "    encoder_pos_enc = params[\"pos_encoding\"][:, :encoder_seq_len, :]\n",
    "    encoder_x = encoder_embeddings + encoder_pos_enc\n",
    "\n",
    "    encoder_output = encoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        encoder_output = encoder_layer_jax(\n",
    "            encoder_output, params[\"encoder\"][f\"layer_{i}\"], mask=None\n",
    "        )\n",
    "    encoder_output = layer_norm_jax(encoder_output, params[\"encoder\"][\"final_norm\"])\n",
    "\n",
    "    decoder_embeddings = embedding_lookup_jax(\n",
    "        decoder_inputs, params[\"decoder\"][\"embedding\"]\n",
    "    )\n",
    "    decoder_pos_enc = params[\"pos_encoding\"][:, :decoder_seq_len, :]\n",
    "    decoder_x = decoder_embeddings + decoder_pos_enc\n",
    "\n",
    "    decoder_output = decoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        decoder_output = decoder_layer_jax(\n",
    "            decoder_output,\n",
    "            encoder_output,\n",
    "            params[\"decoder\"][f\"layer_{i}\"],\n",
    "            look_ahead_mask,\n",
    "            padding_mask=None,\n",
    "        )\n",
    "    decoder_output = layer_norm_jax(decoder_output, params[\"decoder\"][\"final_norm\"])\n",
    "\n",
    "    logits = jnp.matmul(decoder_output, params[\"output_linear\"])\n",
    "    return logits\n",
    "\n",
    "\n",
    "# --- Nabla Full Forward Pass ---\n",
    "def transformer_forward_nabla(encoder_inputs, decoder_inputs, params):\n",
    "    target_seq_len = decoder_inputs.shape[1]\n",
    "    positions = nb.ndarange((target_seq_len,))\n",
    "    causal_mask = nb.greater_equal(\n",
    "        nb.reshape(positions, (target_seq_len, 1)),\n",
    "        nb.reshape(positions, (1, target_seq_len)),\n",
    "    )\n",
    "    look_ahead_mask = nb.reshape(causal_mask, (1, 1, target_seq_len, target_seq_len))\n",
    "\n",
    "    encoder_seq_len = encoder_inputs.shape[1]\n",
    "    decoder_seq_len = decoder_inputs.shape[1]\n",
    "\n",
    "    encoder_embeddings = embedding_lookup_nabla(\n",
    "        encoder_inputs, params[\"encoder\"][\"embedding\"]\n",
    "    )\n",
    "    encoder_pos_enc = params[\"pos_encoding\"][:, :encoder_seq_len, :]\n",
    "    encoder_x = encoder_embeddings + encoder_pos_enc\n",
    "\n",
    "    encoder_output = encoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        encoder_output = encoder_layer_nabla(\n",
    "            encoder_output, params[\"encoder\"][f\"layer_{i}\"], mask=None\n",
    "        )\n",
    "    encoder_output = layer_norm_nabla(encoder_output, params[\"encoder\"][\"final_norm\"])\n",
    "\n",
    "    decoder_embeddings = embedding_lookup_nabla(\n",
    "        decoder_inputs, params[\"decoder\"][\"embedding\"]\n",
    "    )\n",
    "    decoder_pos_enc = params[\"pos_encoding\"][:, :decoder_seq_len, :]\n",
    "    decoder_x = decoder_embeddings + decoder_pos_enc\n",
    "\n",
    "    decoder_output = decoder_x\n",
    "    for i in range(NUM_LAYERS):\n",
    "        decoder_output = decoder_layer_nabla(\n",
    "            decoder_output,\n",
    "            encoder_output,\n",
    "            params[\"decoder\"][f\"layer_{i}\"],\n",
    "            look_ahead_mask,\n",
    "            padding_mask=None,\n",
    "        )\n",
    "    decoder_output = layer_norm_nabla(decoder_output, params[\"decoder\"][\"final_norm\"])\n",
    "\n",
    "    logits = nb.matmul(decoder_output, params[\"output_linear\"])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loss Function\n",
    "\n",
    "We use the standard cross-entropy loss to train our model. To keep the implementations comparable, we manually create one-hot encoded targets from the integer labels and then compute the loss against the model's log-softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_log_softmax_jax(x, axis=-1):\n",
    "    \"\"\"Manual log softmax implementation for JAX.\"\"\"\n",
    "    x_max = jnp.max(x, axis=axis, keepdims=True)\n",
    "    x_shifted = x - x_max\n",
    "    log_sum_exp = jnp.log(jnp.sum(jnp.exp(x_shifted), axis=axis, keepdims=True))\n",
    "    return x_shifted - log_sum_exp\n",
    "\n",
    "\n",
    "def manual_log_softmax_nabla(x, axis=-1):\n",
    "    \"\"\"Manual log softmax implementation for Nabla.\"\"\"\n",
    "    x_max = nb.max(x, axes=[axis], keep_dims=True)\n",
    "    x_shifted = x - x_max\n",
    "    log_sum_exp = nb.log(nb.sum(nb.exp(x_shifted), axes=[axis], keep_dims=True))\n",
    "    return x_shifted - log_sum_exp\n",
    "\n",
    "\n",
    "def cross_entropy_loss_jax(logits, targets):\n",
    "    \"\"\"Cross-entropy loss for JAX.\"\"\"\n",
    "    batch_size, seq_len = targets.shape\n",
    "    vocab_size = logits.shape[-1]\n",
    "    targets_expanded = jnp.expand_dims(targets, -1)\n",
    "    vocab_indices = jnp.arange(vocab_size, dtype=jnp.int32).reshape((1, 1, vocab_size))\n",
    "    one_hot_targets = jnp.equal(targets_expanded, vocab_indices).astype(jnp.float32)\n",
    "    log_probs = manual_log_softmax_jax(logits)\n",
    "    cross_entropy = -jnp.sum(one_hot_targets * log_probs)\n",
    "    return cross_entropy / batch_size\n",
    "\n",
    "\n",
    "def cross_entropy_loss_nabla(logits, targets):\n",
    "    \"\"\"Cross-entropy loss for Nabla.\"\"\"\n",
    "    batch_size, seq_len = targets.shape\n",
    "    vocab_size = logits.shape[-1]\n",
    "    targets_expanded = targets.reshape((batch_size, seq_len, 1))\n",
    "    vocab_indices = nb.ndarange((vocab_size,), dtype=nb.DType.int32).reshape(\n",
    "        (1, 1, vocab_size)\n",
    "    )\n",
    "    one_hot_targets = nb.equal(targets_expanded, vocab_indices).astype(nb.DType.float32)\n",
    "    log_probs = manual_log_softmax_nabla(logits)\n",
    "    cross_entropy = -nb.sum(one_hot_targets * log_probs)\n",
    "    return cross_entropy / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Parameter Initialization\n",
    "\n",
    "We initialize the model's weights and biases. Linear layer weights are initialized using Glorot (Xavier) uniform initialization, while biases and normalization parameters are initialized to zeros and ones, respectively. Embeddings are initialized from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Parameter Initialization ---\n",
    "def _init_encoder_layer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": glorot((D_MODEL, D_FF)),\n",
    "            \"b1\": jnp.zeros(D_FF),\n",
    "            \"w2\": glorot((D_FF, D_MODEL)),\n",
    "            \"b2\": jnp.zeros(D_MODEL),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm2\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "    }\n",
    "\n",
    "\n",
    "def _init_decoder_layer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"masked_mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"cross_mha\": {\n",
    "            \"w_q\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": glorot((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": glorot((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": glorot((D_MODEL, D_FF)),\n",
    "            \"b1\": jnp.zeros(D_FF),\n",
    "            \"w2\": glorot((D_FF, D_MODEL)),\n",
    "            \"b2\": jnp.zeros(D_MODEL),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm2\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "        \"norm3\": {\"gamma\": jnp.ones(D_MODEL), \"beta\": jnp.zeros(D_MODEL)},\n",
    "    }\n",
    "\n",
    "\n",
    "def init_transformer_params_jax():\n",
    "    def glorot(shape):\n",
    "        return jax.nn.initializers.glorot_uniform()(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    def randn(shape):\n",
    "        return jax.random.normal(\n",
    "            jax.random.PRNGKey(int(np.random.randint(0, 1000000))), shape\n",
    "        )\n",
    "\n",
    "    params: dict[str, Any] = {\"encoder\": {}, \"decoder\": {}}\n",
    "    params[\"encoder\"][\"embedding\"] = randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"decoder\"][\"embedding\"] = randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"pos_encoding\"] = positional_encoding_jax(MAX_SEQ_LEN, D_MODEL)\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"encoder\"][f\"layer_{i}\"] = _init_encoder_layer_params_jax()\n",
    "    params[\"encoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": jnp.ones(D_MODEL),\n",
    "        \"beta\": jnp.zeros(D_MODEL),\n",
    "    }\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"decoder\"][f\"layer_{i}\"] = _init_decoder_layer_params_jax()\n",
    "    params[\"decoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": jnp.ones(D_MODEL),\n",
    "        \"beta\": jnp.zeros(D_MODEL),\n",
    "    }\n",
    "    params[\"output_linear\"] = glorot((D_MODEL, VOCAB_SIZE))\n",
    "    return params\n",
    "\n",
    "\n",
    "# --- Nabla Parameter Initialization ---\n",
    "def _init_encoder_layer_params_nabla():\n",
    "    return {\n",
    "        \"mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": nb.glorot_uniform((D_MODEL, D_FF)),\n",
    "            \"b1\": nb.zeros((D_FF,)),\n",
    "            \"w2\": nb.glorot_uniform((D_FF, D_MODEL)),\n",
    "            \"b2\": nb.zeros((D_MODEL,)),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm2\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "    }\n",
    "\n",
    "\n",
    "def _init_decoder_layer_params_nabla():\n",
    "    return {\n",
    "        \"masked_mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"cross_mha\": {\n",
    "            \"w_q\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_k\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_v\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "            \"w_o\": nb.glorot_uniform((D_MODEL, D_MODEL)),\n",
    "        },\n",
    "        \"ffn\": {\n",
    "            \"w1\": nb.glorot_uniform((D_MODEL, D_FF)),\n",
    "            \"b1\": nb.zeros((D_FF,)),\n",
    "            \"w2\": nb.glorot_uniform((D_FF, D_MODEL)),\n",
    "            \"b2\": nb.zeros((D_MODEL,)),\n",
    "        },\n",
    "        \"norm1\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm2\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "        \"norm3\": {\"gamma\": nb.ones((D_MODEL,)), \"beta\": nb.zeros((D_MODEL,))},\n",
    "    }\n",
    "\n",
    "\n",
    "def init_transformer_params_nabla():\n",
    "    params: dict[str, Any] = {\"encoder\": {}, \"decoder\": {}}\n",
    "    params[\"encoder\"][\"embedding\"] = nb.randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"decoder\"][\"embedding\"] = nb.randn((VOCAB_SIZE, D_MODEL))\n",
    "    params[\"pos_encoding\"] = positional_encoding_nabla(MAX_SEQ_LEN, D_MODEL)\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"encoder\"][f\"layer_{i}\"] = _init_encoder_layer_params_nabla()\n",
    "    params[\"encoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": nb.ones((D_MODEL,)),\n",
    "        \"beta\": nb.zeros((D_MODEL,)),\n",
    "    }\n",
    "    for i in range(NUM_LAYERS):\n",
    "        params[\"decoder\"][f\"layer_{i}\"] = _init_decoder_layer_params_nabla()\n",
    "    params[\"decoder\"][\"final_norm\"] = {\n",
    "        \"gamma\": nb.ones((D_MODEL,)),\n",
    "        \"beta\": nb.zeros((D_MODEL,)),\n",
    "    }\n",
    "    params[\"output_linear\"] = nb.glorot_uniform((D_MODEL, VOCAB_SIZE))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Generation\n",
    "\n",
    "This function generates a batch of data for our sequence reversal task. For an input sequence like `[a, b, c]`, it produces:\n",
    "- **Encoder Input:** `[a, b, c]`\n",
    "- **Decoder Input:** `[<START>, c, b, a]` (for teacher-forcing during training)\n",
    "- **Target:** `[c, b, a, <END>]` (the ground truth for the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(batch_size):\n",
    "    \"\"\"Generates a dataset batch using NumPy first, then converts to framework-specific arrays.\"\"\"\n",
    "    base_sequences_np = np.random.randint(\n",
    "        3, VOCAB_SIZE, size=(batch_size, SOURCE_SEQ_LEN), dtype=np.int32\n",
    "    )\n",
    "    reversed_sequences_np = np.flip(base_sequences_np, axis=1)\n",
    "    encoder_input_np = base_sequences_np\n",
    "    start_tokens_np = np.ones((batch_size, 1), dtype=np.int32)  # <START> token (1)\n",
    "    decoder_input_np = np.concatenate([start_tokens_np, reversed_sequences_np], axis=1)\n",
    "    end_tokens_np = np.full((batch_size, 1), 2, dtype=np.int32)  # <END> token (2)\n",
    "    target_np = np.concatenate([reversed_sequences_np, end_tokens_np], axis=1)\n",
    "    return encoder_input_np, decoder_input_np, target_np\n",
    "\n",
    "\n",
    "def create_reverse_dataset_jax(batch_size):\n",
    "    encoder_input_np, decoder_input_np, target_np = create_reverse_dataset(batch_size)\n",
    "    return (\n",
    "        jnp.array(encoder_input_np),\n",
    "        jnp.array(decoder_input_np),\n",
    "        jnp.array(target_np),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_reverse_dataset_nabla(batch_size):\n",
    "    encoder_input_np, decoder_input_np, target_np = create_reverse_dataset(batch_size)\n",
    "    return (\n",
    "        nb.Array.from_numpy(encoder_input_np),\n",
    "        nb.Array.from_numpy(decoder_input_np),\n",
    "        nb.Array.from_numpy(target_np),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Optimizer (AdamW)\n",
    "\n",
    "We implement the AdamW optimizer, which is a variant of Adam that decouples weight decay from the gradient update. This often leads to better performance. The implementation includes state initialization (`m` and `v` vectors) and the update step, which also features gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_adamw_state_recursive(params, m_states, v_states, zeros_like_fn):\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            m_states[key], v_states[key] = {}, {}\n",
    "            _init_adamw_state_recursive(\n",
    "                value, m_states[key], v_states[key], zeros_like_fn\n",
    "            )\n",
    "        else:\n",
    "            m_states[key] = zeros_like_fn(value)\n",
    "            v_states[key] = zeros_like_fn(value)\n",
    "\n",
    "\n",
    "def init_adamw_state_jax(params):\n",
    "    m_states, v_states = {}, {}\n",
    "    _init_adamw_state_recursive(params, m_states, v_states, jnp.zeros_like)\n",
    "    return m_states, v_states\n",
    "\n",
    "\n",
    "def init_adamw_state_nabla(params):\n",
    "    m_states, v_states = {}, {}\n",
    "    _init_adamw_state_recursive(params, m_states, v_states, nb.zeros_like)\n",
    "    return m_states, v_states\n",
    "\n",
    "\n",
    "def adamw_step(params, grads, m, v, step, lr, framework_lib):\n",
    "    \"\"\"A generic AdamW step that can be used by both frameworks.\"\"\"\n",
    "    beta1, beta2, eps, weight_decay = 0.9, 0.999, 1e-8, 0.01\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    total_grad_norm_sq = [0.0]\n",
    "\n",
    "    def _calc_norm(g_dict):\n",
    "        for g in g_dict.values():\n",
    "            if isinstance(g, dict):\n",
    "                _calc_norm(g)\n",
    "            else:\n",
    "                total_grad_norm_sq[0] += framework_lib.sum(g * g)\n",
    "\n",
    "    _calc_norm(grads)\n",
    "    grad_norm = framework_lib.sqrt(total_grad_norm_sq[0])\n",
    "    clip_factor = framework_lib.minimum(1.0, max_grad_norm / (grad_norm + 1e-8))\n",
    "\n",
    "    updated_params, updated_m, updated_v = {}, {}, {}\n",
    "\n",
    "    def _update(p_dict, g_dict, m_dict, v_dict, up_p, up_m, up_v):\n",
    "        for key in p_dict:\n",
    "            if isinstance(p_dict[key], dict):\n",
    "                up_p[key], up_m[key], up_v[key] = {}, {}, {}\n",
    "                _update(\n",
    "                    p_dict[key],\n",
    "                    g_dict[key],\n",
    "                    m_dict[key],\n",
    "                    v_dict[key],\n",
    "                    up_p[key],\n",
    "                    up_m[key],\n",
    "                    up_v[key],\n",
    "                )\n",
    "            else:\n",
    "                p, g, m_val, v_val = (\n",
    "                    p_dict[key],\n",
    "                    g_dict[key] * clip_factor,\n",
    "                    m_dict[key],\n",
    "                    v_dict[key],\n",
    "                )\n",
    "                up_m[key] = beta1 * m_val + (1.0 - beta1) * g\n",
    "                up_v[key] = beta2 * v_val + (1.0 - beta2) * (g * g)\n",
    "                m_corr = up_m[key] / (1.0 - beta1**step)\n",
    "                v_corr = up_v[key] / (1.0 - beta2**step)\n",
    "                up_p[key] = p - lr * (\n",
    "                    m_corr / (framework_lib.sqrt(v_corr) + eps) + weight_decay * p\n",
    "                )\n",
    "\n",
    "    _update(params, grads, m, v, updated_params, updated_m, updated_v)\n",
    "    return updated_params, updated_m, updated_v\n",
    "\n",
    "\n",
    "def adamw_step_jax(params, grads, m, v, step, lr):\n",
    "    return adamw_step(params, grads, m, v, step, lr, jnp)\n",
    "\n",
    "\n",
    "def adamw_step_nabla(params, grads, m, v, step, lr):\n",
    "    return adamw_step(params, grads, m, v, step, lr, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. JIT-Compiled Training Step & Inference\n",
    "\n",
    "We define the core logic for a single training step and the inference (prediction) process.\n",
    "\n",
    "- **`complete_training_step`**: This function encapsulates the forward pass, loss calculation, backpropagation (gradient computation), and optimizer update. We decorate it with `@jax.jit` or `@nb.jit` to compile the entire sequence of operations into a single, highly optimized kernel for maximum performance.\n",
    "- **`predict_sequence`**: This function performs autoregressive inference. It generates the output sequence one token at a time, feeding its own prediction from the previous step back into the model as input for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JAX Implementations ---\n",
    "@jax.jit\n",
    "def complete_training_step_jax(\n",
    "    encoder_in, decoder_in, targets, params, m_states, v_states, step\n",
    "):\n",
    "    def loss_fn(p):\n",
    "        return cross_entropy_loss_jax(\n",
    "            transformer_forward_jax(encoder_in, decoder_in, p), targets\n",
    "        )\n",
    "\n",
    "    loss_value, grads = value_and_grad(loss_fn)(params)\n",
    "    updated_params, updated_m, updated_v = adamw_step_jax(\n",
    "        params, grads, m_states, v_states, step, LEARNING_RATE\n",
    "    )\n",
    "    return updated_params, updated_m, updated_v, loss_value\n",
    "\n",
    "\n",
    "def predict_sequence_jax(encoder_input, params):\n",
    "    if encoder_input.ndim == 1:\n",
    "        encoder_input = jnp.expand_dims(encoder_input, axis=0)\n",
    "    decoder_tokens = [jnp.ones((1,), dtype=jnp.int32)]\n",
    "    decoder_input = jnp.zeros((1, TARGET_SEQ_LEN), dtype=jnp.int32)\n",
    "    decoder_input = decoder_input.at[:, 0].set(1)\n",
    "\n",
    "    for pos in range(1, TARGET_SEQ_LEN):\n",
    "        logits = transformer_forward_jax(encoder_input, decoder_input, params)\n",
    "        next_token_logits = logits[:, pos - 1, :]\n",
    "        predicted_token = jnp.argmax(next_token_logits, axis=-1).astype(jnp.int32)\n",
    "        if pos < TARGET_SEQ_LEN:\n",
    "            decoder_input = decoder_input.at[:, pos].set(predicted_token[0])\n",
    "\n",
    "    return decoder_input[0]\n",
    "\n",
    "\n",
    "# --- Nabla Implementations ---\n",
    "@nb.jit\n",
    "def complete_training_step_nabla(\n",
    "    encoder_in, decoder_in, targets, params, m_states, v_states, step\n",
    "):\n",
    "    def loss_fn(p):\n",
    "        return cross_entropy_loss_nabla(\n",
    "            transformer_forward_nabla(encoder_in, decoder_in, p), targets\n",
    "        )\n",
    "\n",
    "    loss_value, grads = nb.value_and_grad(loss_fn)(params)\n",
    "    updated_params, updated_m, updated_v = adamw_step_nabla(\n",
    "        params, grads, m_states, v_states, step, LEARNING_RATE\n",
    "    )\n",
    "    return updated_params, updated_m, updated_v, loss_value\n",
    "\n",
    "\n",
    "def predict_sequence_nabla(encoder_input, params):\n",
    "    if len(encoder_input.shape) == 1:\n",
    "        encoder_input = encoder_input.reshape((1, encoder_input.shape[0]))\n",
    "\n",
    "    decoder_tokens = [nb.ones((1,), dtype=nb.DType.int32)]\n",
    "\n",
    "    for pos in range(1, TARGET_SEQ_LEN):\n",
    "        current_decoder_input = nb.stack(decoder_tokens, axis=1)\n",
    "\n",
    "        logits = transformer_forward_nabla(encoder_input, current_decoder_input, params)\n",
    "\n",
    "        next_token_logits = logits[:, pos - 1, :]\n",
    "\n",
    "        predicted_token = nb.argmax(next_token_logits, axes=-1).astype(nb.DType.int32)\n",
    "\n",
    "        decoder_tokens.append(predicted_token)\n",
    "\n",
    "    final_sequence = nb.stack(decoder_tokens, axis=1)\n",
    "\n",
    "    return final_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. JAX Training Run\n",
    "\n",
    "This cell contains the complete training loop for the **JAX** implementation. It initializes the parameters and optimizer state, then iterates through the training epochs, calling the JIT-compiled `complete_training_step_jax` function. Finally, it evaluates the trained model on a few test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_jax():\n",
    "    \"\"\"Main training loop for the JAX transformer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 TRAINING TRANSFORMER FROM SCRATCH WITH JAX\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"🔧 Initializing transformer parameters...\")\n",
    "    params = init_transformer_params_jax()\n",
    "    print(\"📈 Initializing AdamW optimizer...\")\n",
    "    m_states, v_states = init_adamw_state_jax(params)\n",
    "\n",
    "    print(\"🔥 JIT warmup (3 steps)...\")\n",
    "    for i in range(3):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_jax(BATCH_SIZE)\n",
    "        params, m_states, v_states, _ = complete_training_step_jax(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, i + 1\n",
    "        )\n",
    "    print(\"✅ Warmup complete! Starting timed training...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_history = []\n",
    "    time_history = [start_time]\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_jax(BATCH_SIZE)\n",
    "        params, m_states, v_states, loss = complete_training_step_jax(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, epoch\n",
    "        )\n",
    "        loss_history.append(float(loss))\n",
    "        time_history.append(time.time())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d} | Loss: {float(loss):.4f} | Time: {time_history[-1] - start_time:.1f}s\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"\\n✅ JAX Training complete! Total time: {time_history[-1] - start_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    return params, loss_history, time_history\n",
    "\n",
    "\n",
    "def plot_loss_curves(\n",
    "    jax_loss_history, jax_time_history, nabla_loss_history, nabla_time_history\n",
    "):\n",
    "    \"\"\"Plot the loss curves for both JAX and Nabla implementations.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Plot loss vs epochs\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(jax_loss_history, label=\"JAX\")\n",
    "    plt.plot(nabla_loss_history, label=\"Nabla\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot loss vs time - ensure equal length by truncating the longer one\n",
    "    plt.subplot(1, 2, 2)\n",
    "    min_len = min(\n",
    "        len(jax_loss_history),\n",
    "        len(jax_time_history),\n",
    "        len(nabla_loss_history),\n",
    "        len(nabla_time_history),\n",
    "    )\n",
    "    jax_times = [t - jax_time_history[0] for t in jax_time_history[:min_len]]\n",
    "    nabla_times = [t - nabla_time_history[0] for t in nabla_time_history[:min_len]]\n",
    "    plt.plot(jax_times, jax_loss_history[:min_len], label=\"JAX\")\n",
    "    plt.plot(nabla_times, nabla_loss_history[:min_len], label=\"Nabla\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the JAX training and store results\n",
    "jax_params, jax_loss_history, jax_time_history = train_transformer_jax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Nabla Training Run\n",
    "\n",
    "This cell contains the complete training loop for the **Nabla** implementation. It follows the same structure as the JAX version, using the `_nabla` suffixed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_nabla():\n",
    "    \"\"\"Main training loop for the Nabla transformer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 TRAINING TRANSFORMER FROM SCRATCH WITH NABLA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"🔧 Initializing transformer parameters...\")\n",
    "    params = init_transformer_params_nabla()\n",
    "    print(\"📈 Initializing AdamW optimizer...\")\n",
    "    m_states, v_states = init_adamw_state_nabla(params)\n",
    "\n",
    "    print(\"🔥 JIT warmup (3 steps)...\")\n",
    "    for i in range(3):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_nabla(BATCH_SIZE)\n",
    "        params, m_states, v_states, _ = complete_training_step_nabla(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, i + 1\n",
    "        )\n",
    "    print(\"✅ Warmup complete! Starting timed training...\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_history = []\n",
    "    time_history = [start_time]\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        enc_in, dec_in, targets = create_reverse_dataset_nabla(BATCH_SIZE)\n",
    "        params, m_states, v_states, loss = complete_training_step_nabla(\n",
    "            enc_in, dec_in, targets, params, m_states, v_states, epoch\n",
    "        )\n",
    "        loss_history.append(float(loss.to_numpy()))\n",
    "        time_history.append(time.time())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d} | Loss: {loss.to_numpy():.4f} | Time: {time_history[-1] - start_time:.1f}s\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"\\n✅ Nabla Training complete! Total time: {time_history[-1] - start_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    return params, loss_history, time_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Nabla training and store results\n",
    "nabla_params, nabla_loss_history, nabla_time_history = train_transformer_nabla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Loss Curves Visualization\n",
    "\n",
    "Now that we've trained both models, let's visualize and compare their training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves for both implementations\n",
    "plot_loss_curves(\n",
    "    jax_loss_history, jax_time_history, nabla_loss_history, nabla_time_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Evaluation\n",
    "\n",
    "Let's evaluate both models on the same test examples to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(params, predict_fn, create_dataset_fn, framework_name):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"🧪 FINAL {framework_name.upper()} EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test on 5 random examples\n",
    "    for i in range(5):\n",
    "        test_enc_in, _, test_target = create_dataset_fn(1)\n",
    "        prediction = predict_fn(test_enc_in[0], params)\n",
    "\n",
    "        if framework_name == \"jax\":\n",
    "            is_correct = jnp.array_equal(prediction[1:], test_target[0])\n",
    "            print(f\"Example {i + 1}:\")\n",
    "            print(f\"  Input:           {test_enc_in[0]}\")\n",
    "            print(f\"  Expected output: {test_target[0]}\")\n",
    "            print(f\"  Predicted:       {prediction[1:]}\")\n",
    "            print(f\"  Correct:         {'✅ YES' if is_correct else '❌ NO'}\")\n",
    "        else:  # nabla\n",
    "            is_correct = np.array_equal(\n",
    "                prediction[1:].to_numpy(), test_target[0].to_numpy()\n",
    "            )\n",
    "            print(f\"Example {i + 1}:\")\n",
    "            print(f\"  Input:           {test_enc_in[0]}\")\n",
    "            print(f\"  Expected output: {test_target[0]}\")\n",
    "            print(f\"  Predicted:       {prediction[1:]}\")\n",
    "            print(f\"  Correct:         {'✅ YES' if is_correct else '❌ NO'}\")\n",
    "\n",
    "\n",
    "# Evaluate JAX model\n",
    "evaluate_model(jax_params, predict_sequence_jax, create_reverse_dataset_jax, \"jax\")\n",
    "\n",
    "# Evaluate Nabla model\n",
    "evaluate_model(\n",
    "    nabla_params, predict_sequence_nabla, create_reverse_dataset_nabla, \"nabla\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
