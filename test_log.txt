(venv) ~/Documents/CodingProjects/nabla $ pytest tests/*
....F.F........F.F................................................................................................................................. [ 31%]
........................................F..........F............................................................................................... [ 63%]
................................................................................................................................................... [ 94%]
.....FF.................                                                                                                                            [100%]
======================================================================== FAILURES =========================================================================
__________________________________________________ TestPPAnalysis.test_transformer_attention_block_pp_2d __________________________________________________

self = <tests.integration.test_pp_analysis.TestPPAnalysis testMethod=test_transformer_attention_block_pp_2d>

    def test_transformer_attention_block_pp_2d(self):
        """Transformer Block with 2D Inputs and 'Row-Parallel' Sharding.
    
        As requested by user:
        - Input is 2D: (BATCH, D_MODEL * STAGES)
        - Input sharded/concatenated on second axis: <*, stage>
        - Weights concatenated/sharded on first axis: (D_MODEL * STAGES, D_MODEL) -> <stage, *>
        - This provokes contraction on the 'stage' axis -> AllReduce.
        """
        print("\n" + "="*80)
        print("TRANSFORMER PP (2D PATTERN)")
        print("="*80)
    
        BATCH = 4
        D_MODEL = 8
        NUM_STAGES = 4
        TOTAL_DIM = D_MODEL * NUM_STAGES
    
        mesh = DeviceMesh("pp", (NUM_STAGES,), ("stage",))
    
        # Input: 2D (BATCH, TOTAL_DIM) sharded on dim 1
        x_np = np.random.randn(BATCH, TOTAL_DIM).astype(np.float32) * 0.01
        x = ops.shard(Tensor.from_dlpack(x_np), mesh, [DimSpec([]), DimSpec(["stage"])])
    
        # Weights: 2D (TOTAL_DIM, D_MODEL) sharded on dim 0
        def make_weight(d_out):
            w_np = np.random.randn(TOTAL_DIM, d_out).astype(np.float32) * 0.1
            return ops.shard(Tensor.from_dlpack(w_np), mesh, [DimSpec(["stage"]), DimSpec([])])
    
        Wq = make_weight(D_MODEL)
        Wk = make_weight(D_MODEL)
        Wv = make_weight(D_MODEL)
    
        # Output projection needs to map D_MODEL -> TOTAL_DIM (for residual)
        # But wait, standard residual adds to input.
        # Input x is [B, D*S].
        # If we reduce Q, K, V to [B, D], we cannot add to x [B, D*S].
        # We'll assume the output of this block is [B, D] (Encoder/Decoder mismatch?)
        # Or we project back up.
        Wo = ops.shard(Tensor.from_dlpack(np.random.randn(D_MODEL, TOTAL_DIM).astype(np.float32)), mesh, [DimSpec([]), DimSpec(["stage"])])
    
        def attention_2d_flat(x, Wq, Wk, Wv, Wo):
            # x: [B, D*S]<*, s>
            # W: [D*S, D]<s, *>
            # Q: [B, D*S] @ [D*S, D] -> [B, D] <*, *> (AllReduce)
            Q = x @ Wq
            K = x @ Wk
            V = x @ Wv
    
            # Simple "Attention" over batch (for 2D testing) or just matmuls
            # [B, D] @ [B, D].T -> [B, B]
            scores = Q @ ops.view.swap_axes(K, 0, 1)
            scores = ops.softmax(scores, axis=-1)
    
            # [B, B] @ [B, D] -> [B, D]
            out = scores @ V
    
            # Project back to [B, D*S]
            # [B, D]<*,*> @ [D, D*S]<*, s> -> [B, D*S]<*, s> (Broadcast input)
            # Use manual shard strategy for Wo to allow parallel fan-out?
            # Wo is [D, D*S] sharded on 1.
            # Matmul should handle it (Input replicated, Weight sharded cols -> Output sharded cols)
            return out @ Wo
    
        def single_pass(x, Wq, Wk, Wv, Wo):
            perm = [(i, (i + 1) % NUM_STAGES) for i in range(NUM_STAGES)]
            y = attention_2d_flat(x, Wq, Wk, Wv, Wo)
            # Residual
            out = x + y
            return communication.ppermute(out, perm)
    
        print(f"\nShapes:")
        print(f"  x: {x.shape} <*, stage> local {x._impl.physical_local_shape(0)}")
        print(f"  Wq: {Wq.shape} <stage, *> local {Wq._impl.physical_local_shape(0)}")
    
        print("\nðŸ“Š TRACE:")
        print("-" * 60)
        t = trace(single_pass, x, Wq, Wk, Wv, Wo)
        print(t)
        print("-" * 60)
    
        # User expects 'stage' to disappear (AllReduce) then reappear (Shard/FanOut)
        self.assertIn("all_reduce", str(t))
    
        result = single_pass(x, Wq, Wk, Wv, Wo)
        self.assertEqual(tuple(int(d) for d in result.shape), (BATCH, TOTAL_DIM))
        print(f"\nâœ… PASS: Result shape {result.shape}")
    
        # NumPy Ref
        # Split inputs and first-layer weights
        x_shards = np.split(x_np, NUM_STAGES, axis=1)
        wq_shards = np.split(Wq.to_numpy(), NUM_STAGES, axis=0)
        wk_shards = np.split(Wk.to_numpy(), NUM_STAGES, axis=0)
        wv_shards = np.split(Wv.to_numpy(), NUM_STAGES, axis=0)
    
        # Q = Sum(x_i @ wq_i)
        # This logic mimics what the trace should do (Row Parallel)
        Q = sum(x_shards[i] @ wq_shards[i] for i in range(NUM_STAGES))
>       K = sum(x_shards[i] @ wk_shards[i] for i in range(NUM_STAGES))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/test_pp_analysis.py:620: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x10f07c720>

>   K = sum(x_shards[i] @ wk_shards[i] for i in range(NUM_STAGES))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 8)

tests/integration/test_pp_analysis.py:620: ValueError
------------------------------------------------------------------ Captured stdout call -------------------------------------------------------------------

================================================================================
TRANSFORMER PP (2D PATTERN)
================================================================================

Shapes:
  x: [Dim(4), Dim(32)] <*, stage> local [Dim(4), Dim(8)]
  Wq: [Dim(32), Dim(8)] <stage, *> local [Dim(8), Dim(8)]

ðŸ“Š TRACE:
------------------------------------------------------------
fn(
    %a1: f32[4,32](<*, stage>)(local=[4,8]),
    %a2: f32[32,8](<stage, *>)(local=[8,8]),
    %a3: f32[32,8](<stage, *>)(local=[8,8]),
    %a4: f32[32,8](<stage, *>)(local=[8,8]),
    %a5: f32[8,32](<*, stage>)(local=[8,8])
) @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v1: f32[4,8](<*, *>)(local=[4,8]) = matmul(%a1, %a2)
  }
  %v2: f32[4,8](<*, *>)(local=[4,8]) = all_reduce(%v1, mesh=@pp, reduce_axes=('stage',))
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v3: f32[4,8](<*, *>)(local=[4,8]) = matmul(%a1, %a3)
  }
  %v4: f32[4,8](<*, *>)(local=[4,8]) = all_reduce(%v3, mesh=@pp, reduce_axes=('stage',))
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v5: f32[8,4](<*, *>)(local=[8,4]) = swap_axes(%v4, axis1=0, axis2=1)
    %v6: f32[4,4](<*, *>)(local=[4,4]) = matmul(%v2, %v5)
    %v7: f32[4,4](<*, *>)(local=[4,4]) = softmax(%v6, axis=-1)
    %v8: f32[4,8](<*, *>)(local=[4,8]) = matmul(%a1, %a4)
  }
  %v9: f32[4,8](<*, *>)(local=[4,8]) = all_reduce(%v8, mesh=@pp, reduce_axes=('stage',))
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v10: f32[4,8](<*, *>)(local=[4,8]) = matmul(%v7, %v9)
    %v11: f32[4,32](<*, stage>)(local=[4,8]) = matmul(%v10, %a5)
    %v12: f32[4,32](<*, stage>)(local=[4,8]) = add(%a1, %v11)
  }
  %v13: f32[4,32](<*, stage>)(local=[4,8]) = ppermute(%v12, permutation=((0,1),(1,2),(2,3),(3,0)))
  return %v13
}
------------------------------------------------------------

âœ… PASS: Result shape [Dim(4), Dim(32)]
_____________________________________________________ TestPPAnalysis.test_unbatched_2d_transformer_pp _____________________________________________________

self = <tests.integration.test_pp_analysis.TestPPAnalysis testMethod=test_unbatched_2d_transformer_pp>

    def test_unbatched_2d_transformer_pp(self):
        """Unbatched 2D Transformer block: attention + FFN.
    
        Same sharding pattern as MLP - concatenate on first axis.
        """
        print("\n" + "="*80)
        print("UNBATCHED 2D TRANSFORMER PP")
        print("="*80)
    
        SEQ_LEN = 4
        D_MODEL = 8
        D_FF = 16
        NUM_STAGES = 4
    
        mesh = DeviceMesh("pp", (NUM_STAGES,), ("stage",))
    
        def attention_2d(Q, K, V):
            """2D attention: (seq, d) @ (d, seq) -> (seq, seq)"""
            scores = Q @ ops.view.swap_axes(K, 0, 1)
            scores = scores / np.sqrt(D_MODEL)
            attn_weights = ops.softmax(scores, axis=-1)
            return attn_weights @ V
    
        def transformer_2d(x, Wq, Wk, Wv, Wo, W1, W2):
            """2D transformer block."""
            Q = x @ Wq
            K = x @ Wk
            V = x @ Wv
            attn_out = attention_2d(Q, K, V) @ Wo
            x = x + attn_out
    
            ffn_out = x @ W1
            ffn_out = ops.relu(ffn_out)
            ffn_out = ffn_out @ W2
            x = x + ffn_out
    
            return x
    
        def single_pass(x, Wq, Wk, Wv, Wo, W1, W2):
            perm = [(i, (i + 1) % NUM_STAGES) for i in range(NUM_STAGES)]
            x = transformer_2d(x, Wq, Wk, Wv, Wo, W1, W2)
            x = communication.ppermute(x, perm)
            return x
    
        # Input and weights - all sharded on first axis
        x_np = np.random.randn(SEQ_LEN * NUM_STAGES, D_MODEL).astype(np.float32) * 0.01
        x = ops.shard(Tensor.from_dlpack(x_np), mesh, [DimSpec(["stage"]), DimSpec([])])
    
        def make_weight(d_in, d_out):
            w_np = np.random.randn(d_in * NUM_STAGES, d_out).astype(np.float32) * 0.1
            return ops.shard(Tensor.from_dlpack(w_np), mesh, [DimSpec(["stage"]), DimSpec([])]), w_np
    
        Wq, wq_np = make_weight(D_MODEL, D_MODEL)
        Wk, wk_np = make_weight(D_MODEL, D_MODEL)
        Wv, wv_np = make_weight(D_MODEL, D_MODEL)
        Wo, wo_np = make_weight(D_MODEL, D_MODEL)
        W1, w1_np = make_weight(D_MODEL, D_FF)
        W2, w2_np = make_weight(D_FF, D_MODEL)
    
        print(f"\nShapes:")
        print(f"  x: {x.shape} local {x._impl.physical_local_shape(0)}")
        print(f"  Wq: {Wq.shape} local {Wq._impl.physical_local_shape(0)}")
    
        print("\nðŸ“Š TRACE:")
        print("-" * 60)
        t = trace(single_pass, x, Wq, Wk, Wv, Wo, W1, W2)
        print(t)
        print("-" * 60)
    
        self.assertNotIn("all_reduce", str(t))
        self.assertIn("ppermute", str(t))
        self.assertIn("softmax", str(t))
    
        result = single_pass(x, Wq, Wk, Wv, Wo, W1, W2)
        self.assertEqual(tuple(int(d) for d in result.shape), (SEQ_LEN * NUM_STAGES, D_MODEL))
        print(f"\nâœ… PASS: Result shape {result.shape}")
    
        # --- Numerical Verification ---
        # --- Numerical Verification ---
        # Split inputs and weights per stage
        x_shards = np.split(x_np, NUM_STAGES, axis=0)
    
        # Wq was created sharded, so convert back then split
        wq_shards = np.split(wq_np, NUM_STAGES, axis=0)
    
        # Helper to split based on "make_weight" logic (sharding on dim 0)
        def split_w(t_val):
            return np.split(t_val, NUM_STAGES, axis=0)
    
        sq = split_w(wq_np); sk = split_w(wk_np); sv = split_w(wv_np); so = split_w(wo_np)
        s1 = split_w(w1_np); s2 = split_w(w2_np)
    
        shard_results = []
        for i in range(NUM_STAGES):
            xi = x_shards[i]
    
            # Attention
            Q = xi @ sq[i]
            K = xi @ sk[i]
            V = xi @ sv[i]
            scores = Q @ K.swapaxes(0, 1) / np.sqrt(D_MODEL)
            # Softmax on last axis
            e_x = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
            attn = e_x / np.sum(e_x, axis=-1, keepdims=True)
            attn_out = (attn @ V) @ so[i]
    
            h = xi + attn_out
    
            # FFN
            ff = np.maximum(h @ s1[i], 0)
            ff_out = ff @ s2[i]
    
            res = h + ff_out
            shard_results.append(res)
    
        # Permute
        permuted = [None] * NUM_STAGES
        for i in range(NUM_STAGES):
            permuted[(i + 1) % NUM_STAGES] = shard_results[i]
    
        # With Shardy AllReduce removed, the result is the local partial sum.
        # But wait, 'softmax' on partial sum is weird.
        # User intent: "we compute local shards".
        # We will verify that what we get matches "Softmax(Partial) @ V_partial".
    
        # Construct expected tensor for Device 0 (which is what to_numpy() returns for <*, *> or <*, stage>?)
        # result is <*, stage>. to_numpy() on sharded tensor GATHERS all shards.
        # So actual should be the concatenation of all `permuted` shards!
    
        # Since result is <*, stage>, actual is (Batch, Total_Dim).
        # It matches expected if expected is concatenation of partial-processed shards.
        # DEBUG: Checking expected construction
        # expected = np.concatenate(permuted, axis=1) # Concatenate along stage axis (1) --> This produces (4, 32) but we want (16, 8) if sharded on axis 0!
        expected = np.concatenate(permuted, axis=0)
        print(f"DEBUG: expected.shape={expected.shape}")
    
        print(f"DEBUG: result.is_sharded={result.is_sharded}")
        print(f"DEBUG: result.sharding={result.sharding}")
    
        actual = result.to_numpy() # This triggers execution
        print(f"DEBUG: actual.shape={actual.shape}")
    
        # We updated logic to be: Q = xi @ sq[i] (Partial).
        # Original code sum() calculated global Q.
        # We need to recalculate `shard_results` using local logic only.
    
        # RE-RUN REF LOGIC FOR LOCAL ONLY
        shard_results_local = []
        for i in range(NUM_STAGES):
            xi = x_shards[i]
            # Local Attention (No Sum)
            Q = xi @ sq[i]
            K = xi @ sk[i]
            V = xi @ sv[i]
            scores = Q @ K.swapaxes(0, 1) / np.sqrt(D_MODEL)
    
            # Softmax on local scores (User: "result is correctly sharded")
            e_x = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
            attn = e_x / np.sum(e_x, axis=-1, keepdims=True)
    
            attn_out = (attn @ V) @ so[i]
            h = xi + attn_out
    
            ff = np.maximum(h @ s1[i], 0)
            ff_out = ff @ s2[i]
            res = h + ff_out
            shard_results_local.append(res)
    
    
        permuted_local = [None] * NUM_STAGES
        for i in range(NUM_STAGES):
            permuted_local[(i + 1) % NUM_STAGES] = shard_results_local[i]
    
        expected = np.concatenate(permuted_local, axis=0) # Unbatched 2D MLP shards on axis 0
    
        actual = result.to_numpy()
>       np.testing.assert_allclose(actual, expected, rtol=1e-4, atol=1e-4)
E       AssertionError: 
E       Not equal to tolerance rtol=0.0001, atol=0.0001
E       
E       (shapes (4, 8), (16, 8) mismatch)
E        ACTUAL: array([[ 0.017683, -0.010936,  0.010396, -0.023707,  0.006874, -0.024118,
E               -0.014339,  0.001874],
E              [ 0.009077, -0.002403, -0.00735 ,  0.002569,  0.010749, -0.008733,...
E        DESIRED: array([[ 0.017683, -0.010936,  0.010396, -0.023707,  0.006874, -0.024118,
E               -0.014339,  0.001874],
E              [ 0.009077, -0.002403, -0.00735 ,  0.002569,  0.010749, -0.008733,...

tests/integration/test_pp_analysis.py:275: AssertionError
------------------------------------------------------------------ Captured stdout call -------------------------------------------------------------------

================================================================================
UNBATCHED 2D TRANSFORMER PP
================================================================================

Shapes:
  x: [Dim(16), Dim(8)] local [Dim(4), Dim(8)]
  Wq: [Dim(32), Dim(8)] local [Dim(8), Dim(8)]

ðŸ“Š TRACE:
------------------------------------------------------------
fn(
    %a1: f32[16,8](<stage, *>)(local=[4,8]),
    %a2: f32[32,8](<stage, *>)(local=[8,8]),
    %a3: f32[32,8](<stage, *>)(local=[8,8]),
    %a4: f32[32,8](<stage, *>)(local=[8,8]),
    %a5: f32[32,8](<stage, *>)(local=[8,8]),
    %a6: f32[32,16](<stage, *>)(local=[8,16]),
    %a7: f32[64,8](<stage, *>)(local=[16,8])
) @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v1: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%a1, %a2)
    %v2: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%a1, %a3)
    %v3: f32[8,16](<*, stage>)(local=[8,4]) = swap_axes(%v2, axis1=0, axis2=1)
    %v4: f32[16,4](<stage, *>)(local=[4,4]) = matmul(%v1, %v3)
  }
  %v5: f32[16,4](<stage, *>)(local=[4,4]) = shard(const, spec=<stage, *>, mesh=@pp)
  spmd @pp(shape=(4,), devices=[0,1,2,3], axes=(stage)) {
    %v6: f32[16,4](<stage, *>)(local=[4,4]) = div(%v4, %v5)
    %v7: f32[16,4](<stage, *>)(local=[4,4]) = softmax(%v6, axis=-1)
    %v8: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%a1, %a4)
    %v9: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%v7, %v8)
    %v10: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%v9, %a5)
    %v11: f32[16,8](<stage, *>)(local=[4,8]) = add(%a1, %v10)
    %v12: f32[16,16](<stage, *>)(local=[4,16]) = matmul(%v11, %a6)
    %v13: f32[16,16](<stage, *>)(local=[4,16]) = relu(%v12)
    %v14: f32[16,8](<stage, *>)(local=[4,8]) = matmul(%v13, %a7)
    %v15: f32[16,8](<stage, *>)(local=[4,8]) = add(%v11, %v14)
  }
  %v16: f32[16,8](<stage, *>)(local=[4,8]) = ppermute(%v15, permutation=((0,1),(1,2),(2,3),(3,0)))
  return %v16
}
------------------------------------------------------------

âœ… PASS: Result shape [Dim(16), Dim(8)]
DEBUG: expected.shape=(16, 8)
DEBUG: result.is_sharded=True
DEBUG: result.sharding=sharding<@pp, [{"stage", ?}, {?}]>
DEBUG: actual.shape=(16, 8)
____________________________________________________ TestShardMapComplex.test_output_spec_enforcement _____________________________________________________

self = <tests.integration.test_shard_map_complex.TestShardMapComplex testMethod=test_output_spec_enforcement>

    def test_output_spec_enforcement(self):
        """Test that out_specs forces resharding if result doesn't match."""
        # Function: x -> x (Identity)
        # Input: Replicated
        # Output Spec: Sharded on 'x'
        # Expectation: Result should be sharded.
    
        in_specs = {0: ShardingSpec(self.mesh, [DimSpec([]), DimSpec([])])} # Replicated input
        out_specs = {0: ShardingSpec(self.mesh, [DimSpec(["x"]), DimSpec([])])} # Sharded output
    
        def identity(x):
            return x
    
        x_np = np.ones((4, 4), dtype=np.float32)
        x = Tensor.from_dlpack(x_np)
    
        sharded_fn = shard_map(identity, self.mesh, in_specs, out_specs)
    
        # Run tracing
        res = sharded_fn(x)
    
        # Verify result IS sharded (it is the physical tensor)
        self.assertIsNotNone(res._impl.sharding)
        self.assertEqual(res._impl.sharding.dim_specs[0].axes, ["x"])
        self.assertEqual(res._impl.sharding.dim_specs[1].axes, [])
    
        # Run it
        res.realize()
        expected = x_np
>       np.testing.assert_allclose(res.to_numpy(), expected)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-07, atol=0
E       
E       (shapes (2, 4), (4, 4) mismatch)
E        ACTUAL: array([[1., 1., 1., 1.],
E              [1., 1., 1., 1.]], dtype=float32)
E        DESIRED: array([[1., 1., 1., 1.],
E              [1., 1., 1., 1.],
E              [1., 1., 1., 1.],
E              [1., 1., 1., 1.]], dtype=float32)

tests/integration/test_shard_map_complex.py:180: AssertionError
_______________________________________________________ TestShardMapComplex.test_reshape_resharding _______________________________________________________

self = <tests.integration.test_shard_map_complex.TestShardMapComplex testMethod=test_reshape_resharding>

    def test_reshape_resharding(self):
        """Test reshape that might imply sharding changes."""
        # Input (4, 4) sharded on x (axis 0).
        # Reshape to (16,) -> How does sharding propagate?
        # Typically reshape preserves total size.
    
        in_specs = {0: ShardingSpec(self.mesh, [DimSpec(["x"]), DimSpec([])])}
        # Output sharded on x? (16,) splits into (8,) per shard on axis x
        out_specs = {0: ShardingSpec(self.mesh, [DimSpec(["x"])])}
    
        def my_reshape(x):
            return view.reshape(x, (16,))
    
        x_np = np.arange(16, dtype=np.float32).reshape(4, 4)
        x = Tensor.from_dlpack(x_np)
    
        sharded_fn = shard_map(my_reshape, self.mesh, in_specs, out_specs)
    
        res = sharded_fn(x)
        res.realize()
    
        expected = x_np.reshape(16)
>       np.testing.assert_allclose(res.to_numpy(), expected, rtol=1e-5)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-05, atol=0
E       
E       (shapes (8,), (16,) mismatch)
E        ACTUAL: array([0., 1., 2., 3., 4., 5., 6., 7.], dtype=float32)
E        DESIRED: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
E              13., 14., 15.], dtype=float32)

tests/integration/test_shard_map_complex.py:78: AssertionError
______________________________________________________ TestSplitOp.test_split_sharded_non_split_axis ______________________________________________________

self = <tests.unit.test_multi_output_ops.TestSplitOp object at 0x10eceb360>, mesh_1d = @mesh_1d = <["dp"=4]>

    def test_split_sharded_non_split_axis(self, mesh_1d):
        """Split with sharding on non-split axis."""
        np_x = make_array(16, 8, seed=42)
        x = tensor_from_numpy(np_x)
    
        # Shard on axis 0
        x_sharded = x.shard(mesh_1d, P("dp", None))
    
        # Split along axis 1 (non-sharded axis)
        results = split(x_sharded, num_splits=2, axis=1)
        expected = np.split(np_x, 2, axis=1)
    
        assert len(results) == 2
        for r, e in zip(results, expected):
            assert_shape(r, (16, 4))
>           assert_allclose(r, e)

tests/unit/test_multi_output_ops.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

result = TensorType(dtype=float32, shape=[Dim(4), Dim(4)], device=cpu:0): [-1.9510351419448853, -1.3021794557571411, 0.12784039...22541332244873, -0.15452948212623596, 0.4127326011657715, 0.4308210015296936, 2.1416475772857666, -0.40641501545906067]
expected = array([[-1.9510351 , -1.3021795 ,  0.1278404 , -0.3162426 ],
       [ 0.0660307 ,  1.1272413 ,  0.46750933, -0.8592924...02185214,  1.6017789 , -0.23935562],
       [ 0.83511126,  0.35687107,  1.4633029 , -1.188763  ]],
      dtype=float32)
rtol = 1e-05, atol = 1e-06

    def assert_allclose(result: Tensor, expected: np.ndarray, rtol: float = 1e-5, atol: float = 1e-6):
        """Assert tensor values match expected numpy array."""
        actual = to_numpy(result)
>       np.testing.assert_allclose(actual, expected, rtol=rtol, atol=atol)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-05, atol=1e-06
E       
E       (shapes (4, 4), (16, 4) mismatch)
E        ACTUAL: array([[-1.951035, -1.302179,  0.12784 , -0.316243],
E              [ 0.066031,  1.127241,  0.467509, -0.859292],
E              [-0.184862, -0.68093 ,  1.222541, -0.154529],
E              [ 0.412733,  0.430821,  2.141648, -0.406415]], dtype=float32)
E        DESIRED: array([[-1.951035, -1.302179,  0.12784 , -0.316243],
E              [ 0.066031,  1.127241,  0.467509, -0.859292],
E              [-0.184862, -0.68093 ,  1.222541, -0.154529],...

tests/conftest.py:62: AssertionError
____________________________________________________________ TestUnbindOp.test_unbind_sharded _____________________________________________________________

self = <tests.unit.test_multi_output_ops.TestUnbindOp object at 0x10eceb950>, mesh_1d = @mesh_1d = <["dp"=4]>

    def test_unbind_sharded(self, mesh_1d):
        """Unbind with sharding."""
        np_x = make_array(4, 8, seed=42)
        x = tensor_from_numpy(np_x)
    
        # Shard on axis 1
        x_sharded = x.shard(mesh_1d, P(None, "dp"))
    
        # Unbind along axis 0 (non-sharded)
        results = unbind(x_sharded, axis=0)
    
        assert len(results) == 4
        for i, r in enumerate(results):
            assert_shape(r, (8,))
>           assert_allclose(r, np_x[i])

tests/unit/test_multi_output_ops.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

result = TensorType(dtype=float32, shape=[Dim(2)], device=cpu:0): [-0.01680115796625614, -0.8530439138412476]
expected = array([-0.01680116, -0.8530439 ,  0.879398  ,  0.7777919 ,  0.0660307 ,
        1.1272413 ,  0.46750933, -0.85929245], dtype=float32)
rtol = 1e-05, atol = 1e-06

    def assert_allclose(result: Tensor, expected: np.ndarray, rtol: float = 1e-5, atol: float = 1e-6):
        """Assert tensor values match expected numpy array."""
        actual = to_numpy(result)
>       np.testing.assert_allclose(actual, expected, rtol=rtol, atol=atol)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-05, atol=1e-06
E       
E       (shapes (2,), (8,) mismatch)
E        ACTUAL: array([-0.016801, -0.853044], dtype=float32)
E        DESIRED: array([-0.016801, -0.853044,  0.879398,  0.777792,  0.066031,  1.127241,
E               0.467509, -0.859292], dtype=float32)

tests/conftest.py:62: AssertionError
_____________________________________ TestVmapShardingMultiOutputOps.test_chunk_with_sharding[mesh_shape0-mesh_axes0] _____________________________________

self = <tests.unit.test_vmap_sharding_comprehensive.TestVmapShardingMultiOutputOps object at 0x10ee3c9d0>, mesh_shape = (2,), mesh_axes = ('tp',)

    @pytest.mark.parametrize("mesh_shape,mesh_axes", [
        ((2,), ("tp",)),
        ((2, 2), ("dp", "tp")),
    ])
    def test_chunk_with_sharding(self, mesh_shape, mesh_axes):
        """vmap(chunk) with sharded input."""
        batch = 4
        mesh = DeviceMesh("mesh", mesh_shape, mesh_axes)
    
        def f(x):  # x has LOGICAL shape (8, 16)
            x_sharded = x.shard(mesh, P(None, mesh_axes[-1]))
            return chunk(x_sharded, chunks=4, axis=0)  # Split into 4 chunks along first axis
    
        np_x = make_array(batch, 8, 16, seed=42)
        x = tensor_from_numpy(np_x)
    
        result = vmap(f)(x)
        expected_chunks = np.array_split(np_x, 4, axis=1)
    
        assert len(result) == 4
        for i, (r, e) in enumerate(zip(result, expected_chunks)):
            assert_shape(r, (batch, 2, 16))
>           assert_allclose(r, e)

tests/unit/test_vmap_sharding_comprehensive.py:1029: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

result = TensorType(dtype=float32, shape=[Dim(4), Dim(2), Dim(8)], device=cpu:0): [-0.5122427344322205, -0.8137727379798889, 0....82969832420349, -0.6050007343292236, -0.533900260925293, -1.0697524547576904, -0.6542832851409912, 0.42789044976234436]
expected = array([[[-0.51224273, -0.81377274,  0.61597943,  1.1289723 ,
         -0.11394746, -0.8401565 , -0.8244812 ,  0.650592...28662  ,  0.36192185,  1.3206617 ,
         -0.34278616, -1.4768578 ,  1.0672225 , -0.33148816]]],
      dtype=float32)
rtol = 1e-05, atol = 1e-06

    def assert_allclose(result: Tensor, expected: np.ndarray, rtol: float = 1e-5, atol: float = 1e-6):
        """Assert tensor values match expected numpy array."""
        actual = to_numpy(result)
>       np.testing.assert_allclose(actual, expected, rtol=rtol, atol=atol)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-05, atol=1e-06
E       
E       (shapes (4, 2, 8), (4, 2, 16) mismatch)
E        ACTUAL: array([[[-0.512243, -0.813773,  0.615979,  1.128972, -0.113947,
E                -0.840156, -0.824481,  0.650593],
E               [ 0.678914,  0.067579,  0.289119,  0.631288, -1.457156,...
E        DESIRED: array([[[-0.512243, -0.813773,  0.615979,  1.128972, -0.113947,
E                -0.840156, -0.824481,  0.650593,  0.743254,  0.543154,
E                -0.66551 ,  0.232161,  0.116686,  0.218689,  0.871429,...

tests/conftest.py:62: AssertionError
_____________________________________ TestVmapShardingMultiOutputOps.test_chunk_with_sharding[mesh_shape1-mesh_axes1] _____________________________________

self = <tests.unit.test_vmap_sharding_comprehensive.TestVmapShardingMultiOutputOps object at 0x10ee3cb00>, mesh_shape = (2, 2), mesh_axes = ('dp', 'tp')

    @pytest.mark.parametrize("mesh_shape,mesh_axes", [
        ((2,), ("tp",)),
        ((2, 2), ("dp", "tp")),
    ])
    def test_chunk_with_sharding(self, mesh_shape, mesh_axes):
        """vmap(chunk) with sharded input."""
        batch = 4
        mesh = DeviceMesh("mesh", mesh_shape, mesh_axes)
    
        def f(x):  # x has LOGICAL shape (8, 16)
            x_sharded = x.shard(mesh, P(None, mesh_axes[-1]))
            return chunk(x_sharded, chunks=4, axis=0)  # Split into 4 chunks along first axis
    
        np_x = make_array(batch, 8, 16, seed=42)
        x = tensor_from_numpy(np_x)
    
        result = vmap(f)(x)
        expected_chunks = np.array_split(np_x, 4, axis=1)
    
        assert len(result) == 4
        for i, (r, e) in enumerate(zip(result, expected_chunks)):
            assert_shape(r, (batch, 2, 16))
>           assert_allclose(r, e)

tests/unit/test_vmap_sharding_comprehensive.py:1029: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

result = TensorType(dtype=float32, shape=[Dim(4), Dim(2), Dim(8)], device=cpu:0): [-0.5122427344322205, -0.8137727379798889, 0....82969832420349, -0.6050007343292236, -0.533900260925293, -1.0697524547576904, -0.6542832851409912, 0.42789044976234436]
expected = array([[[-0.51224273, -0.81377274,  0.61597943,  1.1289723 ,
         -0.11394746, -0.8401565 , -0.8244812 ,  0.650592...28662  ,  0.36192185,  1.3206617 ,
         -0.34278616, -1.4768578 ,  1.0672225 , -0.33148816]]],
      dtype=float32)
rtol = 1e-05, atol = 1e-06

    def assert_allclose(result: Tensor, expected: np.ndarray, rtol: float = 1e-5, atol: float = 1e-6):
        """Assert tensor values match expected numpy array."""
        actual = to_numpy(result)
>       np.testing.assert_allclose(actual, expected, rtol=rtol, atol=atol)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-05, atol=1e-06
E       
E       (shapes (4, 2, 8), (4, 2, 16) mismatch)
E        ACTUAL: array([[[-0.512243, -0.813773,  0.615979,  1.128972, -0.113947,
E                -0.840156, -0.824481,  0.650593],
E               [ 0.678914,  0.067579,  0.289119,  0.631288, -1.457156,...
E        DESIRED: array([[[-0.512243, -0.813773,  0.615979,  1.128972, -0.113947,
E                -0.840156, -0.824481,  0.650593,  0.743254,  0.543154,
E                -0.66551 ,  0.232161,  0.116686,  0.218689,  0.871429,...

tests/conftest.py:62: AssertionError
================================================================= short test summary info =================================================================
FAILED tests/integration/test_pp_analysis.py::TestPPAnalysis::test_transformer_attention_block_pp_2d - ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 8)
FAILED tests/integration/test_pp_analysis.py::TestPPAnalysis::test_unbatched_2d_transformer_pp - AssertionError: 
FAILED tests/integration/test_shard_map_complex.py::TestShardMapComplex::test_output_spec_enforcement - AssertionError: 
FAILED tests/integration/test_shard_map_complex.py::TestShardMapComplex::test_reshape_resharding - AssertionError: 
FAILED tests/unit/test_multi_output_ops.py::TestSplitOp::test_split_sharded_non_split_axis - AssertionError: 
FAILED tests/unit/test_multi_output_ops.py::TestUnbindOp::test_unbind_sharded - AssertionError: 
FAILED tests/unit/test_vmap_sharding_comprehensive.py::TestVmapShardingMultiOutputOps::test_chunk_with_sharding[mesh_shape0-mesh_axes0] - AssertionError: 
FAILED tests/unit/test_vmap_sharding_comprehensive.py::TestVmapShardingMultiOutputOps::test_chunk_with_sharding[mesh_shape1-mesh_axes1] - AssertionError: 
8 failed, 457 passed in 86.95s (0:01:26)